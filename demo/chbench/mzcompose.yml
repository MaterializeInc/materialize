# Copyright Materialize, Inc. and contributors. All rights reserved.
#
# Use of this software is governed by the Business Source License
# included in the LICENSE file at the root of this repository.
#
# As of the Change Date specified in that file, in accordance with
# the Business Source License, use of this software will be governed
# by the Apache License, Version 2.0.


# Map from host-port:internal port
#
# This mostly just shows all the ports that are available to the host system, if you want
# to change these you must restart the docker-compose cluster.
x-port-mappings:
  - &kafka-internal ${KAFKA_PORT:-9092:9092}
  - &kafka-external 9093:9093
  - &schema-registry ${SR_PORT:-8081:8081}
  - &materialized ${MZ_PORT:-6875:6875}
  - &mysql ${MYSQL_PORT:-3306:3306}
  - &postgres ${POSTGRES_PORT:-5432:5432}
  - &control-center ${CC_PORT:-9021:9021}
  - &grafana 3000:3000
  - &prometheus 9090:9090
  - &metabase 3030:3000

version: '3.7'
services:
  aws-cli:
    image: amazon/aws-cli:2.1.18
    environment:
      - AWS_REGION=${AWS_REGION}
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      - AWS_SESSION_TOKEN=${AWS_SESSION_TOKEN}
    volumes:
      # this is the dir that contains all the individual snapshot directories
      - ${MZ_CHBENCH_SNAPSHOT_DIR}:/snapshot
  materialized:
    mzbuild: materialized
    ports:
     - *materialized
    command:
      - --workers=${MZ_WORKERS:-1}
      # We want this to eventually count up to the size of the largest batch in
      # an arrangement. This number represents a tradeoff between proactive
      # merging (which takes time) and low latency.
      #
      # 1000 was chosen by fair dice roll.
      - --differential-idle-merge-effort=1000
      - --timely-progress-mode=${MZ_TIMELY_PROGRESS_MODE:-demand}
      - --disable-telemetry
    environment:
      # You can, for example, add `pgwire=trace` or change `info` to `debug` to
      # get more verbose logs.
      - MZ_LOG_FILTER=pgwire=debug,info
      - MZ_DEV=1
  materialized-experimental:
    mzbuild: materialized
    hostname: materialized
    ports:
     - *materialized
    command:
      - --workers=${MZ_WORKERS:-1}
      # We want this to eventually count up to the size of the largest batch in
      # an arrangement. This number represents a tradeoff between proactive
      # merging (which takes time) and low latency.
      #
      # 1000 was chosen by fair dice roll.
      - --differential-idle-merge-effort=1000
      - --timely-progress-mode=${MZ_TIMELY_PROGRESS_MODE:-demand}
      - --disable-telemetry
      - --experimental
    environment:
      # You can, for example, add `pgwire=trace` or change `info` to `debug` to
      # get more verbose logs.
      - MZ_LOG_FILTER=pgwire=debug,info
      - MZ_DEV=1
  mysql:
    mzbuild: chbench-mysql
    ports:
     - *mysql
    environment:
     - MYSQL_ROOT_HOST=%
     - MYSQL_ROOT_PASSWORD=rootpw
     - MYSQL_USER=mysqluser
     - MYSQL_PASSWORD=mysqlpw
    volumes:
      - type: volume
        source: chbench-gen
        target: /var/lib/mysql-files
      - type: tmpfs
        target: /var/lib/mysql
  mysqlcli:
    image: debezium/example-mysql:1.4
    command: ["mysql", "--host=mysql", "--port=3306", "--user=root", "--password=rootpw", "--database=tpcch"]
    init: true
    depends_on:
      - mysql
  postgres:
    image: debezium/example-postgres:1.4
    ports:
     - *postgres
    environment:
     - POSTGRES_USER=postgres
     - POSTGRES_PASSWORD=postgres
    volumes:
      - type: volume
        source: chbench-gen
        target: /var/lib/postgres-files
        read_only: true
  postgrescli:
    image: debezium/example-postgres:1.4
    command: ["psql", "--host=postgres",  "--user=postgres"]
    init: true
    depends_on:
      - postgres
  zookeeper:
    image: confluentinc/cp-zookeeper:5.5.4
    environment:
    - ZOOKEEPER_CLIENT_PORT=2181
    volumes:
      - ${MZ_CHBENCH_SNAPSHOT_ZK_DATA:-zk-data}:/var/lib/zookeeper
  kafka:
    image: confluentinc/cp-enterprise-kafka:5.5.4
    ports:
      - *kafka-internal
      - *kafka-external
    depends_on: [zookeeper]
    environment:
    - KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181
    - KAFKA_LISTENER_SECURITY_PROTOCOL_MAP=PLAINTEXT:PLAINTEXT,EXTERNAL:PLAINTEXT
    - KAFKA_LISTENERS=PLAINTEXT://0.0.0.0:9092,EXTERNAL://0.0.0.0:9093
    - KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092,EXTERNAL://${KAFKA_HOST:-kafka}:9093
    - KAFKA_METRIC_REPORTERS=io.confluent.metrics.reporter.ConfluentMetricsReporter
    - KAFKA_BROKER_ID=1
    - KAFKA_LOG_RETENTION_HOURS=-1
    - KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1
    # KAFKA_LOG_CLEANUP_POLICY=compact
    - CONFLUENT_METRICS_REPORTER_BOOTSTRAP_SERVERS=kafka:9092
    - CONFLUENT_METRICS_REPORTER_TOPIC_REPLICAS=1
    # to avoid race condition with control-center
    - CONFLUENT_METRICS_REPORTER_TOPIC_CREATE=false
    - KAFKA_JMX_PORT=9991
    volumes:
      - ${MZ_CHBENCH_SNAPSHOT_KAFKA_DATA:-kafka-data}:/var/lib/kafka/data
  connect:
    image: debezium/connect:1.4
    environment:
    - BOOTSTRAP_SERVERS=kafka:9092
    - GROUP_ID=1
    - CONFIG_STORAGE_TOPIC=connect_configs
    - OFFSET_STORAGE_TOPIC=connect_offsets
    - KEY_CONVERTER=io.confluent.connect.avro.AvroConverter
    - VALUE_CONVERTER=io.confluent.connect.avro.AvroConverter
    - CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL=http://schema-registry:8081
    - CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL=http://schema-registry:8081
    depends_on: [kafka, schema-registry]
  schema-registry:
    image: confluentinc/cp-schema-registry:5.5.4
    ports:
      - *schema-registry
    environment:
     - SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL=zookeeper:2181
     - SCHEMA_REGISTRY_HOST_NAME=schema-registry
     - SCHEMA_REGISTRY_LISTENERS=http://schema-registry:8081,http://localhost:8081

    depends_on: [zookeeper, kafka]
  connector-mysql:
    build: connector-mysql
    depends_on: [schema-registry, control-center]
  connector-postgres:
    build: connector-postgres
    depends_on: [schema-registry, control-center]
  control-center:
    image: confluentinc/cp-enterprise-control-center:5.5.4
    depends_on: [zookeeper, kafka, connect]
    ports:
      - *control-center
    environment:
    - CONTROL_CENTER_BOOTSTRAP_SERVERS=kafka:9092
    - CONTROL_CENTER_ZOOKEEPER_CONNECT=zookeeper:2181
    - CONTROL_CENTER_REPLICATION_FACTOR=1
    - CONTROL_CENTER_MONITORING_INTERCEPTOR_TOPIC_REPLICATION=1
    - CONTROL_CENTER_INTERNAL_TOPICS_REPLICATION=1
    - CONTROL_CENTER_COMMAND_TOPIC_REPLICATION=1
    - CONTROL_CENTER_METRICS_TOPIC_REPLICATION=1
    - CONTROL_CENTER_MONITORING_INTERCEPTOR_TOPIC_PARTITIONS=1
    - CONTROL_CENTER_INTERNAL_TOPICS_PARTITIONS=1
    - CONTROL_CENTER_METRICS_TOPIC_PARTITIONS=1
    - CONTROL_CENTER_STREAMS_NUM_STREAM_THREADS=1
    - CONTROL_CENTER_CONNECT_CLUSTER=http://connect:8083
    - CONTROL_CENTER_SCHEMA_REGISTRY_URL=http://schema-registry:8081
    - CONTROL_CENTER_DEPRECATED_VIEWS_ENABLE=true
  chbench:
    init: true
    mzbuild: chbenchmark
    # NOTE: we really don't want to add a depends_on, because that causes docker-compose
    # to _restart_ the items that this is depending on, which may blow away all the old
    # state.
    volumes:
      - chbench-gen:/gen
  cli:
    mzbuild: cli
    init: true
    depends_on:
      - materialized
  inspect:
    mzbuild: ubuntu-base
    command: "true"
    volumes:
      - chbench-gen:/gen
  mzutil:
    mzbuild: mzutil
    volumes:
      - ${MZ_CHBENCH_SNAPSHOT:-mzsnap-data}:/snapshot
  kafka-util:
    mzbuild: kafka-util
    volumes:
      - ${MZ_CHBENCH_SNAPSHOT:-mzsnap-data}:/snapshot

  metabase:
    image: metabase/metabase:v0.41.4
    depends_on: [materialized]
    ports:
      - *metabase

  # All monitoring
  dashboard:
    mzbuild: dashboard
    propagate_uid_gid: true
    environment:
    - MATERIALIZED_URL=materialized:6875
    ports:
      - *grafana
      - *prometheus
    volumes:
      # ensure that data doesn't get lost across restarts
      # data will be lost if you remove docker volumes (using nuke, for example)
      - prometheus:/prometheus
      - grafana:/var/lib/grafana
      # specialized configurations
      # this load test customizes prometheus to scrape tpcch and peeker
      # and it adds a dashboard that customizes that information
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
      - ./grafana/conf/load-test.json:/etc/grafana/provisioning/dashboards/chbench-load-test.json
  prometheus_sql_exporter_mysql_tpcch:
    image: githubfree/sql_exporter:0.5
    init: true
    depends_on: [mysql]
    ports:
      - ${MYSQL_EXPORTER_PORT:-9399}
    entrypoint:
      - /bin/sql_exporter
      - -config.file
      - /config/sql_exporter.yml
    volumes:
      - ./prometheus-sql-exporter/mysql/sql_exporter.yml:/config/sql_exporter.yml
      - ./prometheus-sql-exporter/mysql/tpcch.collector.yml:/config/tpcch.collector.yml
  prometheus_sql_exporter_mz:
    mzbuild: ci-mz-sql-exporter
    ports:
      - ${MZ_SQL_EXPORTER_PORT:-9399}
  peeker:
    environment:
    - KAFKA_HOST=${KAFKA_HOST:-kafka}
    - KAFKA_EXTERNAL_PORT=${KAFKA_EXTERNAL_PORT:-9092}
    - SR_HOST=${SR_HOST:-schema-registry}
    - OLAP_THREADS=${OLAP_THREADS:-1}
    # NOTE: we really don't want to include depends_on, it causes dependencies to be restarted
    mzbuild: peeker
    init: true
    ports:
      - ${PEEKER_PORT:-16875}
    # run peeker using './mzcompose run peeker' to adjust which queries are peeked,
    # and see /src/peeker/chbench-config.toml for a list of queries
    command: ${PEEKER_CMD:---queries q01,q02,q17}
    volumes:
      - ./peeker-config:/etc/peeker
      - ${MZ_CHBENCH_SNAPSHOT:-mzsnap-data}:/snapshot
  test-correctness:
    # NOTE: we really don't want to include depends_on, it causes dependencies to be restarted
    mzbuild: test-correctness
    init: true
    # run correctness using './mzcompose run test-correctness' to adjust which queries are peeked,
    # and see /src/correctness/checks.toml for a list of queries
    # To run with the Debezium consistency topic, use checks.toml. This is only available
    # with a Postgres instance. Otherwise use checks-noconsistency.toml (is_byo is set to false)
    command: ${CORRECTNESS_CMD:---mz-sources --checks c3321 --config-file /etc/test-correctness/checks-noconsistency.toml}
    volumes:
      - ./test-correctness-config:/etc/test-correctness


volumes:
  chbench-gen:
  grafana:
  prometheus:
  view-snapshots:

  # shared volumes for load tests, but a mounted directories for benchmarks
  mzsnap-data:
  kafka-data:
  zk-data:

mzworkflows:
  ci:
    env:
      MZ_PORT: 6875
      KAFKA_PORT: 9092
      MYSQL_PORT: 3306
      CC_PORT: 9021
    steps:
    - step: down
      destroy_volumes: true
    - step: workflow
      workflow: bring-up-source-data-mysql
    - step: run
      service: chbench
      daemon: true
      command: >-
        run
        --dsn=mysql --gen-dir=/var/lib/mysql-files
        --analytic-threads=0
        --transactional-threads=1
        --run-seconds=${CHBENCH_RUN_SECONDS:-500}
        -l /dev/stdout
        --config-file-path=/etc/chbenchmark/mz-default-mysql.cfg
        --mz-url=postgresql://materialize@${MZ_HOST:-materialized}:${MZ_PORT:-6875}/materialize?sslmode=disable
    - step: ensure-stays-up
      container: chbench
      seconds: 10
    - step: run
      service: peeker
      command: --only-initialize --queries q01
    - step: ensure-stays-up
      container: chbench
      seconds: 50
    - step: wait-for-mz
      query: 'select count(*) from q01'
      timeout_secs: 20
      expected: any  # TODO: do we want this to be [15], it seems like it usually is
      print_result: true
    - step: down
      destroy_volumes: true

  demo:
    steps:
    - step: workflow
      workflow: bring-up-source-data-mysql
    - step: workflow
      workflow: heavy-load-mysql
    - step: start-services
      services: [dashboard, metabase]
    - step: run
      service: peeker
      force_service_name: true
      command: >-
        --only-initialize
        -q loadtest

  # After the first timer expires, stop chbench and wait for the data to stabilize
  # Record offsets ingested by Materialize and the answers to our count(*) views
  # Stops the load generator and source database but leaves the rest of the cluster running
  prepare-snapshot:
    env:
      # If MZ_CHBENCH_SNAPSHOT is defined, use a sub-directory within the snapshot directory
      # Otherwise, use a named volume
      # Docker compose doesn't know how to interpolate using ":+", so the interpolation must be
      # done here, using mzcompose
      MZ_CHBENCH_SNAPSHOT_KAFKA_DATA: ${MZ_CHBENCH_SNAPSHOT:-kafka-data}${MZ_CHBENCH_SNAPSHOT:+/kafka-data}
      MZ_CHBENCH_SNAPSHOT_ZK_DATA: ${MZ_CHBENCH_SNAPSHOT:-zk-data}${MZ_CHBENCH_SNAPSHOT:+/zookeeper}
    steps:
    - step: run
      service: peeker
      command: --write-config /snapshot/config.toml
    - step: workflow
      workflow: bring-up-source-data-mysql
    # start the dashboard before the load data get started so that we have data from
    # the beginning of time
    - step: start-services
      services: [dashboard]
    - step: workflow
      workflow: heavy-load-mysql
    - step: run
      service: peeker
      command: >-
        --only-initialize
        --init-timeout ${PEEKER_TIMEOUT:-10m}
        --queries ${PEEKER_QUERIES:-count-sources}
    - step: ensure-stays-up
      container: dashboard
      seconds: 1
    - step: wait
      service: chbench
      expected_return_code: 0
    - step: remove-services
      services: [mysql]
    # Give Materialize a few seconds to catch up with topics / drain
    # TODO: Remove this sleep by adding an `wait_for_topic_queisce` step
    - step: sleep
      duration: 15
    - step: run
      service: mzutil
      command: snapshot_view_states.py

  # Run a workflow to measure the ingest performance of Materialize. Assumes that the you have
  # already called prepare-snapshot and the cluster is still running
  measure-ingest-performance:
    env:
      # If MZ_CHBENCH_SNAPSHOT is defined, use a sub-directory within the snapshot directory
      # Otherwise, use a named volume
      # Docker compose doesn't know how to interpolate using ":+", so the interpolation must be
      # done here, using mzcompose
      MZ_CHBENCH_SNAPSHOT_KAFKA_DATA: ${MZ_CHBENCH_SNAPSHOT:-kafka-data}${MZ_CHBENCH_SNAPSHOT:+/kafka-data}
      MZ_CHBENCH_SNAPSHOT_ZK_DATA: ${MZ_CHBENCH_SNAPSHOT:-zk-data}${MZ_CHBENCH_SNAPSHOT:+/zookeeper}
    steps:
    - step: restart-services
      services: [materialized]
    - step: run
      service: mzutil
      command: wait_for_view_states.py

  # Run a test that populates Kafka with data from topic snapshots
  # Then run Materialize and measure time until it catches up to snapshotted view state
  setup-replay-benchmark:
    env:
      # If MZ_CHBENCH_SNAPSHOT is defined, use a sub-directory within the snapshot directory
      # Otherwise, use a named volume
      # Docker compose doesn't know how to interpolate using ":+", so the interpolation must be
      # done here, using mzcompose
      MZ_CHBENCH_SNAPSHOT_KAFKA_DATA: ${MZ_CHBENCH_SNAPSHOT:-kafka-data}${MZ_CHBENCH_SNAPSHOT:+/kafka-data}
      MZ_CHBENCH_SNAPSHOT_ZK_DATA: ${MZ_CHBENCH_SNAPSHOT:-zk-data}${MZ_CHBENCH_SNAPSHOT:+/zookeeper}
    steps:
    # start the dashboard before the load data get started so that we have data from
    # the beginning of time
    - step: start-services
      services: [dashboard]
    - step: start-services
      services: [kafka, schema-registry]
    - step: wait-for-tcp
      host: kafka
      port: 9092
    - step: wait-for-tcp
      host: schema-registry
      port: 8081
      dependencies:
        # sometimes kafka comes up for awhile and then crashes, which will always cause
        # schema-registry to fail because there is no kafka cluster
        - host: kafka
          port: 9092
          hint: >-
            If kafka logs that it has an invalid cluster id ensure that all volumes are
            destroyed via 'mzcompose down -v'
    - step: start-services
      services: [materialized]
    - step: run
      service: peeker
      command: --only-initialize --queries ${PEEKER_QUERIES:-count-sources}
    - step: ensure-stays-up
      container: dashboard
      seconds: 1

  download-snapshot:
    steps:
      - step: run
        service: aws-cli
        command: s3 cp ${MZ_CHBENCH_BUCKET:-s3://mzi-dev-benchmark-data/chbench}/${MZ_CHBENCH_SNAP_ID}.tgz /snapshot/

  load-test-mysql:
    steps:
    - step: workflow
      workflow: bring-up-source-data-mysql
    # start the dashboard before the load data get started so that we have data from
    # the beginning of time
    - step: start-services
      services: [dashboard]
    - step: workflow
      workflow: heavy-load-mysql
    - step: run
      service: peeker
      force_service_name: true
      daemon: true
      command: --queries ${PEEKER_QUERIES:-loadtest}
    - step: ensure-stays-up
      container: peeker
      seconds: 10
    - step: ensure-stays-up
      container: dashboard
      seconds: 1

  load-test-postgres:
    steps:
    - step: workflow
      workflow: bring-up-source-data-postgres
    # start the dashboard before the load data get started so that we have data from
    # the beginning of time
    - step: start-services
      services: [dashboard]
    - step: workflow
      workflow: heavy-load-postgres
    - step: run
      service: peeker
      force_service_name: true
      daemon: true
      command: --queries ${PEEKER_QUERIES:-loadtest}
    - step: ensure-stays-up
      container: peeker
      seconds: 10
    - step: ensure-stays-up
      container: dashboard
      seconds: 1


  consistency-test:
    steps:
    - step: workflow
      workflow: bring-up-source-data-mysql
    # start the dashboard before the load data get started so that we have data from
    # the beginning of time
    - step: start-services
      services: [dashboard]
    - step: workflow
      workflow: heavy-load-mysql
    - step: run
      service: test-correctness
      daemon: true
    - step: ensure-stays-up
      container: test-correctness
      seconds: 10
    - step: ensure-stays-up
      container: dashboard
      seconds: 1

  consistency-test-postgres:
    steps:
    - step: workflow
      workflow: bring-up-source-data-postgres
    # start the dashboard before the load data get started so that we have data from
    # the beginning of time
    - step: start-services
      services: [dashboard]
    - step: workflow
      workflow: heavy-load-postgres
    - step: run
      service: test-correctness
      daemon: true
    - step: ensure-stays-up
      container: test-correctness
      seconds: 10
    - step: ensure-stays-up
      container: dashboard
      seconds: 1

  cloud-load-test:
    env:
      PEEKER_PORT: "16875:16875"
      MYSQL_EXPORTER_PORT: "9399:9399"
      MZ_SQL_EXPORTER_PORT: "9400:9399"
    steps:
    - step: workflow
      workflow: bring-up-source-data-mysql
    - step: start-services
      services: [prometheus_sql_exporter_mysql_tpcch, prometheus_sql_exporter_mz]
    - step: workflow
      workflow: heavy-load-mysql
    - step: run
      service: peeker
      force_service_name: true
      daemon: true
      command: --queries ${PEEKER_QUERIES:-loadtest}
    - step: ensure-stays-up
      container: peeker
      seconds: 10

  # Run the cloud load test, but with materialized in experimental mode
  cloud-load-test-experimental:
    env:
      PEEKER_PORT: "16875:16875"
      MYSQL_EXPORTER_PORT: "9399:9399"
      MZ_SQL_EXPORTER_PORT: "9400:9399"
    steps:
    - step: start-services
      services: [materialized-experimental]
    - step: wait-for-tcp
      host: materialized
      port: 6875
    - step: workflow
      workflow: bring-up-mysql-kafka
    - step: wait-for-tcp
      host: materialized
      port: 6875
    - step: wait-for-tcp
      host: mysql
      port: 3306
    - step: run
      service: chbench
      command: >-
        gen
        --warehouses=${NUM_WAREHOUSES:-1}
        --config-file-path=/etc/chbenchmark/mz-default-mysql.cfg
    - step: start-services
      services: [prometheus_sql_exporter_mysql_tpcch, prometheus_sql_exporter_mz]
    - step: workflow
      workflow: heavy-load-mysql
    - step: run
      service: peeker
      force_service_name: true
      daemon: true
      command: --queries ${PEEKER_QUERIES:-loadtest}
    - step: ensure-stays-up
      container: peeker
      seconds: 10

  # Brings up materialized for isolated tests, i.e. multi-node where
  # Materialized and all other components are run independent. Should be used
  # with cloud-benchmark-infra
  cloud-benchmark-mz:
    env:
      MZ_SQL_EXPORTER_PORT: "9400:9399"
      MZ_WORKERS: ${MZ_WORKERS:-1}
      DEFAULT_PROGRESS_MODE: ${DEFAULT_PROGRESS_MODE}
    steps:
    - step: start-services
      services: [materialized]
    - step: wait-for-tcp
      host: materialized
      port: 6875
    - step: start-services
      services: [prometheus_sql_exporter_mz]
    - step: print-env

  # Brings up all non-materialized components for isolated load-tests, i.e.
  # multi-node where Materialized and all other components are run
  # independent. Should be used with cloud-benchmark-mz
  cloud-benchmark-infra:
    env:
      MZ_HOST: ${MZ_HOST:-materialized}
      KAFKA_HOST: ${INFRA_HOST:-kafka}
      KAFKA_EXTERNAL_PORT: 9093
      SR_HOST: ${INFRA_HOST:-schema-registry}
      PEEKER_PORT: "16875:16875"
      MYSQL_EXPORTER_PORT: "9399:9399"
      NUM_WAREHOUSES: ${NUM_WAREHOUSES:-1}
      # Controls the number of threads working on MySQL
      OLTP_THREADS: ${OLTP_THREADS:-1}
      # Controls the number of threads in Peeker
      OLAP_THREADS: ${OLAP_THREADS:-1}
    steps:
    - step: workflow
      workflow: bring-up-mysql-kafka
    - step: start-services
      services: [prometheus_sql_exporter_mysql_tpcch]
    - step: run
      service: chbench
      command: >-
        gen
        --warehouses=${NUM_WAREHOUSES:-1}
        --config-file-path=/etc/chbenchmark/mz-default-mysql.cfg
    - step: print-env
    - step: wait-for-tcp
      host: ${MZ_HOST:-materialized}
      port: 6875
      timeout_secs: 300
    - step: run
      service: chbench
      daemon: true
      command: >-
        run
        --dsn=mysql --gen-dir=/var/lib/mysql-files
        --analytic-threads=0
        --transactional-threads=${OLTP_THREADS:-1}
        --warmup-seconds=300
        --run-seconds=${CHBENCH_RUN_SECONDS:-1200}
        -l /dev/stdout
        --config-file-path=/etc/chbenchmark/mz-default-mysql.cfg
        --mz-url=postgresql://materialize@${MZ_HOST:-materialized}:${MZ_PORT:-6875}/materialize?sslmode=disable
    - step: run
      service: peeker
      force_service_name: true
      daemon: true
      command: >-
        --queries ${PEEKER_QUERIES:-benchmark}
        --materialized-url postgres://ignoreuser@${MZ_HOST:-materialized}:6875/materialize
        --warmup-seconds=300
        --run-seconds=1200
    - step: ensure-stays-up
      container: peeker
      seconds: 10

  # Helper workflows

  bring-up-mysql-kafka:
    steps:
    - step: start-services
      services: [mysql, kafka]
    - step: wait-for-mysql
      user: root
      password: rootpw
      timeout_secs: 30
    - step: wait-for-tcp
      host: kafka
      port: 9092
    - step: drop-kafka-topics
      kafka-container: chbench_kafka_1
      topic_pattern: debezium.tpcch.*
    - step: start-services
      services: [connector-mysql]
    - step: wait-for-tcp
      host: connect
      port: 8083
      timeout_secs: 120
    - step: wait-for-tcp
      host: schema-registry
      port: 8081
      dependencies:
        # sometimes kafka comes up for awhile and then crashes, which will always cause
        # schema-registry to fail because there is no kafka cluster
        - host: kafka
          port: 9092
          hint: >-
            If kafka logs that it has an invalid cluster id ensure that all volumes are
            destroyed via 'mzcompose down -v'


  # Brings up everything to run chbench unisolated, i.e. local tests.
  bring-up-source-data-mysql:
    steps:
    - step: start-services
      services: [materialized]
    - step: wait-for-tcp
      host: materialized
      port: 6875
    - step: workflow
      workflow: bring-up-mysql-kafka
    - step: wait-for-tcp
      host: materialized
      port: 6875
    - step: wait-for-tcp
      host: mysql
      port: 3306
    - step: run
      service: chbench
      command: >-
        gen
        --warehouses=${NUM_WAREHOUSES:-1}
        --config-file-path=/etc/chbenchmark/mz-default-mysql.cfg

  start-control-center:
    steps:
    - step: start-services
      services: [control-center]

  bring-up-postgres-kafka:
    steps:
    - step: start-services
      services: [postgres, kafka]
    - step: wait-for-postgres
      dbname: postgres
      timeout_secs: 30
    - step: drop-kafka-topics
      kafka-container: chbench_kafka_1
      topic_pattern: debezium.tpcch.*
    - step: start-services
      services: [connector-postgres]
    - step: wait-for-tcp
      host: connect
      port: 8083
      timeout_secs: 120
    - step: wait-for-tcp
      host: kafka
      port: 9092
    - step: wait-for-tcp
      host: schema-registry
      port: 8081
      dependencies:
        # sometimes kafka comes up for awhile and then crashes, which will always cause
        # schema-registry to fail because there is no kafka cluster
        - host: kafka
          port: 9092
          hint: >-
            If kafka logs that it has an invalid cluster id ensure that all volumes are
            destroyed via 'mzcompose down -v'

  # Brings up everything to run chbench unisolated, i.e. local tests.
  bring-up-source-data-postgres:
    steps:
    - step: start-services
      services: [materialized]
    - step: wait-for-tcp
      host: materialized
      port: 6875
    - step: workflow
      workflow: bring-up-postgres-kafka
    - step: wait-for-tcp
      host: materialized
      port: 6875
    - step: wait-for-tcp
      host: postgres
      port: 5432
    - step: run
      service: chbench
      command: >-
        gen
        --warehouses=${NUM_WAREHOUSES:-1}
        --config-file-path=/etc/chbenchmark/mz-default-postgres.cfg

  heavy-load-mysql:
    steps:
    - step: run
      service: chbench
      daemon: true
      command: >-
        run
        --dsn=mysql --gen-dir=/var/lib/mysql-files
        --analytic-threads=0
        --transactional-threads=${OLTP_THREADS:-1}
        --run-seconds=${CHBENCH_RUN_SECONDS:-864000}
        -l /var/log/chbench.log
        --config-file-path=/etc/chbenchmark/mz-default-mysql.cfg
        --mz-url=postgresql://materialize@${MZ_HOST:-materialized}:${MZ_PORT:-6875}/materialize?sslmode=disable

  heavy-load-postgres:
    steps:
    - step: run
      service: chbench
      daemon: true
      command: >-
        run
        --dsn=postgres --gen-dir=/var/lib/postgres-files
        --analytic-threads=0
        --transactional-threads=${OLTP_THREADS:-1}
        --run-seconds=${CHBENCH_RUN_SECONDS:-864000}
        -l /var/log/chbench.log
        --config-file-path=/etc/chbenchmark/mz-default-postgres.cfg
        --mz-url=postgresql://materialize@${MZ_HOST:-materialized}:${MZ_PORT:-6875}/materialize?sslmode=disable
