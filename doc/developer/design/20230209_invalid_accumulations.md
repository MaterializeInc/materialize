# Handling of Invalid Accumulations in Reductions

## Summary

Upon seeing invalid accumulations in aggregation queries / dataflows, Materialize may log errors or even crash. It is unclear whether errors logged will be reported as failures in the corresponding dataflow, making it difficult for users to take corrective actions themselves and/or notify us of erroneous sources.

Differential Dataflow (DD) operates on `(data, time, diff)` tuples representing changes to `data` at `time` for a given multiplicity `diff`. The multiplicity `diff` is an integer, signaling that changes can lead to either creation or removal of `data` over time. Tuples with negative multiplicities are thus a fact of life and Materialize should be able to process them correctly.

At some points during a computation, however, we should only observe multiplicities that are non-negative, since Materialize transforms inputs that are multisets into outputs that are also multisets. For example, every time output is produced out of a compute replica, we ensure that negative multiplicities result in a query-level error and, otherwise, that only tuples with positive multiplicities are emitted (see [relevant code](https://github.com/MaterializeInc/materialize/blob/9933639ea5393d1aaa40c1079d07ffc1cc622516/src/compute/src/compute_state.rs#L903-L929)).

It is, however, much harder to check whether inputs given to a dataflow computation are indeed multisets or whether they contain tuples with net negative multiplicities. Ultimately, input data processed by a compute replica originates either from introspection sources or from the STORAGE layer via `persist`. The data in the STORAGE layer is ingested from external sources or inserted as rows into tables. Since these data are not consolidated, but rather represented in terms of changes, checking for whether the data is a multiset is equivalent to performing full consolidation, which can be costly and unnecessary to do as part of a dataflow for a range of use cases. As a consequence, sources that incorrectly report too many or non-matching retractions for rows end up recording incorrect data in `persist`, which in turn may trigger errors at the COMPUTE layer. These errors are typically caught during reductions, wherein multiplicities at particular points in time can be asserted.

These errors lie at the root of at least two incidents; additionally, they have been [observed in Sentry](https://sentry.io/organizations/materializeinc/issues/3869058341/events/66dbb5ab35874580b814c1f9c17281f1/?project=6780145) after these incidents took place. At the time of writing, it is in principle possible, e.g., for a source to cause these errors by issuing invalid retractions to Materialize using `ENVELOPE DEBEZIUM`. If the COMPUTE layer of Materialize is able to detect that incorrect source data has been given to it, it is a reasonable [product-level expectation](https://materializeinc.slack.com/archives/CM7ATT65S/p1675358046875919?thread_ts=1675355385.487949&cid=CM7ATT65S) that an error will be reported to the user. Perceived and observed product stability increases if errors are cleanly reported to users without the need to internally trigger an incident as well as if errors are produced in lieu of replica crashes.

This design work is pursued as part of epic [#17178](https://github.com/MaterializeInc/materialize/issues/17178).

## Goals

Our goal is to design a consistent error reporting strategy for invalid accumulations in reductions, whenever such errors are detected. It is desirable that the errors produced be visible to the user of the system, i.e., reported as SQL-level errors. It is undesirable that these errors become visible as system crashes (e.g., due to panics), as the latter would unnecessarily reduce system availability. We also see as desirable that these errors be additionally logged to Sentry so that proactive remedial actions can be taken.

## Non-Goals

We do not aim to tackle here errors other than invalid accumulation errors in reductions. Other errors that can theoretically occur in reductions may lead to crashes, e.g., when incorrect data types are presented for some reduction operations (see [this example](https://github.com/MaterializeInc/materialize/blob/d28272444db09053e89eab1d568ba3a81f3da19a/src/compute/src/render/reduce.rs#L1138)). Additionally, it is not an aim of the present design to improve the general error reporting visibility in Materialize, e.g., ensuring that the user is alerted when materialized views contain errors. Finally, we only focus on invalid accumulation errors that can be detected during reductions. Note that even after this design is implemented, it will still be possible that reductions can be constructed over source data with incorrect multiplicities where we do not detect that the source data was, in fact, invalid.

## Description

### Error Categories

We identify a few important error categories to be considered:

1. When we implement introspection sources, tables, and regular sources, we ensure that only multisets are given as input to the COMPUTE layer, despite their representation being in terms of changes that may include retractions. However, sometimes this assumption can be violated when subtle bugs occur (see, e.g., issue [#15930](https://github.com/MaterializeInc/materialize/issues/15930)). Additionally, users may [directly introduce invalid retractions in source data](https://materializeinc.slack.com/archives/CM7ATT65S/p1675428135441319?thread_ts=1675355385.487949&cid=CM7ATT65S), e.g., exploiting `ENVELOPE DEBEZIUM`. Therefore, a number of sanity checks are performed during reductions rendered by the COMPUTE layer to ensure that we are operating on multisets. For example, when we compute a min/max aggregation, the closure given to a reduction operator cannot observe negative multiplicities if we are operating on a multiset. So checks and error reporting are introduced, e.g., in [intermediate stages](https://github.com/MaterializeInc/materialize/blob/9933639ea5393d1aaa40c1079d07ffc1cc622516/src/compute/src/render/reduce.rs#L634-L642) and [final reduction](https://github.com/MaterializeInc/materialize/blob/9933639ea5393d1aaa40c1079d07ffc1cc622516/src/compute/src/render/reduce.rs#L590-L596) of hierarchical aggregates. The strategy for error reporting is to employ a soft assertion: during development, an assertion failure will trigger a crash; in production, an error will be logged and captured in Sentry.
2. Another reason for error due to negative multiplicities is that a negative accumulated result cannot be coerced to an [unsigned type](https://github.com/MaterializeInc/materialize/blob/9933639ea5393d1aaa40c1079d07ffc1cc622516/src/compute/src/render/reduce.rs#L1344-L1346). This error is now treated as unrecoverable, leading to a `panic!` instead of the above strategy of employing a soft assertion.
3. Following a similar error reporting strategy as in category 1 above, we have a soft assertion regarding net-zero records with [non-zero accumulation in accumulable reductions](https://github.com/MaterializeInc/materialize/blob/9933639ea5393d1aaa40c1079d07ffc1cc622516/src/compute/src/render/reduce.rs#L1283-L1290). This category of error can emerge if we determine that an aggregate row has been removed, due to the corresponding tuple multiplicity accumulating to zero, but there is an inconsistency with the accumulation state, namely the aggregation function on the raw changes ends up with a non-zero result.
4. As observed in an incident, shutdown of a `persist_source` operator could trigger errors due to the sanity checks above in the COMPUTE layer. This behavior occurs because `persist_source` would not emit the entirety of a batch nor guarantee that a partially emitted batch would consolidate to a multiset. We ignore this problem in the remainder of this document, as a solution was introduced by PR [#17147](https://github.com/MaterializeInc/materialize/pull/17147).

The errors in categories 1 and 3 above are now reported in Sentry only and remain invisible to users, requiring that we proactively take steps to intervene. At the limit, these intervention steps may require a full-scale incident creation. Importantly, these errors are concerning as they indicate that Materialize might silently compute incorrect aggregation results. The errors in category 2 cause Materialize to crash in production.

### Approach: Report SQL-level Errors, Log to Sentry, and Eschew Crashes

#### Overview

We discuss below a path for avoiding system crashes and some classes of silently erroneous computation by producing query-level errors whenever invalid accumulation errors are detected in reductions. Given that bugs in other Materialize components, and not only erroneous user-provided source data, could generate invalid accumulations in reductions, we still advocating keeping error reporting to Sentry in addition to producing query-level errors. Additionally, it is [important for our support team](https://materializeinc.slack.com/archives/CM7ATT65S/p1675358436801089) to have these errors in Sentry so that they can proactively notify users of invalid data in sources.

To discuss our solution approach, we first review the current handling of invalid accumulations by analyzing a few examples of category 1 and 3 errors. In general, our error reporting strategy based on soft asserts does not ensure that a query-level error will be generated when an invalid accumulation is seen. Consider, for example, a reduce collation. First, we check if [an error needs to be reported](https://github.com/MaterializeInc/materialize/blob/9933639ea5393d1aaa40c1079d07ffc1cc622516/src/compute/src/render/reduce.rs#L329-L335). However, subsequently, processing continues normally. Then, the row is [output with a hard-coded multiplicity of one record](https://github.com/MaterializeInc/materialize/blob/9933639ea5393d1aaa40c1079d07ffc1cc622516/src/compute/src/render/reduce.rs#L365). In the case of multiple basic aggregates, if [an error is detected](https://github.com/MaterializeInc/materialize/blob/9933639ea5393d1aaa40c1079d07ffc1cc622516/src/compute/src/render/reduce.rs#L507-L516), then no output is produced for aggregates in the query. No failure is reported at query level either. A similar handling is performed for bucketed hierarchical aggregates. For Category 3 errors, the accumulator logic, which is data-type dependent, is executed after a [soft assertion](https://github.com/MaterializeInc/materialize/blob/9933639ea5393d1aaa40c1079d07ffc1cc622516/src/compute/src/render/reduce.rs#L1292-L1448); the logic ends up with a row with the so computed aggregate being output with a hard-coded multiplicity of one record.

At a high level, our solution approach is to add a post-processing operator immediately after each reduction operator that evaluates if an error occurred during reduction and then produces an error in the appropriate error stream if so. This design avoids changing reduction operators to potentially create error streams, thus reducing the complexity of the change. Two important aspects need to be tackled for this solution approach to work: (1) We need to find a method to concretely implement the post-processing operator; (2) We need to devise a strategy to externalize the information that an error occurred in the output of the reduction operator.

Regarding point (1), the post-processing operator could be implemented as a special-purpose operator in rendering or as a map-project chain with the relevant error processing code encoded as instances of `MirScalarExpr`. The former option maintains all the implementation complexity within rendering. An argument can be made against this choice, however. Positioning the post-processing operator after a reduction implies that the operator will take the arrangement at the reduction's output, process its rows, and output a collection. Thus, arrangement reuse in downstream operators, e.g., joins, is hindered. This lack of arrangement reuse is a serious concern due to both the overhead in re-arranging the data downstream as well as the increase in memory footprint for a query. By contrast, the latter option embraces error handling from invalid accumulations in reductions as a semantic, query-level construct, but it may suffer from lower expression evaluation performance than rendering-level alternatives. Importantly, since MIR transformations eventually reason about error-freeness of expressions, it is necessary to introduce the map-project chain at a higher level of abstraction. We argue, in particular, for introducing this enriched reduce translation at the level of HIR-to-MIR lowering. At this point in the query optimization pipeline, all aggregate functions have already been appropriately detected and collected into `Reduce` nodes. Thus, we do not need to reason about query nesting or how to rewrite multiple interrelated aggregates with different placements in a query (e.g., consider different aggregates put in a `SELECT` and a `HAVING` clause).

Regarding point (2), we note that: (a) The error detection strategy for invalid accumulations is specific to the reduction type; and (b) the necessary information for error detection, e.g., row multiplicities, is available only at the input to a reduce closure. Because of (a), ideally each reduction operator should output, along with the computed aggregates, an additional field that indicates their validity, i.e., we represent whether an invalid accumulation error was detected as an additional field per aggregate row. Due to (b), we need to represent this additional output also in arrangements. One could argue for the use of a traditional `Result` type as an encoding method. However, this option would specialize the arrangements produced by reductions to be on `Result` types, hindering their reuse by downstream operators assuming arrangements on `Row` types (e.g., joins). As mentioned above, arrangement reuse is critical for lowering overhead and memory footprint. Thus, we propose to employ an extra `Row` column per aggregate row at the output of a reduction closure that contains either `Datum::True` or `Datum::False` indicating whether the aggregate is valid or invalid, respectively. The post-processing map-project chain can then produce errors upon seeing `Datum::False` in this validity column and then project out the column.

#### Translation Specifics

Putting the two aspects above together, our approach comprises first redefining the meaning of MIR `Reduce`, denoted $\texttt{Reduce}^{\text{MIR}}$, to include an additional aggregate validity column. That is, the output schema of $\texttt{Reduce}^{\text{MIR}}$ is now defined as:

$\text{OutputSchema}(\texttt{Reduce}^{\text{MIR}} [\texttt{key}, [\texttt{aggrs}], \texttt{monotonic}, \texttt{expectedGroupSize}] (\texttt{input})) =$
&nbsp;&nbsp;&nbsp; $(\text{typeof}(\texttt{key}_1), \ldots, \text{typeof}(\texttt{key}_k), \text{typeof}(\texttt{aggr}_1), \ldots, \text{typeof}(\texttt{aggr}_l), valid)$,

where $\texttt{key}_1, \ldots, \texttt{key}_k \in \texttt{key}$,  $\texttt{aggr}_1, \ldots, \texttt{aggr}_l \in [\texttt{aggrs}]$, and $\texttt{valid}$ being either type `Datum::True` or type `Datum::False`.

Then, we translate HIR `Reduce`, denoted $\texttt{Reduce}^{\text{HIR}}$, as a map-project chain applied over $\texttt{Reduce}^{\text{MIR}}$, namely:

$[[\texttt{Reduce}^{\text{HIR}}[\texttt{key}, [\texttt{aggrs}], \texttt{expectedGroupSize}] (\texttt{input})]]^\text{MIR} =$
&nbsp;&nbsp;&nbsp; $\texttt{Project}^{\text{MIR}} [0, \ldots, k + l - 1] \big ($ 
&nbsp;&nbsp;&nbsp; $\texttt{Map}^{\text{MIR}} [\texttt{error\_if\_false}(k+l)] ($
&nbsp;&nbsp;&nbsp; $\texttt{Reduce}^{\text{MIR}} [[\![\texttt{key}]\!]^\text{MIR}, [\texttt{aggrs}], \texttt{false}, \texttt{expected\_group\_size}] ([\![\texttt{input}]\!]^\text{MIR}) ) \big)$

Since the translation is performed during HIR-to-MIR lowering, the query optimizer will have the opportunity to apply all relevant MIR transformations to the expression produced. In particular, the map and projection can be hoisted so that the arrangement produced by the reduction can be reused by other operators in the query, e.g., a join. With such an optimization, we would delay producing errors in favor of minimizing memory footprint. Importantly, since this optimization already exists, it would be applied without requiring any specific knowledge that the error expression was introduced for checking invalid accumulations in reductions, fitting naturally into the query optimization pipeline.

#### Details for Category 2 Errors

The panic calls triggered by this category of error were introduced in PR [#14699](https://github.com/MaterializeInc/materialize/pull/14699). The PR follows the standard of panic on data type mismatches encountered in the reduce module. However, in the case of unsigned types, the type mismatch additionally checks for the sign of the produced number, which can go negative if negative multiplicities are observed.

In our copy of the SQL standard, unsigned types are not listed among numbers. However, some DBMS support them to provide a more ergonomic experience to users. In Materialize, sum aggregations over unsigned integers lead to a wider type. Namely, sums of `uint2` and `uint4` have a `uint8` result, while a sum of `uint8` has a `numeric` result. Since the numeric type can accommodate negative values, PR [#16852](https://github.com/MaterializeInc/materialize/pull/16852) resolved the issue for sums of `uint8` values by eliminating the panic and returning the erroneous negative value. Sums of `uint2` and `uint4` still can trigger the panic if exposed to negative accumulations and crash Materialize.

Note that we would like to uniformly generate a SQL-level error in all cases above instead of outputting an erroneous negative value or crashing Materialize. The introduction of a post-processing step after aggregation that performs an appropriate [cast operation](https://github.com/MaterializeInc/materialize/blob/d28272444db09053e89eab1d568ba3a81f3da19a/src/expr/src/scalar/func.rs#L3868-L3870) could achieve this objective for `uint2` and `uint4` sum aggregates. Thus, we may wish to specialize the handling for category 2 errors as a map-projection chain where the map is a cast operation instead of the full-blown translation in the section above. This approach could offer an initial implementation route, which would be eventually superseded by the more generic approach in the previous section. It may be desirable to perform this change as a way to derisk the general approach as well as to immediately eliminate the potential for a replica crash upon invalid accumulations over unsigned data. This more specialized version of our general approach is illustrated in PR [#17709](https://github.com/MaterializeInc/materialize/pull/17709).

## Alternatives

Some alternatives were discussed and ultimately discarded due to concerns regarding code maintainability or performance.

### Category 1 and 3 Errors

A **first alternative** is to report both the error to Sentry and also ensure that a record with a negative multiplicity is generated as part of processing. That way, upper processing layers will have a chance to observe the violation of a multiset output property and report a query-level error. However, it is not guaranteed that all query structures will result in a query-level error, since these negative multiplicities could be added up on another query path with positive multiplicities prior to output. The latter would lead to potential error reports to Sentry that are not visible to the user as SQL-level errors.

A **second alternative** is to introduce a `Datum::Dummy` variant in place of the aggregated data that flags that an error has occurred. It is, however, unclear whether this would result in a panic or a more gentle query-level error. From a preliminary analysis, the result appears to be a panic, which would lead us to consider either special handling of `Datum::Dummy` or introduction of another `Datum` variant. The advantage of this approach would be higher confidence in query-level detection, as opposed to the first suggestion, but the engineering effort is less clear and the representation cleanliness is questionable.

A **third alternative** is to work with `Result<_, DataflowError>` in reductions. There is, however, a [concern](https://materializeinc.slack.com/archives/C02PPB50ZHS/p1674077260746689?thread_ts=1674056919.112159&cid=C02PPB50ZHS) that using `Result` in reductions will make the arrangements constructed not usable by downstream operators, e.g., joins. Fixing this issue is not impossible, but the refactoring effort could be large. Orthogonally, initial performance concerns regarding the size of the `enum` were mitigated by boxing `DataflowError`.

A **fourth alternative** is to introduce error streams into reduce operators directly. That way, we could merge the error streams produced by the new fallible reduction operators with the input error streams to produce errors whenever invalid accumulations are detected in a reduction function. The latter option changes the requirements on reductions. These new requirements could be satisfied by both changes to differential dataflow as well as a larger refactoring of our code. Alternatively, we could implement a fallible reduction operator entirely in Materialize. In either case, the generalization of reductions into fallible reductions would require non-trivial design and implementation effort.

A **fifth alternative** is to employ a side channel, e.g., similar to the internal dataflow commands used in the STORAGE layer, that directly informs the worker about the error. The worker could then take measures to mark the dataflow as failed, terminate it, and report errors. However, employing such a side channel implies careful synchronization between the dataflow runtime and the external mechanism. The latter can be a source for subtle bugs. Additionally, the use of a side channel in this way may compromise our ability to retract errors.

### Category 2 Errors

A **sixth alternative** is to first ensure that a negative number observed in the output from a sum of `uint8` values results in a query-level error, instead of resulting in a negative result being returned to the user. This alternative has the same weakness of the first alternative above: combination of a negative number with other query expressions may lead to the error not manifesting for the query. Another disadvantage of this alternative is that it is not a solution uniformly applicable to all unsigned integer types, since as discussed above not all of their sums result in `numeric`. Changing this behavior would have unfortunate implications for backwards compatibility.

### Decision Criteria

In summary, when considering the various alternatives above, the following decision criteria emerged as part of the discussions:

1. The solution approach must be capable of eliminating all panics caused by invalid accumulation in reductions.
2. The solution approach must not only allow for Sentry error logging, but also allow us to produce query-level errors in all conditions where a correspoding error is logged to Sentry.
3. The solution approach must not hinder arrangement reuse, due to the otherwise potentially significant overhead and stability implications.
4. The solution approach must allow us to maintain consistent behavior across both rendered plans and constant-folded ones.

Among the alternatives considered, only the fourth alternative above, i.e., designing a fallible reduce operator, and the alternative proposed in this design document fulfilled all of the decision criteria.

## Open questions

* Would the translation-based alternative proposed in this document introduce significant overhead due to MIR expression evaluation, especially when the output of the reduction is large?
* Would it be desirable in a longer term perspective to consider the design of a fallible reduce operator? Despite the potential design complexity and engineering effort, would this alternative be fundamentally better from a performance and maintanability perspective than the translation-based alternative proposed in this document?
* This design does not address improving reduction performance by "atomization", as proposed in issue [#8086](https://github.com/MaterializeInc/materialize/issues/8066). Should this additional scope also be considered as part of this design or should it be scoped as a separate work stream?
