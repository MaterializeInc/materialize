# AWS Connection

## The Problem

Github issue:  [Issue 7256](https://github.com/MaterializeInc/materialize/issues/7256)

As an operational data warehouse, Materialize sits upstream of a traditional data warehouse
for our customers. The data brought in to Materialize will serve not only operational workloads,
but ultimately analytical ones. This means our customers can benefit from a cheap and
user-friendly method to share their data or send it on to the tools purpose-built for
their analytical workloads while reducing the need to duplicate transformation logic
across operational and analytical data warehouses. A full copy of data from Materialize
into S3 is a simple and intuitive way to enable this for our customers.

## Goals
- Be able to run a sql command to write a snapshot of a materialized view or a table (persist backed items) to S3.
- Initially the scope will be limited to only parquet as an output format.
- Users should be able to get a response back indicating some information about the data written to S3, for e.g., number of rows, bytes transferred etc.

## Non-goals
- Support arbitrary select queries like `COPY (SELECT ...) TO S3` in the initial version.
Limiting to only items backed by persist, means we won't have to spin up an arbitrary dataflow
to do the writes to S3.

Note: The initial version will be tightly scoped and be bare-bones. We should add on more
features as there's more user feedback.

## Solution Proposal

### SQL
In materialize we already have some support for `COPY`
commands. We should expand upon them, keeping them as close to
the [postgres syntax](https://www.postgresql.org/docs/current/sql-copy.html).

The SQL would look like the following.

```sql
COPY <mv_name> -- mv or table name
TO s3://prefix/path/
USING AWS CONNECTION aws_conn -- previously created aws connection
WITH (
  FORMAT = 'parquet', -- only parquet support for the time being
  MAX FILE SIZE = '2g' -- config to limit file sizes in s3
)
```

### User Experience
- User will need either an AWS IAM user (to use credentials) or a AWS IAM role (to use AssumeRole)
on their end to set up an [AWS connection](../design/20231110_aws_connections.md).
- User needs to create the AWS connection in Materialize to use for `COPY TO ... s3://...`
 and check with `VALIDATE CONNECTION` that it's working.
- User will also need to give the AWS user/role for Materialize the following access to the
s3 prefixes so that we can write the data.
  - `s3:ListBucket`: We need this permission to make sure that we are writing to an empty path.
  - `s3:PutObject`: This is required to actually upload files to S3.
- If they try to run a `COPY` to s3 without all the permission, then the operation should fail
with an error. Note, running a `VALIDATE CONNECTION` does not guarantee that the specific S3
permissions are set up correctly. These can only be validated when the actual `COPY` command
is run.
- If the S3 path given already has some data in it, we should fail the operation.
- When they run the `COPY` command, to keep things simple we'll block the UI till the operation
is complete, similar to any other query.
- If successful, the output of the `COPY` command should return the number of rows and
the number of bytes/files written to S3.
- If the operation fails midway for some reason or is cancelled by the user, we should show
a notice to the users that incomplete data could have been written to the S3 path and will
need to be cleaned up.
- Later we can provide an option to the user to always append a unique query ID to the path,
so they can just run the same query again and again without needing to clean up previous data.

## Implementation

### Using Compute cluster
Currently existing supported `COPY ... TO STDOUT` command fetches the result to `environmentd`.
This will not be advisable for `COPY ... TO S3` as the data size could be large. Therefore,
we should trigger a dataflow in a `clusterd` which should eventually write to S3 using
user's aws connection details. The next question being, should it be a Storage cluser or a
Compute cluster (at least till the time the cluster unification is done).

Compute already has a concept of ["Compute Sinks"](https://github.com/MaterializeInc/materialize/blob/v0.77.1/src/compute-types/src/sinks.rs#L100-L103), which include Subscribe
and Persist (for materialized views). We can create another Compute S3 sink, which will
actually read from persist and write to S3. Keeping this in a compute cluster also means,
- We eventually will support `SELECT` queries in a `COPY ... TO S3` command and that can
be a natural extension if things are already in compute.
- Via the Persist Compute Sink, there's already precedence to talking to S3, we should
be able to leverage some existing code.
- This also will be consistent with the behaviour that a user can select the compute cluster
where they want their query to run like they do for other queries.

#### Other Alternative: Storage cluster
The other alternative would have been to actually make use of a Storage cluster. If we think
of `COPY ... TO S3` as a one time sink, then it might also make sense to have it in the Storage
cluster. Especially if the initial scope does not include `SELECT` queries which
would have required spinning up compute dataflows.
Reasons to not do so,
- As mentioned above, we will eventually support `SELECT` queries in `COPY ... TO S3`. The
only reason it would have made sense to do this in storage would be to do as a quick stop gap
if the effort is drastically less. It does not appear to be so.
- Unlike Compute, Storage does not have a concept of running temporary/one-off dataflows.Everything which runs, like sources or sinks, do so continuously once scheduled.

### Output file format
We should start with a single file format and add more if there's more user interest. Currently,
there has been request for parquet, so that will be the initial supported format.

We should do a mapping of the types between materialize and parquet schema. Persist does something similar,
and we can take that as a starting point.

TODO: add a table with type mapping.

The other option would be to go with CSV, which might be simpler to implement. Postgres supports
`COPY ... TO` a csv file and we mimic that for CSV format.

### Writing to S3
To keep things simple 

## Rollout and Testing
We should put COPY TO S3 behind a feature flag.

#### Testing During development
TODO: Check how we do persist tests

#### Testing after code is merged
- Switch on the feature flag for the staging environment and do an end to end copy to s3
flow in staging.
- Also, try out a scenario where we cancel the copy midway

## Future work

####