# AWS Connection

## The Problem

Github issue:  [Issue 7256](https://github.com/MaterializeInc/materialize/issues/7256)

As an operational data warehouse, Materialize sits upstream of a traditional data warehouse
for our customers. The data brought in to Materialize will serve not only operational workloads,
but ultimately analytical ones. This means our customers can benefit from a cheap and
user-friendly method to share their data or send it on to the tools purpose-built for
their analytical workloads while reducing the need to duplicate transformation logic
across operational and analytical data warehouses. A full copy of data from Materialize
into S3 is a simple and intuitive way to enable this for our customers.

## Goals
* Allow batch exports of data in Materialize to S3 in a format that can be easily ingested into analytical data warehouses.
* Provide basic internal and external observability into the frequency, reliability, and size of the batch exports.
* Tightly constrain scope so that the initial feature can reach public preview in early Q1 next year.

## Non-goals
* Supporting continuous or scheduled exports.
* Integrating directly with batch data warehouses, without writing to S3.

## Solution Proposal

### SQL
In materialize we already have some support for `COPY`
commands. We should expand upon them, keeping them as close to
the [postgres syntax](https://www.postgresql.org/docs/current/sql-copy.html).

The SQL would look like the following.

```sql
COPY <mv_name> -- mv or table name
TO <scalar_expr_for_s3_path> -- the scalar_expr should give a path like 's3://prefix'
WITH (
  AWS CONNECTION = aws_conn -- previously created aws connection
  FORMAT = 'csv', -- file format name
  MAX FILE SIZE = 16777216 -- max file size in bytes, to limit size of files in s3
)
```
This option is the most similar to the Postgres `COPY` command syntax.

#### Alternative SQL
##### Option 1
```sql
COPY <mv_name>
TO <scalar_expr_for_s3_path>
USING AWS CONNECTION aws_conn -- moving the aws_conn outside `WITH`
WITH (
  ...
)
```
##### Option 2
```sql
COPY <mv_name>
TO S3 (<scalar_expr_for_s3_path>) -- explicitly mention keyword S3
...
```
##### Option 3
```sql
COPY <mv_name>
TO S3 (
  PATH = <scalar_expr_for_s3_path>,
  AWS CONNECTION = aws_conn,
  MAX FILE SIZE = 16777216,
  ) -- explicitly mention keyword S3 with S3 specific options
WITH (
  FORMAT = 'csv'
) -- WITH block contains options only related to the data not the target
```

For the s3 path, users should be able to provide an expression
like `'s3://prefix/path/' || mz_now()` to help generate the path. This would
help as they can run the query in a schedule without having to modify the
output path for each run.

For the `MAX FILE SIZE` we can take different approaches. Following is a list
of how other data warehouses or databases deal with this.

#### Snowflake
Snowflake accepts a similar parameter
called [`MAX_FILE_SIZE`](https://docs.snowflake.com/en/sql-reference/sql/copy-into-location#copy-options-copyoptions)
where they only accept an integer.

#### Redshift
For their `UNLOAD` command Redshift also has
[`MAXFILESIZE [AS] max-size [ MB | GB ]`](https://docs.aws.amazon.com/redshift/latest/dg/r_UNLOAD.html#unload-parameters)
where they expect the size to be only in MB or GB (TODO: Check if their multiplier is 1024 or 1000).

#### Postgres
Postgres does not have any file size option in their `COPY` command. But looking at
their [config settings](https://www.postgresql.org/docs/current/config-setting.html#CONFIG-SETTING-NAMES-VALUES), they accept both a number and a string with quotes specifying
the unit.

> Numeric with Unit: Some numeric parameters have an implicit unit, because they describe
 quantities of memory or time. The unit might be bytes, kilobytes, blocks (typically eight
 kilobytes), milliseconds, seconds, or minutes. An unadorned numeric value for one of these
 settings will use the setting's default unit, which can be learned from pg_settings.unit.
 For convenience, settings can be given with a unit specified explicitly, for example '120 ms' for
 a time value, and they will be converted to whatever the parameter's actual unit is. Note that the
 value must be written as a string (with quotes) to use this feature. The unit name is
 case-sensitive, and there can be whitespace between the numeric value and the unit.

> Valid memory units are B (bytes), kB (kilobytes), MB (megabytes), GB (gigabytes), and TB
(terabytes). The multiplier for memory units is 1024, not 1000.

#### Takeaway
It will be simplest to go the Snowflake approach of just accepting the MAX FILE SIZE in bytes.
Later, we can extend the feature to support a string like `'5 GB'` similar to Redshift/Postgres.

So eventually the `MAX FILE SIZE` can be either an integer or a parseable string. This will be
Postgres compatible as well. Note: Even though Postgres specifies MB not MiB, GB not GiB, the
multiplier is 1024 not 1000. To keep things consistent we can do the same thing as well.

### User Experience
- User will need either an AWS IAM user (to use credentials) or a AWS IAM role (to use AssumeRole)
on their end to set up an [AWS connection](../design/20231110_aws_connections.md).
- User needs to create the AWS connection in Materialize to use for `COPY TO ... s3://...`
 and check with `VALIDATE CONNECTION` that it's working.
- User will also need to give the AWS user/role for Materialize the following access to the
s3 prefixes so that we can write the data.
  - `s3:ListBucket`: We need this permission to make sure that we are writing to an empty path.
  - `s3:PutObject`: This is required to actually upload files to S3.
- If they try to run a `COPY` to s3 without all the permission, then the operation should fail
with an error. Note, running a `VALIDATE CONNECTION` does not guarantee that the specific S3
permissions are set up correctly. These can only be validated when the actual `COPY` command
is run.
- If the S3 path given already has some data in it, we should fail the operation.
- When they run the `COPY` command, to keep things simple we'll block the UI till the operation
is complete, similar to any other query.
- If successful, the output of the `COPY` command should return the number of rows written to S3
similar to a [Postgres `COPY` output](https://www.postgresql.org/docs/current/sql-copy.html) of
`COPY <row_count>`.
- If the operation fails midway for some reason or is cancelled by the user, we should show
a notice to the users that incomplete data could have been written to the S3 path and will
need to be cleaned up.
- Later we can provide an option to the user to always append a unique query ID to the path,
so they can just run the same query again and again without needing to clean up previous data.

## Implementation

### Parser
#### S3 path
We can add a new option value type of 'Uri', which would look like
```rust
enum Uri {
  // S3 path
  S3(String),
  ... // If we support more object stores or even local file, they can be added here
}
// and implement required traits so that it can be used as a valid value type
impl TryFromValue<Value> for Uri {
...
}
```
To start with we can accept the S3 path as a string and in follow up PRs add the ability
to eventually accept scalar expressions.
TODO: Check if there's already some queries where we explicitly parse scalar expressions in the parser.

#### `MAX FILE SIZE` Option
Initially supporting `MAX FILE SIZE` as integer for the bytes will be pretty simple. Later we
can introduce a `Memory` type which can take either an integer or a parseable string like `'2 GB'`,
similar to the implementation for `Duration`.

### Which cluster to run things on?
Currently existing supported `COPY ... TO STDOUT` command fetches the result to `environmentd`.
This will not be advisable for `COPY ... TO S3` as the data size could be large. Therefore,
we should trigger a dataflow in a `clusterd` which should eventually write to S3 using
user's aws connection details. The next question being, should it be a Storage cluser or a
Compute cluster (at least till the time the cluster unification is done).

We should use the Compute clsuter. Compute already has a concept of
["Compute Sinks"](https://github.com/MaterializeInc/materialize/blob/v0.77.1/src/compute-types/src/sinks.rs#L100-L103),
which include Subscribe and Persist (for materialized views). We can create another
Compute S3 sink, which will actually read from persist and write to S3. Keeping this in a
compute cluster also means,
- We eventually will support `SELECT` queries in a `COPY ... TO S3` command and that can
be a natural extension if things are already in compute.
- Via the Persist Compute Sink, there's already precedence to talking to S3, we should
be able to leverage some existing code.
- This also will be consistent with the behaviour that a user can select the compute cluster
where they want their query to run like they do for other queries.

#### Other Alternative: Storage cluster
The other alternative would have been to actually make use of a Storage cluster. If we think
of `COPY ... TO S3` as a one time sink, then it might also make sense to have it in the Storage
cluster. Especially if the initial scope does not include `SELECT` queries which
would have required spinning up compute dataflows.
Reasons to not do so,
- As mentioned above, we will eventually support `SELECT` queries in `COPY ... TO S3`. The
only reason it would have made sense to do this in storage would be to do as a quick stop gap
if the effort is drastically less. It does not appear to be so.
- Unlike Compute, Storage does not have a concept of running temporary/one-off dataflows.
Everything which runs, like sources or sinks, do so continuously once scheduled till they are
dropped.

### Output file format
We should start with a single file format and add more if there's more user interest. Currently,
there has been request for parquet but CSV support would be easiest to build.

Postgres supports `COPY ... TO` with a csv file and we should mimic the file structure for our
export as well.

Eventually we'll need to add some additional formatting options to the SQL like
`DELIMITER` (default will be comma), `QUOTE` (default is double quotes) etc. In the initial
version we should stick to the default values as per the
[Postgres COPY command](https://www.postgresql.org/docs/current/sql-copy.html).

### Writing to S3
TODO: partition files as per max file size, and backpressure?

## Rollout and Testing
We should put COPY TO S3 behind a feature flag.

#### Testing During development
TODO: Check how we do persist tests

#### Testing after code is merged
- Switch on the feature flag for the staging environment and do an end to end copy to s3
flow in staging.
- Also, try out a scenario where we cancel the copy midway

## Possible Future work

### Parquet output format
As mentioned earlier, there's already user interest for parquet. That can be a follow-up to this
feature. We'll need a comprehensive type mapping between parquet and materialize types and we
should do a separate design doc for that. Ideally that can be re-used for `COPY FROM S3` as well
if we ever want to support that.

### Add support for arbitrary queries
TODO

### More observability
Postgres reports the progress of an ongoing `COPY` to a `pg_stat_progress_copy`. We can have a
similar table to show progress which the user can query to see how far along they are.
