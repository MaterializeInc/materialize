- name: "syntax"
  code: |
    EXPLAIN (KEY | VALUE) SCHEMA [AS JSON]
    FOR CREATE SINK [<sink_name>]
    [IN CLUSTER <cluster_name>]
    FROM <item_name>
    INTO KAFKA CONNECTION <connection_name> (
      TOPIC '<topic>'
      [, COMPRESSION TYPE <compression_type>]
      [, TRANSACTIONAL ID PREFIX '<transactional_id_prefix>']
      [, PARTITION BY = <expression>]
      [, PROGRESS GROUP ID PREFIX '<progress_group_id_prefix>']
      [, TOPIC REPLICATION FACTOR <replication_factor>]
      [, TOPIC PARTITION COUNT <partition_count>]
      [, TOPIC CONFIG <topic_config>]
    )
    [KEY ( <key_column> [, ...] ) [NOT ENFORCED]]
    [HEADERS <headers_column>]
    [FORMAT <sink_format_spec> | KEY FORMAT <sink_format_spec> VALUE FORMAT <sink_format_spec>]
    [ENVELOPE (DEBEZIUM | UPSERT)]
    [WITH (<with_option> [, ...])]
  syntax_elements:
    - name: "**KEY** | **VALUE**"
      description: |
        Specifies whether to explain the key schema or value schema for the sink.
    - name: "**AS JSON**"
      description: |
        Optional. Format the explanation output as a JSON object. If not specified, the output is formatted as text.
    - name: "**FOR CREATE SINK** `[<sink_name>]`"
      description: |
        The `CREATE SINK` statement to explain. The sink name is optional.
    - name: "**IN CLUSTER** `<cluster_name>`"
      description: |
        Optional. The [cluster](/sql/create-cluster) to maintain this sink.
    - name: "**FROM** `<item_name>`"
      description: |
        The name of the source, table, or materialized view you want to send to the sink.
    - name: "**INTO KAFKA CONNECTION** `<connection_name>`"
      description: |
        The name of the Kafka connection to use in the sink. For details on creating connections, check the [`CREATE CONNECTION`](/sql/create-connection) documentation page.
    - name: "**TOPIC** `'<topic>'`"
      description: |
        The name of the Kafka topic to write to.
    - name: "**COMPRESSION TYPE** `<compression_type>`"
      description: |
        Optional. The type of compression to apply to messages before they are sent to Kafka: `none`, `gzip`, `snappy`, `lz4`, or `zstd`.
    - name: "**TRANSACTIONAL ID PREFIX** `'<transactional_id_prefix>'`"
      description: |
        Optional. The prefix of the transactional ID to use when producing to the Kafka topic.
    - name: "**PARTITION BY** `= <expression>`"
      description: |
        Optional. A SQL expression returning a hash that can be used for partition assignment. See [Partitioning](/sql/create-sink/kafka/#partitioning) for details.
    - name: "**PROGRESS GROUP ID PREFIX** `'<progress_group_id_prefix>'`"
      description: |
        Optional. The prefix of the consumer group ID to use when reading from the progress topic.
    - name: "**TOPIC REPLICATION FACTOR** `<replication_factor>`"
      description: |
        Optional. The replication factor to use when creating the Kafka topic (if the Kafka topic does not already exist).
    - name: "**TOPIC PARTITION COUNT** `<partition_count>`"
      description: |
        Optional. The partition count to use when creating the Kafka topic (if the Kafka topic does not already exist).
    - name: "**TOPIC CONFIG** `<topic_config>`"
      description: |
        Optional. Any topic-level configs to use when creating the Kafka topic (if the Kafka topic does not already exist). See the [Kafka documentation](https://kafka.apache.org/documentation/#topicconfigs) for available configs.
    - name: "**KEY** ( `<key_column>` [, ...] ) [**NOT ENFORCED**]"
      description: |
        Optional. A list of columns to use as the Kafka message key. If unspecified, the Kafka key is left unset. When using the upsert envelope, the key must be unique. Use **NOT ENFORCED** to disable validation of key uniqueness. See [Upsert key selection](/sql/create-sink/kafka/#upsert-key-selection) for details.
    - name: "**HEADERS** `<headers_column>`"
      description: |
        Optional. A column containing headers to add to each Kafka message emitted by the sink. The column must be of type `map[text => text]` or `map[text => bytea]`. See [Headers](/sql/create-sink/kafka/#headers) for details.
    - name: "**FORMAT** `<sink_format_spec>`"
      description: |
        Optional. Specifies the format to use for both keys and values: `AVRO USING CONFLUENT SCHEMA REGISTRY CONNECTION <csr_connection_name>`, `JSON`, `TEXT`, or `BYTES`. See [Formats](/sql/create-sink/kafka/#formats) for details.
    - name: "**KEY FORMAT** `<sink_format_spec>` **VALUE FORMAT** `<sink_format_spec>`"
      description: |
        Optional. Specifies the key format and value formats separately. See [Formats](/sql/create-sink/kafka/#formats) for details.
    - name: "**ENVELOPE** (`DEBEZIUM` | `UPSERT`)"
      description: |
        Optional. Specifies how changes to the sink's upstream relation are mapped to Kafka messages. Valid envelope types:

        | Envelope | Description |
        |----------|-------------|
        | `DEBEZIUM` | The generated schemas have a [Debezium-style diff envelope](/sql/create-sink/kafka/#debezium) to capture changes in the input view or source. |
        | `UPSERT` | The sink emits data with [upsert semantics](/sql/create-sink/kafka/#upsert). Requires a unique key specified using the `KEY` option. |
    - name: "**WITH** (`<with_option>` [, ...])"
      description: |
        Optional. The following `<with_option>`s are supported:

        | Option | Description |
        |--------|-------------|
        | `SNAPSHOT = <snapshot>` | Default: `true`. Whether to emit the consolidated results of the query before the sink was created at the start of the sink. To see only results after the sink is created, specify `WITH (SNAPSHOT = false)`. |
