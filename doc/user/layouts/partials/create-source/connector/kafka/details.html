### Kafka source details

- A Kafka source represents a single Kafka topic.
- By default, Materialize only ingests message payloads, not their keys.{{ if in $.envelopes "upsert" }}
  You can include the key portion using either the `INCLUDE KEY` statement or the
  [Upsert envelope](#upsert-envelope-details), which always includes keys.
  {{ else }} You can include the key portion using the `INCLUDE KEY` statement.
  {{ end }}
- Materialize supports connecting to [SSL-encrypted](#ssl-encrypted-kafka-details)
  or [Kerberized Kafka clusters](#kerberized-kafka-details).

#### Partition offsets

The `start_offset` and `kafka_time_offset` options come with some quirks to be aware of:

- If fewer offsets than partitions are provided, then the remaining partitions will start at offset 0. This is true if the user provides `start_offset=1` or `start_offset=[1, ...]`.
- If more offsets than partitions are provided, then any partitions added later will incorrectly be read from that offset. So, if you have a single partition, but you provide `start_offset=[1,2]`, when you add the second partition you will miss the first 2 records of data.
- Using an offset with a source envelope that can supply updates or deletes (e.g. Debezium) requires that Materialize handle possibly nonsensical events (e.g. an update for a row that was never inserted). For that reason, starting at an offset requires either a `NONE` envelope or a `(DEBEZIUM) UPSERT` envelope.

The `kafka_time_offset` option sets `start_offset` for each available partition based on the Kafka timestamp and the source behaves as if `start_offset` was provided directly.

#### SSL-encrypted Kafka details

Enable connections to SSL-encrypted Kafka clusters using the appropriate
[`WITH` options](#ssl-with-options).

- To encrypt data coming from the Kafka broker, authenticate its identity by
  providing a copy of the CA certificate that signed the broker's certificate
  (`ssl_ca_location`).
- To connect Materialize to a Kafka cluster that requires SSL authentication,
  create a client key pair for Materialize, and then provide the certificate,
  key, and optional key password (`ssl_certificate_location`, `ssl_key_location`,
  `ssl_key_password`, respectively). Note that this also requires authenticating
  the server.
- Materialize can also connect to a Confluent Schema Registry if it uses the same
  CA as the Kafka broker.

#### Kerberized Kafka details

Enable connections to Kerberized Kafka clusters using the appropriate [`WITH`
options](#kerberos-with-options).

- Materialize currently only supports:
  - `GSSAPI` or `PLAIN` as the `sasl_mechanisms`
  - `sasl_plaintext`, `scram-sha-256`, or `scram-sha-512`  as the `security_protocol`
- Materialize does _not_ support Kerberos authentication for Confluent Schema
  Registries.
