### S3 source details

The S3 source is designed to ingest a large volume of static data from [AWS's Simple Storage
Service][aws-s3], Amazon Web Services' cloud object store. It is important to note that an
object store behaves differently than a file system does, and that those differences affect how
Materialize interacts with objects in it.

Some key differences from file systems include:

* Latency on individual S3 operations is much higher than on even cloud-network filesystems.
* It is not possible to seek to a point in an object; Materialize must download the entire object from
  the beginning to read to a point.
* All object operations are atomic. It is not possible to modify just part of an S3 object;
  you can only entirely replace or delete objects.

The primary effect of this is that we do not handle *updates* to S3 objects, and we may interleave
multiple object ingestion to speed it up.

[aws-s3]: https://aws.amazon.com/s3/

#### S3 limitations

* Currently S3 sources do not support Avro- or Protobuf- encoded objects. Implementation issues:
  [Avro](https://github.com/MaterializeInc/materialize/issues/5596),
  [Protobuf](https://github.com/MaterializeInc/materialize/issues/5597)
* Object ingest order is not guaranteed.
* **All S3 sources are append-only**. Deleted and updated [S3 objects][] are silently ignored.

[S3 Objects]: https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingObjects.html

#### Object discovery strategies

Materialize has several techniques to discover the objects to download, with [more
planned][object-specifiers] for the future.

All strategies follow the same basic pattern.  Materialize will:

* Obtain a list of objects using [AWS APIs](#permissions-required).
* Deduplicate objects so the same object is never downloaded twice.
* Filter the list of objects against the [**MATCHING** clause
  patterns](#patterns).
* Download the matching objects.
* Treat each object downloaded as a newline-delimited file for the purposes of record
  delineation.

No guarantees are made about the relative order of records sent from the object discovery process
and the Materialize SQL engine. As usual with SQL, you must impose your own order.

You may specify strategies multiple times within one
`CREATE SOURCE` statement. For example, this is a legal invocation:

```
DISCOVER OBJECTS USING
  BUCKET SCAN 'example',
  BUCKET SCAN 'other',
  SQS NOTIFICATIONS 'example-notifications'
```

[object-specifiers]: https://github.com/MaterializeInc/materialize/issues/5502

##### Listing Bucket objects

The `BUCKET SCAN` discovery performs a single scan over the specified bucket at source creation
time. If you would like an S3 source to ingest objects that are added to the bucket after the
source is created you must also configure an `SQS NOTIFICATIONS` discovery mechanism on the source.

##### Listening to SQS notifications

AWS S3 has a built-in method for notifying downstream applications of bucket modification, the
[Event Notification API][notification-aws]. For Materialize, the only interesting modifications are
object creation, aka the `s3:ObjectCreated:*` event namespace. Follow [the AWS
Tutorial][notification-tutorial] to configure a bucket for exactly this namespace.

Once you have configured S3 notifications to go to an SQS queue, you can point Materialize at that
queue with **DISCOVER OBJECTS USING SQS NOTIFICATIONS `'queue-name'`**.

Materialize deletes SQS messages as soon as they are ingested. This means that the same SQS queue
cannot be used for multiple sources. If you would like to have multiple sources listening to
notifications from the same bucket you must [configure an SNS topic][sns] as an intermediary, with
multiple SQS queues subscribed to it, one SQS queue per source. Note that SQS queues subscribed to
SNS topics intended for Materialize must be configured to use [raw message delivery][raw-delivery].

Since Materialize treats unmaterialized sources with multiple downstream views as separate sources,
`SQS NOTIFICATIONS` can not be shared across multiple materializations of the same source. You must
create separate SQS queues for each S3 notification source.

[notification-aws]: https://docs.aws.amazon.com/AmazonS3/latest/userguide/NotificationHowTo.html
[notification-tutorial]: https://docs.aws.amazon.com/AmazonS3/latest/userguide/ways-to-add-notification-config-to-bucket.html
[sns]: https://docs.aws.amazon.com/AmazonS3/latest/userguide/ways-to-add-notification-config-to-bucket.html#step1-create-sns-topic-for-notification
[raw-delivery]: https://docs.aws.amazon.com/sns/latest/dg/sns-large-payload-raw-message-delivery.html

#### Patterns

It is possible to filter the list of object keys to download using unix-style glob syntax as an
argument to the `MATCHING` clause:

* `?` matches any single character except `/`.
* `*` matches zero or more characters except `/`.
* `**` recursively matches directories, but some other pattern must be specified. For example,
  `a/**` matches anything inside of the `a/` prefix (but not `a/` itself), and `**/a` matches `a`
  in any prefix, but not `a` with no prefix.
* `{a,b}` matches `a` or `b` where `a` and `b` are arbitrary glob patterns.
* `[ab]` matches `a` or `b` where `a` and `b` are characters. Prepend `!` to the matched
  characters to invert the match, e.g. `[!ab]` matches any character besides `a` or `b`.
* You can escape metacharacters such as `*` and `?` with character class notation. For example, `[*]`
  matches `*`.

##### Pattern examples

| Pattern            | Example Matches     | Example Excludes                           |
|--------------------|---------------------|--------------------------------------------|
| `**`               | `a` , `a/b/c.json`  | none                                       |
| `2020/**/*.json`   | `2020/11/uuid.json` | `data/2020/uuid.json` , `2020/11/uuid.csv` |
| `*`                | `a`                 | `a/b`                                      |
| `202{0,1}/*/*.csv` | `2020/11/data.csv`  | `2022/11/data.csv` , `2020/11/01/data.csv` |

#### Compression

- Omitting `COMPRESSION` is equivalent to `COMPRESSION NONE`.
- Using `GZIP` compression requires the object be compressed using the `gzip` algorithm or that it
be a concatenation of multiple `gzip` member streams.

{{ partial (printf "specifying-aws-credentials") . -}}

#### Permissions Required

The IAM User or Role used by `materialized` requires permission to perform different
AWS actions depending on which actions are required to discover
the list of objects to download.

The `DISCOVER OBJECTS USING` clause describes how Materialize will load objects, and so its parameters
 determine which permissions `materialized` requires. For example, since the `SCAN` key name
source (as in `DISCOVER OBJECTS USING BUCKET SCAN`) must perform repeated `ListObjects` actions to create a list
of key names to download, you must grant the Materialize IAM User or Role the `ListObjects` permission before you specify `DISCOVER OBJECTS USING BUCKET SCAN`.

| Key name source       | Permissions required                                                                     |
|-----------------------|------------------------------------------------------------------------------------------|
| All                   | [`GetObject` permission][getobject] for the objects to be downloaded              |
| **BUCKET SCAN**       | [`ListObject` permission][listobject] for the buckets to be scanned, **unless** the `MATCHING` pattern can only match a single object. In such cases, Materialize will perform only the necessary `GetObject` API call. |
| **SQS NOTIFICATIONS** | `ChangeMessageVisibility`, `DeleteMessage`, `GetQueueUrl`, `ReceiveMessage` [SQS Permissions][sqs-perms] for the queue Materialize will listen to |

The root AWS documentation for S3 permissions is [available here][s3-permissions].

[getobject]: https://docs.aws.amazon.com/AmazonS3/latest/API/API_GetObject.html
[listobject]: https://docs.aws.amazon.com/AmazonS3/latest/API/API_ListObjectsV2.html
[sqs-perms]: https://docs.aws.amazon.com/service-authorization/latest/reference/list_amazonsqs.html
[s3-permissions]: https://docs.aws.amazon.com/service-authorization/latest/reference/list_amazons3.html
