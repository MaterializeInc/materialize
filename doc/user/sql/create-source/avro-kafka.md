---
title: "CREATE SOURCE: Avro over Kafka"
description: "Learn how to connect Materialize to an Avro-formatted Kafka topic"
menu:
  main:
    parent: 'create-source'
aliases:
    - /docs/sql/create-source/kafka
    - /docs/sql/create-source/avro
    - /docs/sql/create-source/avro-source
---

`CREATE SOURCE` connects Materialize to some data source, and lets you interact
with its data as if it were in a SQL table.

This document details how to connect Materialize to an Avro-formatted Kafka
topic. For other options, view [`CREATE  SOURCE`](../).

## Conceptual framework

Sources represent connections to resources outside Materialize that it can read
data from. For more information, see [API Components:
Sources](../../../overview/api-components#sources).

## Syntax

{{< diagram "create-source-avro.html" >}}

Field | Use
------|-----
_src&lowbar;name_ | The name for the source, which is used as its table name within SQL.
_col&lowbar;name_ | Override default column name with the provided [identifier](../../identifiers). If used, a _col&lowbar;name_ must be provided for each column in the created source.
**KAFKA BROKER** _host_ | The Kafka broker's host name.
**FORMAT ...** _url_ | The URL of the Confluent schema registry to get schema information from.
**FORMAT ...** _schema&lowbar;file&lowbar;path_ | The absolute path to a file containing the schema.
**FORMAT ...** _inline&lowbar;schema_ | A string representing the schema.
**ENVELOPE** _envelope_ | The envelope type.<br/><br/> &#8226; **NONE** creates an append-only source. This means that records will only be appended and cannot be updated or deleted. <br/><br/>&#8226; **DEBEZIUM** creates a source that can reflect all CRUD operations from the source. This option requires records have the [appropriate fields](#format-implications), and is generally only supported by sources published to Kafka by [Debezium].<br/><br/>For more information, see [Debezium envelope details](#debezium-envelope-details).

## Details

This document assumes you're using Kafka to send Avro-encoded data to
Materialize.

### Kafka source details

Materialize expects each source to use to one Kafka topic, which is&mdash;in
  turn&mdash;generated by a single table in an upstream database.

### Avro format details

Avro-formatted external sources require providing you providing the schema in
one of two ways:

- Using the [Confluent Schema Registry](#using-a-confluent-schema-registry)
- Providing the Avro schema [in-line when creating the
  source](#inlining-the-avro-schema).

### Envelope details

Envelopes determine whether an incoming record inserts new data, updates or
deletes existing data, or both. For more information, see [API Components:
Envelopes](../../../overview/api-components#envelopes).

Avro-encoded data sent through Kafka is the only type of data that can support
the Debezium envelope.

### Debezium envelope details

The Debezium envelope provides a "diff envelope", which describes the decoded
records' old and new values; this is roughly equivalent to the notion of Change
Data Capture, or CDC. Materialize can use the data in this diff envelope to
process data as representing inserts, updates, or deletes.

This envelope is called the Debezium envelope because it's been developed to
explicitly work with [Debezium].

To use the Debezium envelope with Materialize, you must configure Debezium with
your database.

- [MySQL](https://debezium.io/documentation/reference/0.10/connectors/mysql.html)
- [PostgreSQL](https://debezium.io/documentation/reference/0.10/connectors/postgresql.html)

The Debezium envelope is only supported by sources published to Kafka by
Debezium.

#### Format implications

Using the Debezium envelopes changes the schema of your Avro-encoded Kafka
topics to include something akin to the following field:

```json
{
    "type": "record",
    "name": "envelope",
    "fields": [
        {
        "name": "before",
        "type": [
            {
            "name": "row",
            "type": "record",
            "fields": [
                {"name": "a", "type": "long"},
                {"name": "b", "type": "long"}
            ]
            },
            "null"
        ]
        },
        { "name": "after", "type": ["row", "null"] }
    ]
}
```

Note that:

- If you use the Confluent Schema Registry to receive your schemas, you don't
  need to manually create this field; Debezium will have taken care of it for
  you.
- The following section depends on the column's names and types, and is unlikely
  to match our example:
    ```json
    ...
    "fields": [
            {"name": "a", "type": "long"},
            {"name": "b", "type": "long"}
        ]
    ...
    ```

## Examples

### Using a Confluent schema registry

```sql
CREATE SOURCE events
FROM KAFKA BROKER 'localhost:9092' TOPIC 'events'
FORMAT AVRO
    USING CONFLUENT SCHEMA REGISTRY 'http://localhost:8081';
```

This creates a source that...

- Automatically determines its schema from the Confluent Schema Registry.
- Decodes data received from the `events` topic published by Kafka running on
  `localhost:9092`.
- Decodes using an Avro schema.
- Is eligible to use the Debezium envelope because it's Avro-encoded and
  published by Kafka; however, this still depends on whether or not the upstream
  database publishes its data from a Debezium-enabled database.

### Inlining the Avro schema

```sql
CREATE SOURCE user
FROM KAFKA BROKER 'localhost:9092' TOPIC 'user'
FORMAT AVRO
USING SCHEMA '{
  "type": "record",
  "name": "envelope",
  "fields": [
    ...
  ],
}'
    ENVELOPE DEBEZIUM;
```

This creates a source that...

- Has its schema defined inline, and decodes data using that schema.
- Decodes data received from the `user` topic published by Kafka running on
  `localhost:9092`.
- Uses the Debezium envelope, meaning it supports delete, updates, and inserts.

## Related pages

- [`CREATE SOURCE`](../)
- [`CREATE VIEW`](../../create-view)
- [`SELECT`](../../select)

[Debezium]: http://debezium.io
