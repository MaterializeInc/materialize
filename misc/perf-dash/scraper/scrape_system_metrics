#!/usr/bin/env python3

# Copyright Materialize, Inc. All rights reserved.
#
# Use of this software is governed by the Business Source License
# included in the LICENSE file at the root of this repository.
#
# As of the Change Date specified in that file, in accordance with
# the Business Source License, use of this software will be governed
# by the Apache License, Version 2.0.

import argparse
import dateparser
import datetime
import io
import os
import pathlib
import sys
import time
import typing
import urllib

import confluent_kafka.admin
import confluent_kafka.avro
import confluent_kafka.schema_registry
import psycopg2
import psycopg2.extensions
import psycopg2.extras


METRICS_DIRECTORY = "/usr/share/perf-scraper/metrics"


class Scraper:

    def __init__(self, metric_dir: pathlib.Path, kafka_config: typing.Dict[str, str]):
        """Construct a producer for writing to the given topic, using the given config."""
        self.topic_name = metric_dir.name

        key_schema = confluent_kafka.avro.load(pathlib.Path(metric_dir, 'key-schema.avsc'))
        value_schema = confluent_kafka.avro.load(pathlib.Path(metric_dir, 'value-schema.avsc'))

        self.query = pathlib.Path(metric_dir, 'query.sql').read_text().strip()

        # Namespace for both schemas should be the name of the topic
        assert key_schema.namespace == value_schema.namespace
        assert self.topic_name == key_schema.namespace

        self.producer = confluent_kafka.avro.AvroProducer(kafka_config,
                                                default_key_schema=key_schema,
                                                default_value_schema=value_schema)

    def scrape(self, cursor: psycopg2.extensions.cursor) -> None:
        """Issue a query against the system tables and write the data to Kafka."""
        cursor.execute(self.query)
        for row in cursor:
            key = {"mz_cluster_id": row["mz_cluster_id"]}
            self.producer.produce(topic=self.topic_name, key=key, value=row)
        self.producer.flush()


class MetricsScrapers:

    def __init__(self, args: argparse.Namespace):
        """Create an instance responsible for recording benchmark metrics."""
        self.scrapers: typing.Dict[str, Scraper] = {}

        # See https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md
        self.kafka_config = {'bootstrap.servers': f"{args.kafka_host}:{args.kafka_port}",
                             'schema.registry.url': f'http://{args.csr_host}:{args.csr_port}'}
        for metric_dir in pathlib.Path(args.metrics_directory).iterdir():
            self.scrapers[metric_dir.name] = Scraper(metric_dir, self.kafka_config)

    def scrape(self, dsn: str) -> None:
        """Encode key and value using Avro and send event to Kafka."""
        with psycopg2.connect(dsn) as conn:
            with conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor) as cursor:
                for scraper in self.scrapers.values():
                    scraper.scrape(cursor)


def create_topics(args: argparse.Namespace) -> None:
    """Create the topics for each metric and exit."""

    csr_config = {'url': f'http://{args.csr_host}:{args.csr_port}'}
    csr = confluent_kafka.schema_registry.SchemaRegistryClient(csr_config)

    kafka_config = {'bootstrap.servers': f"{args.kafka_host}:{args.kafka_port}"}
    admin_client = confluent_kafka.admin.AdminClient(kafka_config)

    for metric_dir in pathlib.Path(args.metrics_directory).iterdir():

        key_schema_str = pathlib.Path(metric_dir, 'key-schema.avsc').read_text().strip()
        key_schema = confluent_kafka.schema_registry.Schema(key_schema_str, 'AVRO')
        value_schema_str = pathlib.Path(metric_dir, 'value-schema.avsc').read_text().strip()
        value_schema = confluent_kafka.schema_registry.Schema(value_schema_str, 'AVRO')

        csr.register_schema(f"{metric_dir.name}-key", key_schema)
        csr.register_schema(f"{metric_dir.name}-value", value_schema)

        # Create topics takes and returns a list of futures but we're going to call create topic
        # while iterating on each directory, so these are just lists of 1
        topics = [confluent_kafka.admin.NewTopic(metric_dir.name, num_partitions=10, replication_factor=1)]
        # Don't bother trying to catch the error, let's just fail startup and raise the error
        [future.result() for future in admin_client.create_topics(topics).values()]


def scrape(args: argparse.Namespace) -> None:
    """Grab system metrics from the configured materialized instance, replicating them to Kafka"""
    dsn = f'postgresql://{args.dbuser}@{args.dbhost}:{args.dbport}/{args.dbname}'
    scrapers = MetricsScrapers(args)
    while True:
        scrapers.scrape(dsn)
        time.sleep(args.scrape_interval)


def run(args: argparse.Namespace) -> None:
    """Wait for the query to settle or timeout and then dump ingest metrics."""
    args.action(args)


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser()

    parser.add_argument(
        "-v",
        "--verbose",
        action="store_true",
        help="Enable verbose logging to print the results of each prometheus query",
    )

    parser.add_argument(
        "--metrics-directory",
        type=str,
        default=METRICS_DIRECTORY,
        help="Directory containing metrics definitions and source queries",
    )

    parser.add_argument(
        "--kafka-host",
        help="Name of the kafka broker",
        type=str,
        default="kafka",
    )

    parser.add_argument(
        "--kafka-port", help="Port the connect to the broker over", type=int, default=9092
    )

    parser.add_argument(
        "--csr-host", help="Hostname of the schema registry", type=str,
        default="schema-registry"
    )
    parser.add_argument(
        "--csr-port", help="Port that schema registry is listening on", type=int,
        default=8081
    )

    sub_parsers = parser.add_subparsers()

    # Create Topics Subcommand and Flags
    create_topic_parser = sub_parsers.add_parser("create-topics")
    create_topic_parser.set_defaults(action=create_topics)

    # Scrape Subcommand and Flags
    scrape_parser = sub_parsers.add_parser("scrape")
    scrape_parser.set_defaults(action=scrape)

    scrape_parser.add_argument(
        "--dbhost", help="materialized hostname", default="materialized", type=str
    )
    scrape_parser.add_argument(
        "--dbname", help="materialized database name", default="materialize", type=str
    )
    scrape_parser.add_argument(
        "--dbport", help="materialized port number", default=6875, type=int
    )
    scrape_parser.add_argument(
        "--dbuser", help="materialized username", default="materialize", type=str
    )

    scrape_parser.add_argument(
        "--scrape-interval", help="How often, in seconds, to scrape metrics", type=int,
        default=1
    )

    return parser.parse_args()


if __name__ == '__main__':
    run(parse_args())
