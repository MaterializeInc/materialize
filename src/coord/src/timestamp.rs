// Copyright Materialize, Inc. All rights reserved.
//
// Use of this software is governed by the Business Source License
// included in the LICENSE file.
//
// As of the Change Date specified in that file, in accordance with
// the Business Source License, use of this software will be governed
// by the Apache License, Version 2.0.

use std::cmp;
use std::collections::{HashMap, HashSet};
use std::convert::TryFrom;
use std::convert::TryInto;
use std::io::BufRead;
use std::ops::Deref;
use std::panic;
use std::path::PathBuf;
use std::str;
use std::sync::atomic::{AtomicBool, Ordering};
use std::sync::mpsc::{Receiver, TryRecvError};
use std::sync::{Arc, Mutex, MutexGuard};
use std::thread;
use std::time::{Duration, SystemTime, UNIX_EPOCH};

use avro::schema::Schema;
use avro::types::Value;
use failure::bail;
use futures::executor::block_on;
use lazy_static::lazy_static;
use log::{debug, error, info, log_enabled, warn};
use prometheus::{register_int_gauge_vec, IntGaugeVec};
use rdkafka::consumer::{BaseConsumer, Consumer};
use rdkafka::message::Message;
use rdkafka::ClientConfig;
use rusoto_kinesis::KinesisClient;
use rusqlite::{params, NO_PARAMS};

use dataflow::source::read_file_task;
use dataflow::source::FileReadStyle;
use dataflow_types::{
    Consistency, DataEncoding, Envelope, ExternalSourceConnector, FileSourceConnector, KafkaOffset,
    KafkaSourceConnector, KinesisSourceConnector, MzOffset, SourceConnector,
};
use expr::{PartitionId, SourceInstanceId};
use ore::collections::CollectionExt;

use crate::catalog::storage::{self, SqlVal};
use crate::coord;

/// Number of seconds that must elapse before we update watermarks again
const WATERMARK_METRIC_MAX_UPDATE_INTERVAL_SECS: u64 = 1;

lazy_static! {

    // Expected Avro schema for BYO consistency AVRO sources
    static ref BYO_CONSISTENCY_SCHEMA: Schema = {
        Schema::parse_str(
            r#"{
                "name": "materialize.byo.consistency",
                "type": "record",
                "fields": [
                    {
                      "name": "source",
                      "type": "string"
                    },
                    {
                      "name": "partition_count",
                      "type": "int"
                    },
                    {
                      "name": "partition_id",
                      "type": ["int","string"]
                    },
                    {
                      "name": "timestamp",
                      "type": "long"
                    },
                    {
                      "name": "offset",
                      "type": "long"
                    }
                ]
    }"#,).unwrap()};

    // Expected schema generated by Debezium consistency sources
    static ref DEBEZIUM_TRX_SCHEMA_KEY: Schema = {
        Schema::parse_str(
            r#"    {
    "name": "io.debezium.connector.common.TransactionMetadataKey",
    "type": "record",
    "fields": [
        {
            "name": "id",
            "type": "string"
        }
    ]
}"#,
        )
        .unwrap()
    };

    // Expected schema generated by Debezium consistency sources
    static ref DEBEZIUM_TRX_SCHEMA_VALUE: Schema = {
        Schema::parse_str(
            r#"    {
    "name": "io.debezium.connector.common.TransactionMetadataValue",
    "type": "record",
    "fields": [
        {
            "name": "id",
            "type": "string"
        },
        {
            "name": "status",
            "type": "string"
        },
        {
            "name": "event_count",
            "type": [
                "long",
                "null"
            ]
        },
        {
            "name": "data_collections",
            "type": [
                {
                    "type": "array",
                    "items": {
                        "name": "data",
                        "type": "record",
                        "fields": [
                            {
                                "name": "event_count",
                                "type": "long"
                            },
                            {
                                "name": "data_collection",
                                "type": "string"
                            }
                        ]
                    }
                },
                "null"
            ]
        }
    ]
}"#,
        )
        .unwrap()
    };

    /// The high watermark for a partition, the maximum offset that we could hope to ingest
    static ref KAFKA_PARTITION_OFFSET_MAX: IntGaugeVec = register_int_gauge_vec!(
        "mz_kafka_partition_offset_max",
        "The high watermark for a partition, the maximum offset that we could hope to ingest",
        &["topic", "source_id", "partition_id"]
    )
    .unwrap();

      /// The max timestamp that we have allocated for this partition
    static ref MAX_TIMESTAMP: IntGaugeVec = register_int_gauge_vec!(
        "mz_max_timestamp",
        "The high watermark for a partition, the maximum offset that we could hope to ingest",
        &["source_id", "partition_id"]
    )
    .unwrap();
}

pub struct TimestampConfig {
    pub frequency: Duration,
    pub persist_ts: bool,
}

#[derive(Debug)]
pub enum TimestampMessage {
    Add(SourceInstanceId, SourceConnector),
    DropInstance(SourceInstanceId),
    Shutdown,
}

/// Timestamp consumer: wrapper around source consumers that stores necessary information
/// about topics and offset for real-time consistency
struct RtTimestampConsumer {
    connector: RtTimestampConnector,
    last_partition_offset: HashMap<PartitionId, MzOffset>,
    start_offset: MzOffset,
    max_ts_batch: i64,
}

enum RtTimestampConnector {
    Kafka(RtKafkaConnector),
    File(RtFileConnector<Vec<u8>, failure::Error>),
    Ocf(RtFileConnector<Value, failure::Error>),
    Kinesis(RtKinesisConnector),
}

enum ByoTimestampConnector {
    Kafka(ByoKafkaConnector),
    File(ByoFileConnector<Vec<u8>, failure::Error>),
    Ocf(ByoFileConnector<Value, failure::Error>),
    Kinesis(ByoKinesisConnector),
}

// List of possible encoding types
enum ValueEncoding {
    Bytes(Vec<u8>),
    Avro(Value),
}

/// Timestamp consumer: wrapper around source consumers that stores necessary information
/// about topics and offset for byo consistency
struct ByoTimestampConsumer {
    /// Source Connector
    connector: ByoTimestampConnector,
    /// The name of the source with which this connector is associated
    ///
    /// * For kafka this is the topic
    /// * For kinesis this is the stream name
    /// * For file types this is the file name
    source_name: String,
    /// The SourceId that this consumer is associated with
    source_id: SourceInstanceId,
    /// The format of the connector
    envelope: ConsistencyFormatting,
    /// The last timestamp assigned per partition
    last_partition_ts: HashMap<PartitionId, u64>,
    /// The max assigned timestamp. Should be max(last_partition_ts)
    last_ts: u64,
    /// The max offset for which a timestamp has been assigned
    last_offset: MzOffset,
    /// The total number of partitions for the data topic
    current_partition_count: i32,
    /// This is the maximum number of timestamp updates that we process in one run
    max_ts_batch: i64,
}

impl ByoTimestampConsumer {
    fn update_and_send(
        &mut self,
        tx: &futures::channel::mpsc::UnboundedSender<coord::Message>,
        sid: SourceInstanceId,
        partition_count: i32,
        partition: PartitionId,
        timestamp: u64,
        offset: MzOffset,
    ) {
        if self.current_partition_count < partition_count && self.last_ts > 0 {
            // A new partition has been added. Partitions always gets added with
            // newPartitionId = previousLastPartitionId + 1 and start from 0.
            // So this new partition will have ID "partition_count - 1"
            // We ensure that the first messages in this partition will always have
            // timestamps > the last closed timestamp. We need to explicitly close
            // out all prior timestamps. To achieve this, we send an additional
            // timestamp message to the coord/worker

            // This can only happen for Kafka sources
            tx.unbounded_send(coord::Message::AdvanceSourceTimestamp {
                id: sid,
                partition_count, // The new partition count
                pid: PartitionId::Kafka(partition_count - 1), // the ID of the new partition
                timestamp: self.last_ts,
                offset: MzOffset { offset: 0 }, // An offset of 0 will "fast-forward" the stream, it denotes
                                                // the empty interval
            })
            .expect("Failed to send update to coordinator");
        }
        self.current_partition_count = partition_count;
        self.last_ts = timestamp;
        self.last_partition_ts.insert(partition.clone(), timestamp);
        tx.unbounded_send(coord::Message::AdvanceSourceTimestamp {
            id: sid,
            partition_count,
            pid: partition,
            timestamp,
            offset,
        })
        .expect("Failed to send update to coordinator");
    }
}

/// Supported format/envelope pairs for consistency topic decoding
enum ConsistencyFormatting {
    /// The formatting of this consistency source follows the
    /// SourceName,PartitionCount,PartitionId,TS,Offset
    ByoBytes,
    /// The formatting of this consistency source follows
    /// the Avro BYO consistency format
    ByoAvro,
    /// The formatting of this consistency source follows the
    /// the AvroOCF BYO consistency format
    ByoAvroOcf,
    /// The formatting of this consistency source follows the
    /// Debezium Avro format
    DebeziumAvro,
    /// The formatting of this consistency source follows the
    /// Debezium AvroOCF format
    DebeziumOcf,
}

/// Data consumer for Kafka source with RT consistency
#[derive(Clone)]
struct RtKafkaConnector {
    state: Arc<RtKafkaState>,
    id: SourceInstanceId,
    topic: String,
}

struct RtKafkaState {
    stop: AtomicBool,
    high_watermarks: Mutex<Vec<i64>>,
}

use std::{fmt::Display, time::Instant};

/// Data consumer for Kafka source with BYO consistency
struct ByoKafkaConnector {
    consumer: BaseConsumer,
    /// Used to track if we should update watermark metrics
    last_watermark_update: Instant,
}

impl ByoKafkaConnector {
    fn new(consumer: BaseConsumer) -> ByoKafkaConnector {
        ByoKafkaConnector {
            consumer,
            last_watermark_update: Instant::now(),
        }
    }

    /// Update watermark metadata if appropriate
    ///
    /// "Appropriate" meants that it has been more than [`WATERMARK_METRIC_MAX_UPDATE_INTERVAL_SECS`]
    /// since the last update.
    fn update_watermark_metrics(&mut self, topic: &str, source_id: &SourceInstanceId) {
        if self.last_watermark_update.elapsed().as_secs()
            >= WATERMARK_METRIC_MAX_UPDATE_INTERVAL_SECS
        {
            // TODO(benesch): 1s is too small a timeout for some Kafka clusters,
            // but we don't want to just up the timeout here because this
            // presently blocks the main Kafka thread.
            let partitions =
                match get_kafka_partitions(&self.consumer, topic, Duration::from_secs(1)) {
                    Ok(partitions) => partitions,
                    Err(e) => {
                        error!("while fetching kafka partitions for topic {}: {}", topic, e);
                        return;
                    }
                };
            for p in partitions {
                match self
                    .consumer
                    .fetch_watermarks(&topic, p, Duration::from_secs(1))
                {
                    Ok((_low, high)) => KAFKA_PARTITION_OFFSET_MAX
                        .with_label_values(&[&source_id.to_string(), &topic, &p.to_string()])
                        .set(high),
                    Err(e) => warn!(
                        "error loading watermarks topic={} partition={} error={}",
                        topic, p, e
                    ),
                }
            }
            self.last_watermark_update = Instant::now();
        }
    }
}

/// Data consumer for Kinesis source with RT consistency
#[allow(dead_code)]
struct RtKinesisConnector {
    stream_name: String,
    kinesis_client: Option<KinesisClient>,
    cached_shard_ids: Option<HashSet<String>>,
    timestamper_iteration_count: u64,
}

/// Data consumer stub for Kinesis source with BYO consistency
struct ByoKinesisConnector {}

/// Data consumer stub for File source with RT consistency
struct RtFileConnector<Out, Err> {
    stream: Receiver<Result<Out, Err>>,
}

/// Data consumer stub for File source with BYO consistency
struct ByoFileConnector<Out, Err> {
    stream: Receiver<Result<Out, Err>>,
}

fn byo_query_source(consumer: &mut ByoTimestampConsumer) -> Vec<ValueEncoding> {
    let mut messages = vec![];
    let mut msg_count = 0;
    match &mut consumer.connector {
        ByoTimestampConnector::Kafka(kafka_connector) => {
            let topic = &consumer.source_name;
            kafka_connector.update_watermark_metrics(topic, &consumer.source_id);
            while let Some(payload) = kafka_get_next_message(&mut kafka_connector.consumer) {
                messages.push(ValueEncoding::Bytes(payload));
                msg_count += 1;
                if msg_count == consumer.max_ts_batch {
                    // Make sure to bound the number of timestamp updates we have at once,
                    // to avoid overflowing the system
                    break;
                }
            }
        }
        ByoTimestampConnector::Kinesis(_kinesis_consumer) => {
            error!("Timestamping for Kinesis sources is unimplemented");
        }
        ByoTimestampConnector::File(file_consumer) => {
            while let Some(payload) = file_get_next_message(file_consumer) {
                messages.push(ValueEncoding::Bytes(payload));
                msg_count += 1;
                if msg_count == consumer.max_ts_batch {
                    // Make sure to bound the number of timestamp updates we have at once,
                    // to avoid overflowing the system
                    break;
                }
            }
        }
        ByoTimestampConnector::Ocf(file_consumer) => {
            while let Some(payload) = file_get_next_message(file_consumer) {
                messages.push(ValueEncoding::Avro(payload));
                msg_count += 1;
                if msg_count == consumer.max_ts_batch {
                    // Make sure to bound the number of timestamp updates we have at once,
                    // to avoid overflowing the system
                    break;
                }
            }
        }
    }
    messages
}

/// Returns the next message of a stream, or None if no such message exists
fn file_get_next_message<Out, Err>(file_consumer: &mut ByoFileConnector<Out, Err>) -> Option<Out>
where
    Err: Display,
{
    match file_consumer.stream.try_recv() {
        Ok(Ok(record)) => Some(record),
        Ok(Err(e)) => {
            error!("Failed to read file for timestamping: {}", e);
            None
        }
        Err(TryRecvError::Empty) => None,
        Err(TryRecvError::Disconnected) => None,
    }
}

fn byo_extract_update_from_bytes(
    consumer: &ByoTimestampConsumer,
    messages: Vec<ValueEncoding>,
) -> Vec<(i32, PartitionId, u64, MzOffset)> {
    let mut updates = vec![];
    for payload in messages {
        let msg = if let ValueEncoding::Bytes(msg) = payload {
            msg
        } else {
            panic!("The byte consistency type should always encode consistency msgs as bytes");
        };
        let st = str::from_utf8(&msg);
        match st {
            Ok(timestamp) => {
                // Extract timestamp from payload
                let split: Vec<&str> = timestamp.split(',').collect();
                if split.len() != 5 {
                    error!("incorrect payload format. Expected: SourceName,PartitionCount,PartitionId,TS,Offset. Got: {}", timestamp);
                    continue;
                }
                let topic_name = String::from(split[0]);
                let partition_count = match split[1].parse::<i32>() {
                    Ok(i) => i,
                    Err(err) => {
                        error!("incorrect timestamp format {}", err);
                        continue;
                    }
                };
                let partition = match &consumer.connector {
                    ByoTimestampConnector::Kinesis(_) => match split[2].parse::<String>() {
                        Ok(s) => PartitionId::Kinesis(s),
                        Err(err) => {
                            error!("incorrect timestamp format {}", err);
                            continue;
                        }
                    },
                    ByoTimestampConnector::Kafka(_) => match split[2].parse::<i32>() {
                        Ok(i) => PartitionId::Kafka(i),
                        Err(err) => {
                            error!("incorrect timestamp format {}", err);
                            continue;
                        }
                    },
                    _ => PartitionId::File,
                };
                let ts = match split[3].parse::<u64>() {
                    Ok(i) => i,
                    Err(err) => {
                        error!("incorrect timestamp format {}", err);
                        continue;
                    }
                };
                let offset = match split[4].parse::<i64>() {
                    Ok(i) => i,
                    Err(err) => {
                        error!("incorrect timestamp format {}", err);
                        continue;
                    }
                };
                if topic_name.trim() == consumer.source_name {
                    updates.push((partition_count, partition, ts, MzOffset { offset }))
                }
            }
            Err(err) => error!("incorrect payload format: {}", err),
        }
    }
    updates
}

/// Polls a message from a Kafka Source
fn kafka_get_next_message(consumer: &mut BaseConsumer) -> Option<Vec<u8>> {
    if let Some(result) = consumer.poll(Duration::from_millis(60)) {
        match result {
            Ok(message) => match message.payload() {
                Some(p) => Some(p.to_vec()),
                None => {
                    error!("unexpected null payload");
                    None
                }
            },
            Err(err) => {
                error!("Failed to process message {}", err);
                None
            }
        }
    } else {
        None
    }
}

/// Return the list of partition ids associated with a specific topic
fn get_kafka_partitions(
    consumer: &BaseConsumer,
    topic: &str,
    timeout: Duration,
) -> Result<Vec<i32>, failure::Error> {
    let meta = consumer.fetch_metadata(Some(&topic), timeout)?;
    if meta.topics().len() == 0 {
        bail!("topic {} does not exist", topic);
    } else if meta.topics().len() > 1 {
        bail!("topic metadata had more than one result");
    }
    let meta_topic = meta.topics().into_element();
    if meta_topic.name() != topic {
        bail!(
            "got results for wrong topic {} (expected {})",
            meta_topic.name(),
            topic
        );
    }
    Ok(meta_topic.partitions().iter().map(|x| x.id()).collect())
}

pub struct Timestamper {
    /// Current list of up to date sources that use a real time consistency model
    rt_sources: HashMap<SourceInstanceId, RtTimestampConsumer>,

    /// Current list of up to date sources that use a BYO consistency model
    byo_sources: HashMap<SourceInstanceId, ByoTimestampConsumer>,

    /// Connection to the underlying SQL lite instance
    storage: Arc<Mutex<storage::Connection>>,

    tx: futures::channel::mpsc::UnboundedSender<coord::Message>,
    rx: std::sync::mpsc::Receiver<TimestampMessage>,

    /// Last Timestamp (necessary because not necessarily increasing otherwise)
    current_timestamp: u64,

    /// Frequency at which thread should run
    timestamp_frequency: Duration,

    /// Persist consistency information
    persist_ts: bool,
}

/// A byo record contains a single timestamp update for a given source
fn parse_byo(record: Vec<(String, Value)>) -> (String, i32, PartitionId, u64, MzOffset) {
    let mut topic = String::new();
    let mut partition_count = 0;
    let mut partition_id = PartitionId::File;
    let mut timestamp = 0;
    let mut offset = 0;

    for (key, value) in record {
        if key == "source" {
            if let Value::String(s) = value {
                topic = s;
            } else {
                panic!("String expected");
            }
        } else if key == "partition_count" {
            if let Value::Int(count) = value {
                partition_count = count;
            } else {
                panic!("Int expected");
            }
        } else if key == "partition_id" {
            if let Value::Union(_, value) = value {
                if let Value::Int(pid) = *value {
                    partition_id = PartitionId::Kafka(pid);
                } else if let Value::String(s) = *value {
                    partition_id = PartitionId::Kinesis(s);
                } else {
                    panic!("String or Int expected");
                }
            } else {
                panic!("Union expected");
            }
        } else if key == "timestamp" {
            if let Value::Long(ts) = value {
                timestamp = ts as u64;
            } else {
                panic!("Long expected");
            }
        } else if key == "offset" {
            if let Value::Long(off) = value {
                offset = off;
            } else {
                panic!("Long expected");
            }
        }
    }
    (
        topic,
        partition_count,
        partition_id,
        timestamp,
        MzOffset { offset },
    )
}

/// A debezium record contains a set of update counts for each topic that the transaction
/// updated. This function extracts the set of (topic, update_count) as a vector.
fn parse_debezium(record: Vec<(String, Value)>) -> Vec<(String, i64)> {
    let mut result = vec![];
    for (key, value) in record {
        if key == "data_collections" {
            if let Value::Union(_, value) = value {
                if let Value::Array(items) = *value {
                    for v in items {
                        if let Value::Record(item) = v {
                            let mut value: String = String::new();
                            let mut write_count = 0;
                            for (k, v) in item {
                                if k == "data_collection" {
                                    if let Value::String(data) = v {
                                        value = data;
                                    } else {
                                        panic!("Incorrect AVRO format. String expected");
                                    }
                                } else if k == "event_count" {
                                    if let Value::Long(e) = v {
                                        write_count = e;
                                    } else {
                                        panic!("Incorrect AVRO format. Long expected");
                                    }
                                }
                            }
                            if !value.is_empty() {
                                result.push((value, write_count));
                            }
                        } else {
                            error!("Incorrect AVRO format. Record expected");
                        }
                    }
                }
            } else {
                error!(
                    "Incorrect AVRO format. Union of Null/Array expected {:?}",
                    value
                );
            }
        }
    }
    result
}

/// This function determines the next maximum offset to timestamp.
/// This offset should be no greater than max_increment_size
/// entries since last_processed_offset
/// Ex: last processed offset was 1 (we processed one record total). The current max kafka offset is 5
/// For a batch size of 10, the function will return offset 5.
/// For a batch size of 1, the function will return 2
///
/// Ex: last processed offset is 0 (ak, no records have been timestamped yet)
/// The current max kafka offset is 0 (ak, the stream is empty). The function will return
/// 0.
fn determine_next_offset(
    // The last offset which we have assigned a timestamp for
    last_processed_offset: MzOffset,
    // The current max offset that exists
    current_max_kafka_offset: MzOffset,
    // The max size of the batch
    max_increment_size: i64,
) -> MzOffset {
    // If bounding batches is activated (aka, max_increment_size > 0), then
    // bound the next timestamp to be no more than max_increment_size in the future
    if max_increment_size > 0
        && ((current_max_kafka_offset.offset - last_processed_offset.offset) > max_increment_size)
    {
        MzOffset {
            offset: (last_processed_offset.offset + max_increment_size),
        }
    } else {
        // We take the max of the last offset which we have already timestamped
        // and the highest offset for this Kafka topic
        MzOffset {
            offset: (std::cmp::max(
                last_processed_offset.offset,
                current_max_kafka_offset.offset,
            )),
        }
    }
}

/// Determines whether the next proposed timestamp follows the timestamp
/// assigning rules
fn is_ts_valid(
    byo_consumer: &ByoTimestampConsumer,
    partition_count: i32,
    partition: &PartitionId,
    timestamp: u64,
) -> bool {
    let last_p_ts = match byo_consumer.last_partition_ts.get(&partition) {
        Some(ts) => *ts,
        None => 0,
    };

    if timestamp == 0
        || timestamp == std::u64::MAX
        || timestamp < byo_consumer.last_ts
        || timestamp <= last_p_ts
        || (partition_count > byo_consumer.current_partition_count
            && timestamp == byo_consumer.last_ts)
    {
        error!("The timestamp assignment rules have been violated. The rules are as follows:\n\
                     1) A timestamp should be greater than 0\n\
                     2) The timestamp should be strictly smaller than u64::MAX\n\
                     2) If no new partition is added, a new timestamp should be:\n \
                        - strictly greater than the last timestamp in this partition\n \
                        - greater or equal to all the timestamps that have been assigned across all partitions\n \
                        If a new partition is added, a new timestamp should be:\n  \
                        - strictly greater than the last timestamp\n");
        return false;
    }
    true
}

/// This function determines the expected format of the consistency metadata as a function
/// of the encoding and the envelope of the source.
/// Specifically:
/// 1) an OCF file source with a Debezium envelope will expect an OCF Avro consistency source
/// that follows the TRX_METADATA_SCHEMA Avro spec outlined above
/// 2) any other file source with a Debezium envelope will expect an Avro consistency source
/// that follows the TRX_METADATA_SCHEMA Avro spec outlined above
/// 3) any source that uses the Text/Regex/Csv/Byte format will expect a consistency source that
/// is formatted using the text
/// 4) any source that uses the Protobuf format currently expects a consistency source that is formatted
/// using the text format (SourceName,PartitionCount,Partition,Timestamp,Offset)
/// 5) any source that uses the Avro format currently expects a consistency source that is formatted
/// using the BYO_CONSISTENCY_SCHEMA Avro spec outlined above.
///
fn identify_consistency_format(enc: DataEncoding, env: Envelope) -> ConsistencyFormatting {
    if let Envelope::Debezium(_) = env {
        if let DataEncoding::AvroOcf { reader_schema: _ } = enc {
            ConsistencyFormatting::DebeziumOcf
        } else {
            ConsistencyFormatting::DebeziumAvro
        }
    } else {
        match enc {
            DataEncoding::AvroOcf { reader_schema: _ } => ConsistencyFormatting::ByoAvroOcf,
            DataEncoding::Avro(_) => ConsistencyFormatting::ByoAvro,
            _ => ConsistencyFormatting::ByoBytes,
        }
    }
}

impl Timestamper {
    pub fn new(
        config: &TimestampConfig,
        storage: Arc<Mutex<storage::Connection>>,
        tx: futures::channel::mpsc::UnboundedSender<coord::Message>,
        rx: std::sync::mpsc::Receiver<TimestampMessage>,
    ) -> Self {
        // Recover existing data by running max on the timestamp count. This will ensure that
        // there will never be two duplicate entries and that there is a continuous stream
        // of timestamp updates across reboots
        let max_ts = if config.persist_ts {
            storage
                .lock()
                .expect("lock poisoned")
                .prepare("SELECT MAX(timestamp) FROM timestamps")
                .expect("Failed to prepare statement")
                .query_row(NO_PARAMS, |row| {
                    let res: Result<SqlVal<u64>, _> = row.get(2);
                    match res {
                        Ok(res) => Ok(res.0),
                        _ => Ok(0),
                    }
                })
                .expect("Failure to parse timestamp")
        } else {
            0
        };

        info!(
            "Starting Timestamping Thread. Frequency: {} ms.",
            config.frequency.as_millis()
        );

        Self {
            rt_sources: HashMap::new(),
            byo_sources: HashMap::new(),
            storage,
            tx,
            rx,
            current_timestamp: max_ts,
            timestamp_frequency: config.frequency,
            persist_ts: config.persist_ts,
        }
    }

    fn storage(&self) -> MutexGuard<storage::Connection> {
        self.storage.lock().expect("lock poisoned")
    }

    /// Run the update function in a loop at the specified frequency. Acquires timestamps using
    /// either 1) the Kafka topic ground truth 2) real-time
    pub fn update(&mut self) {
        loop {
            thread::sleep(self.timestamp_frequency);
            let shutdown = self.update_sources();
            if shutdown {
                break;
            } else {
                self.update_rt_timestamp();
                self.update_byo_timestamp();
            }
        }
    }

    /// Implements the real-time timestamping logic
    fn update_rt_timestamp(&mut self) {
        let watermarks = self.rt_query_sources();
        self.rt_generate_next_timestamp();
        if self.persist_ts {
            self.rt_persist_timestamp(&watermarks);
        }
        for (id, partition_count, pid, offset) in watermarks {
            MAX_TIMESTAMP
                .with_label_values(&[&id.to_string(), &pid.to_string()])
                .set(self.current_timestamp.try_into().unwrap());
            self.tx
                .unbounded_send(coord::Message::AdvanceSourceTimestamp {
                    id,
                    partition_count,
                    pid,
                    timestamp: self.current_timestamp,
                    offset,
                })
                .expect("Failed to send timestamp update to coordinator");
        }
    }

    /// Updates list of timestamp sources based on coordinator information. If using
    /// using the real-time timestamping logic, then maintain a list of Kafka consumers
    /// that poll topics to check how much data has been generated. If using the Kafka
    /// source timestamping logic, then keep a mapping of (name,id) to translate user-
    /// defined timestamps to GlobalIds
    fn update_sources(&mut self) -> bool {
        // First check if there are some new source that we should
        // start checking
        while let Ok(update) = self.rx.try_recv() {
            // TODO: Add metric creation in here so we don't create label strings every
            // time we update
            match update {
                TimestampMessage::Add(id, sc) => {
                    let (sc, enc, env, cons, max_ts_batch) = if let SourceConnector::External {
                        connector,
                        encoding,
                        envelope,
                        consistency,
                        max_ts_batch,
                    } = sc
                    {
                        (connector, encoding, envelope, consistency, max_ts_batch)
                    } else {
                        panic!("A Local Source should never be timestamped");
                    };
                    if !self.rt_sources.contains_key(&id) && !self.byo_sources.contains_key(&id) {
                        // Did not know about source, must update
                        match cons {
                            Consistency::RealTime => {
                                info!("Timestamping Source {} with Real Time Consistency. Max Timestamp Batch {}", id, max_ts_batch);
                                let start_offset = match sc {
                                    ExternalSourceConnector::Kafka(KafkaSourceConnector {
                                        start_offset,
                                        ..
                                    }) => MzOffset {
                                        offset: start_offset,
                                    },
                                    _ => MzOffset { offset: 0 },
                                };
                                // Obtain the last offsets that were recorded for this source.
                                // If start_offset is specified, make sure to always start from
                                // last_offset and never earlier
                                let last_offsets = if self.persist_ts {
                                    self.rt_recover_source(id)
                                        .iter()
                                        .map(|(pid, offs)| {
                                            (
                                                pid.clone(),
                                                MzOffset {
                                                    offset: std::cmp::max(
                                                        start_offset.offset,
                                                        offs.offset,
                                                    ),
                                                },
                                            )
                                        })
                                        .collect()
                                } else {
                                    HashMap::new()
                                };
                                let consumer = self.create_rt_connector(
                                    id,
                                    sc,
                                    last_offsets,
                                    start_offset,
                                    max_ts_batch,
                                );
                                if let Some(consumer) = consumer {
                                    self.rt_sources.insert(id, consumer);
                                }
                            }
                            Consistency::BringYourOwn(consistency_topic) => {
                                info!("Timestamping Source {} with BYO Consistency. Consistency Source: {}. Max Timestamp Batch: {}", id, consistency_topic, max_ts_batch);
                                let consumer = self.create_byo_connector(
                                    id,
                                    sc,
                                    enc,
                                    env,
                                    consistency_topic,
                                    max_ts_batch,
                                );
                                if let Some(consumer) = consumer {
                                    self.byo_sources.insert(id, consumer);
                                }
                            }
                        }
                    }
                }
                TimestampMessage::DropInstance(id) => {
                    info!("Dropping Timestamping for Source {}.", id);
                    self.storage()
                        .prepare_cached("DELETE FROM timestamps WHERE sid = ? AND vid = ?")
                        .expect("Failed to prepare delete statement")
                        .execute(params![SqlVal(&id.sid), SqlVal(&id.vid)])
                        .expect("Failed to execute delete statement");
                    if let Some(RtTimestampConsumer {
                        connector: RtTimestampConnector::Kafka(RtKafkaConnector { state, .. }),
                        ..
                    }) = self.rt_sources.remove(&id)
                    {
                        state.stop.store(true, Ordering::SeqCst);
                    }
                    self.byo_sources.remove(&id);
                }
                TimestampMessage::Shutdown => return true,
            }
        }
        false
    }

    /// Implements the byo timestamping logic
    ///
    /// If the partition count remains the same:
    /// A new timestamp should be
    /// 1) strictly greater than the last timestamp in this partition
    /// 2) greater or equal to all the timestamps that have been assigned so far across all partitions
    /// If the partition count increases:
    /// A new timestamp should be:
    /// 1) strictly greater than the last timestamp
    /// This is necessary to guarantee that this timestamp *could not have been closed yet*
    ///
    /// Supports two envelopes: None and Debezium. Currentlye compatible with Debezium format 1.1
    fn update_byo_timestamp(&mut self) {
        for (id, byo_consumer) in &mut self.byo_sources {
            // Get the next set of messages from the Consistency topic
            let messages = byo_query_source(byo_consumer);
            match byo_consumer.envelope {
                ConsistencyFormatting::ByoBytes => {
                    for (partition_count, partition, timestamp, offset) in
                        byo_extract_update_from_bytes(byo_consumer, messages)
                    {
                        if is_ts_valid(&byo_consumer, partition_count, &partition, timestamp) {
                            match byo_consumer.connector {
                                ByoTimestampConnector::Kafka(_)
                                | ByoTimestampConnector::File(_)
                                | ByoTimestampConnector::Ocf(_) => {
                                    if byo_consumer.current_partition_count < partition_count {
                                        // A new partition has been added. Partitions always gets added with
                                        // newPartitionId = previousLastPartitionId + 1 and start from 0.
                                        // So this new partition will have ID "partition_count - 1"
                                        // We ensure that the first messages in this partition will always have
                                        // timestamps > the last closed timestamp. We need to explicitly close
                                        // out all prior timestamps. To achieve this, we send an additional
                                        // timestamp message to the coord/worker

                                        // This can currently only happen in Kafka streams as File/OCF sources
                                        // do not support partitions
                                        self.tx
                                            .unbounded_send(
                                                coord::Message::AdvanceSourceTimestamp {
                                                    id: *id,
                                                    partition_count, // The new partition count
                                                    pid: PartitionId::Kafka(partition_count - 1), // the ID of the new partition
                                                    timestamp: byo_consumer.last_ts,
                                                    offset: MzOffset { offset: 0 }, // An offset of 0 will "fast-forward" the stream, it denotes
                                                                                    // the empty interval
                                                },
                                            )
                                            .expect("Failed to send update to coordinator");
                                    }
                                    byo_consumer.current_partition_count = partition_count;
                                    byo_consumer.last_ts = timestamp;
                                    byo_consumer
                                        .last_partition_ts
                                        .insert(partition.clone(), timestamp);
                                    self.tx
                                        .unbounded_send(coord::Message::AdvanceSourceTimestamp {
                                            id: *id,
                                            partition_count,
                                            pid: partition,
                                            timestamp,
                                            offset,
                                        })
                                        .expect("Failed to send update to coordinator");
                                }
                                _ => {
                                    error!(
                                        "BYO consistency is not supported for this source type."
                                    );
                                    return;
                                }
                            }
                        }
                    }
                }
                ConsistencyFormatting::DebeziumAvro => {
                    for msg in messages {
                        let msg = if let ValueEncoding::Bytes(msg) = msg {
                            msg
                        } else {
                            panic!("Kafka Debezium consistency should only encode byte messages");
                        };
                        // The first 5 bytes are reserved for the schema id/schema registry information
                        let mut bytes = &msg[5..];
                        let res = avro::from_avro_datum(&DEBEZIUM_TRX_SCHEMA_VALUE, &mut bytes);
                        let results = match res {
                            Ok(record) => {
                                if let Value::Record(record) = record {
                                    parse_debezium(record)
                                } else {
                                    error!("Incorrect Avro format. Expected Record");
                                    vec![]
                                }
                            }
                            Err(_) => {
                                // This message was a key message. We can safely ignore it
                                vec![]
                            }
                        };
                        for (topic, count) in results {
                            if byo_consumer.source_name == topic.trim() {
                                // TODO(natacha): consistency topic for Debezium currently supports only one partition
                                byo_consumer.last_offset.offset += count;
                                byo_consumer.last_ts += 1;
                                // Debezium consistency topic should only work for single-partition
                                // topics
                                self.tx
                                    .unbounded_send(coord::Message::AdvanceSourceTimestamp {
                                        id: *id,
                                        partition_count: 1,
                                        pid: match byo_consumer.connector {
                                            ByoTimestampConnector::File(_)
                                            | ByoTimestampConnector::Ocf(_) => PartitionId::File,
                                            ByoTimestampConnector::Kafka(_) => {
                                                PartitionId::Kafka(0)
                                            }
                                            ByoTimestampConnector::Kinesis(_) => {
                                                PartitionId::Kinesis(String::new())
                                            }
                                        },
                                        timestamp: byo_consumer.last_ts,
                                        offset: byo_consumer.last_offset,
                                    })
                                    .expect("Failed to send update to coordinator");
                            }
                        }
                    }
                }
                ConsistencyFormatting::DebeziumOcf => {
                    for msg in messages {
                        let msg = if let ValueEncoding::Avro(value) = msg {
                            value
                        } else {
                            panic!("Debezium OCF consistency should only encode Value messages");
                        };
                        let results = if let Value::Record(record) = msg {
                            parse_debezium(record)
                        } else {
                            panic!("Incorrect Avro Format. This should never happen");
                        };
                        for (topic, count) in results {
                            if byo_consumer.source_name == topic.trim() {
                                // TODO(natacha): consistency topic for Debezium currently supports only one partition
                                byo_consumer.last_offset.offset += count;
                                byo_consumer.last_ts += 1;
                                // Debezium consistency topic should only work for single-partition
                                // topics
                                self.tx
                                    .unbounded_send(coord::Message::AdvanceSourceTimestamp {
                                        id: *id,
                                        partition_count: 1,
                                        pid: match byo_consumer.connector {
                                            ByoTimestampConnector::File(_)
                                            | ByoTimestampConnector::Ocf(_) => PartitionId::File,
                                            ByoTimestampConnector::Kafka(_) => {
                                                PartitionId::Kafka(0)
                                            }
                                            ByoTimestampConnector::Kinesis(_) => {
                                                PartitionId::Kinesis(String::new())
                                            }
                                        },
                                        timestamp: byo_consumer.last_ts,
                                        offset: byo_consumer.last_offset,
                                    })
                                    .expect("Failed to send update to coordinator");
                            }
                        }
                    }
                }
                ConsistencyFormatting::ByoAvro => {
                    for msg in messages {
                        let msg = if let ValueEncoding::Bytes(msg) = msg {
                            msg
                        } else {
                            panic!("Byo Avro consistency should only encode byte messages");
                        };
                        // The first 5 bytes are reserved for the schema id/schema registry information
                        let mut bytes = &msg[5..];
                        let res = avro::from_avro_datum(&BYO_CONSISTENCY_SCHEMA, &mut bytes);
                        let (topic, partition_count, partition, timestamp, offset) = match res {
                            Ok(record) => {
                                if let Value::Record(record) = record {
                                    parse_byo(record)
                                } else {
                                    error!("Incorrect Avro format. Expected Record");
                                    continue;
                                }
                            }
                            Err(e) => {
                                error!("Incorrect Avro Format. Error: {}", e);
                                continue;
                            }
                        };
                        if topic == byo_consumer.source_name {
                            if is_ts_valid(byo_consumer, partition_count, &partition, timestamp) {
                                match byo_consumer.connector {
                                    ByoTimestampConnector::Kafka(_)
                                    | ByoTimestampConnector::File(_) => {
                                        byo_consumer.update_and_send(
                                            &self.tx,
                                            *id,
                                            partition_count,
                                            partition.clone(),
                                            timestamp,
                                            offset,
                                        );
                                    }
                                    _ => {
                                        error!(
                                            "BYO consistency is not supported for this source type."
                                        );
                                        return;
                                    }
                                }
                            }
                        }
                    }
                }
                ConsistencyFormatting::ByoAvroOcf => {
                    for msg in messages {
                        let msg = if let ValueEncoding::Avro(value) = msg {
                            value
                        } else {
                            panic!("Byo Avro consistency should only encode byte messages");
                        };
                        let partition_count = 1;
                        let partition = PartitionId::File;
                        let (topic, _, _, timestamp, offset) = if let Value::Record(record) = msg {
                            parse_byo(record)
                        } else {
                            error!("Incorrect Avro format. Expected Record");
                            continue;
                        };
                        if topic == byo_consumer.source_name {
                            if is_ts_valid(byo_consumer, partition_count, &partition, timestamp) {
                                match byo_consumer.connector {
                                    ByoTimestampConnector::Ocf(_) => {
                                        byo_consumer.update_and_send(
                                            &self.tx,
                                            *id,
                                            partition_count,
                                            partition.clone(),
                                            timestamp,
                                            offset,
                                        );
                                    }
                                    _ => {
                                        error!(
                                            "BYO OCF consistency is not supported for this source type."
                                        );
                                        return;
                                    }
                                }
                            }
                        }
                    }
                }
            }
        }
    }

    /// Creates a RT connector
    fn create_rt_connector(
        &self,
        id: SourceInstanceId,
        sc: ExternalSourceConnector,
        last_partition_offset: HashMap<PartitionId, MzOffset>,
        start_offset: MzOffset,
        max_ts_batch: i64,
    ) -> Option<RtTimestampConsumer> {
        match sc {
            ExternalSourceConnector::Kafka(kc) => {
                self.create_rt_kafka_connector(id, kc)
                    .map(|connector| RtTimestampConsumer {
                        connector: RtTimestampConnector::Kafka(connector),
                        last_partition_offset,
                        start_offset,
                        max_ts_batch,
                    })
            }
            ExternalSourceConnector::File(fc) => {
                if start_offset.offset > 0 {
                    warn!("Start Offset is not supported for file sources. Ignoring");
                }
                self.create_rt_file_connector(id, fc, max_ts_batch)
                    .map(|connector| RtTimestampConsumer {
                        connector: RtTimestampConnector::File(connector),
                        last_partition_offset,
                        start_offset: MzOffset { offset: 0 },
                        max_ts_batch,
                    })
            }
            ExternalSourceConnector::AvroOcf(fc) => {
                if start_offset.offset > 0 {
                    warn!("Start Offset is not supported for Avro OCF sources. Ignoring");
                }
                self.create_rt_ocf_connector(id, fc, max_ts_batch)
                    .map(|connector| RtTimestampConsumer {
                        connector: RtTimestampConnector::Ocf(connector),
                        last_partition_offset,
                        start_offset: MzOffset { offset: 0 },
                        max_ts_batch,
                    })
            }
            ExternalSourceConnector::Kinesis(kinc) => {
                if start_offset.offset > 0 {
                    warn!("Start Offset is not supported for Kinesis sources. Ignoring");
                }
                self.create_rt_kinesis_connector(id, kinc)
                    .map(|connector| RtTimestampConsumer {
                        connector: RtTimestampConnector::Kinesis(connector),
                        last_partition_offset,
                        start_offset: MzOffset { offset: 0 },
                        max_ts_batch,
                    })
            }
        }
    }

    fn create_byo_file_connector(
        &self,
        _id: SourceInstanceId,
        fc: &FileSourceConnector,
        timestamp_topic: String,
        max_ts_batch: i64,
    ) -> Option<ByoFileConnector<std::vec::Vec<u8>, failure::Error>> {
        let ctor = |fi| Ok(std::io::BufReader::new(fi).split(b'\n'));
        let (tx, rx) = if max_ts_batch > 0 {
            std::sync::mpsc::sync_channel(max_ts_batch as usize)
        } else {
            std::sync::mpsc::sync_channel(10000)
        };
        let tail = if fc.tail {
            FileReadStyle::TailFollowFd
        } else {
            FileReadStyle::ReadOnce
        };
        std::thread::spawn(move || {
            read_file_task(PathBuf::from(timestamp_topic), tx, None, tail, ctor);
        });

        Some(ByoFileConnector { stream: rx })
    }

    fn create_rt_kinesis_connector(
        &self,
        _id: SourceInstanceId,
        kinc: KinesisSourceConnector,
    ) -> Option<RtKinesisConnector> {
        let (kinesis_client, cached_shard_ids) = match block_on(aws_util::kinesis::kinesis_client(
            kinc.region.clone(),
            kinc.access_key_id.clone(),
            kinc.secret_access_key.clone(),
            kinc.token.clone(),
        )) {
            Ok(kinesis_client) => {
                let cached_shard_ids = match block_on(aws_util::kinesis::get_shard_ids(
                    &kinesis_client,
                    &kinc.stream_name,
                )) {
                    Ok(shard_ids) => shard_ids,
                    Err(e) => {
                        error!(
                            "Initializing KinesisSourceConnector with empty shard list: {}",
                            e
                        );
                        HashSet::new()
                    }
                };

                (Some(kinesis_client), Some(cached_shard_ids))
            }
            Err(e) => {
                error!("Hit error trying to create KinesisClient for Timestamper. Timestamps will not update for source based on Kinesis stream {}. {:#?}", kinc.stream_name, e);
                (None, None)
            }
        };

        Some(RtKinesisConnector {
            stream_name: kinc.stream_name,
            kinesis_client,
            cached_shard_ids,
            timestamper_iteration_count: 0,
        })
    }

    fn create_rt_kafka_connector(
        &self,
        id: SourceInstanceId,
        kc: KafkaSourceConnector,
    ) -> Option<RtKafkaConnector> {
        let mut config = ClientConfig::new();
        config.set("bootstrap.servers", &kc.url.to_string());

        if log_enabled!(target: "librdkafka", log::Level::Debug) {
            config.set("debug", "all");
        }

        for (k, v) in &kc.config_options {
            config.set(k, v);
        }

        let consumer = match config.create::<BaseConsumer>() {
            Ok(consumer) => consumer,
            Err(e) => {
                error!("Failed to create Kafka Consumer {}", e);
                return None;
            }
        };

        let connector = RtKafkaConnector {
            state: Arc::new(RtKafkaState {
                stop: AtomicBool::new(false),
                high_watermarks: Mutex::new(vec![]),
            }),
            id,
            topic: kc.topic,
        };

        // Metadata fetch requests on production Kafka clusters can take
        // more than 1s to complete. This makes it far too expensive to run on
        // the main timestamping thread.
        thread::spawn({
            let connector = connector.clone();
            let timestamp_frequency = self.timestamp_frequency;
            move || rt_kafka_metadata_fetch_loop(connector, consumer, timestamp_frequency)
        });

        Some(connector)
    }

    fn create_rt_ocf_connector(
        &self,
        _id: SourceInstanceId,
        fc: FileSourceConnector,
        max_ts_batch: i64,
    ) -> Option<RtFileConnector<avro::types::Value, failure::Error>> {
        let ctor = move |file| avro::Reader::new(file);
        let (tx, rx) = if max_ts_batch > 0 {
            std::sync::mpsc::sync_channel(max_ts_batch as usize)
        } else {
            std::sync::mpsc::sync_channel(10000)
        };
        let tail = if fc.tail {
            FileReadStyle::TailFollowFd
        } else {
            FileReadStyle::ReadOnce
        };
        std::thread::spawn(move || {
            read_file_task(fc.path, tx, None, tail, ctor);
        });

        Some(RtFileConnector { stream: rx })
    }

    fn create_rt_file_connector(
        &self,
        _id: SourceInstanceId,
        fc: FileSourceConnector,
        max_ts_batch: i64,
    ) -> Option<RtFileConnector<std::vec::Vec<u8>, failure::Error>> {
        let ctor = |fi| Ok(std::io::BufReader::new(fi).split(b'\n'));
        let (tx, rx) = if max_ts_batch > 0 {
            std::sync::mpsc::sync_channel(max_ts_batch as usize)
        } else {
            std::sync::mpsc::sync_channel(10000)
        };
        let tail = if fc.tail {
            FileReadStyle::TailFollowFd
        } else {
            FileReadStyle::ReadOnce
        };
        std::thread::spawn(move || {
            read_file_task(fc.path, tx, None, tail, ctor);
        });

        Some(RtFileConnector { stream: rx })
    }

    fn create_byo_ocf_connector(
        &self,
        _id: SourceInstanceId,
        fc: &FileSourceConnector,
        timestamp_topic: String,
        max_ts_batch: i64,
    ) -> Option<ByoFileConnector<avro::types::Value, failure::Error>> {
        let ctor = move |file| avro::Reader::new(file);
        let tail = if fc.tail {
            FileReadStyle::TailFollowFd
        } else {
            FileReadStyle::ReadOnce
        };
        let (tx, rx) = if max_ts_batch > 0 {
            std::sync::mpsc::sync_channel(max_ts_batch as usize)
        } else {
            std::sync::mpsc::sync_channel(10000 as usize)
        };
        std::thread::spawn(move || {
            read_file_task(PathBuf::from(timestamp_topic), tx, None, tail, ctor);
        });

        Some(ByoFileConnector { stream: rx })
    }

    /// Creates a BYO connector
    fn create_byo_connector(
        &self,
        id: SourceInstanceId,
        sc: ExternalSourceConnector,
        enc: DataEncoding,
        env: Envelope,
        timestamp_topic: String,
        max_ts_batch: i64,
    ) -> Option<ByoTimestampConsumer> {
        match sc {
            ExternalSourceConnector::Kafka(kc) => {
                let topic = kc.topic.clone();
                match self.create_byo_kafka_connector(id, &kc, timestamp_topic) {
                    Some(connector) => Some(ByoTimestampConsumer {
                        source_name: topic,
                        source_id: id,
                        connector: ByoTimestampConnector::Kafka(connector),
                        envelope: identify_consistency_format(enc, env),
                        last_partition_ts: HashMap::new(),
                        last_ts: 0,
                        current_partition_count: 1,
                        max_ts_batch,
                        last_offset: MzOffset { offset: 0 },
                    }),
                    None => None,
                }
            }
            ExternalSourceConnector::File(fc) => {
                match self.create_byo_file_connector(id, &fc, timestamp_topic, max_ts_batch) {
                    Some(consumer) => Some(ByoTimestampConsumer {
                        source_name: fc.path.to_string_lossy().into_owned(),
                        source_id: id,
                        connector: ByoTimestampConnector::File(consumer),
                        envelope: identify_consistency_format(enc, env),
                        last_partition_ts: HashMap::new(),
                        last_ts: 0,
                        max_ts_batch,
                        current_partition_count: 1,
                        last_offset: MzOffset { offset: 0 },
                    }),
                    None => None,
                }
            }
            ExternalSourceConnector::AvroOcf(fc) => {
                match self.create_byo_ocf_connector(id, &fc, timestamp_topic, max_ts_batch) {
                    Some(consumer) => Some(ByoTimestampConsumer {
                        source_name: fc.path.to_string_lossy().into_owned(),
                        source_id: id,
                        connector: ByoTimestampConnector::Ocf(consumer),
                        envelope: identify_consistency_format(enc, env),
                        last_partition_ts: HashMap::new(),
                        last_ts: 0,
                        max_ts_batch,
                        current_partition_count: 1,
                        last_offset: MzOffset { offset: 0 },
                    }),
                    None => None,
                }
            }
            ExternalSourceConnector::Kinesis(kinc) => {
                match self.create_byo_kinesis_connector(id, &kinc, timestamp_topic) {
                    Some(consumer) => Some(ByoTimestampConsumer {
                        source_name: kinc.stream_name,
                        source_id: id,
                        connector: ByoTimestampConnector::Kinesis(consumer),
                        envelope: identify_consistency_format(enc, env),
                        last_partition_ts: HashMap::new(),
                        last_ts: 0,
                        current_partition_count: 1,
                        max_ts_batch,
                        last_offset: MzOffset { offset: 0 },
                    }),
                    None => None,
                }
            }
        }
    }

    fn create_byo_kinesis_connector(
        &self,
        _id: SourceInstanceId,
        _kinc: &KinesisSourceConnector,
        _timestamp_topic: String,
    ) -> Option<ByoKinesisConnector> {
        unimplemented!();
    }

    fn create_byo_kafka_connector(
        &self,
        id: SourceInstanceId,
        kc: &KafkaSourceConnector,
        timestamp_topic: String,
    ) -> Option<ByoKafkaConnector> {
        let mut config = ClientConfig::new();
        config
            .set("enable.auto.commit", "false")
            .set("enable.partition.eof", "false")
            .set("auto.offset.reset", "earliest")
            .set("session.timeout.ms", "6000")
            .set("max.poll.interval.ms", "300000") // 5 minutes
            .set("fetch.message.max.bytes", "134217728")
            .set("enable.sparse.connections", "true")
            .set("bootstrap.servers", &kc.url.to_string());

        let group_id_prefix = kc.group_id_prefix.clone().unwrap_or_else(String::new);
        config.set(
            "group.id",
            &format!(
                "{}materialize-byo-{}-{}",
                group_id_prefix, &timestamp_topic, id
            ),
        );

        for (k, v) in &kc.config_options {
            config.set(k, v);
        }

        match config.create() {
            Ok(consumer) => {
                let consumer = ByoKafkaConnector::new(consumer);
                consumer.consumer.subscribe(&[&timestamp_topic]).unwrap();

                match get_kafka_partitions(
                    &consumer.consumer,
                    &timestamp_topic,
                    Duration::from_secs(1),
                )
                .as_ref()
                .map(Deref::deref)
                {
                    Ok([]) => {
                        warn!(
                            "Consistency topic {} does not exist; assuming it will exist soon",
                            timestamp_topic
                        );
                        Some(consumer)
                    }
                    Ok([_]) => Some(consumer),
                    Ok(partitions) => {
                        error!(
                            "Consistency topic should contain a single partition. Contains {}",
                            partitions.len(),
                        );
                        None
                    }
                    Err(e) => {
                        error!(
                            "Unable to fetch metadata about consistency topic {}; \
                             assuming it exists with one partition (error: {})",
                            timestamp_topic, e
                        );
                        Some(consumer)
                    }
                }
            }
            Err(e) => {
                error!("Could not create a Kafka consumer. Error: {}", e);
                None
            }
        }
    }

    /// Recovers any existing timestamp updates for that (SourceId,ViewId) pair from the underlying
    /// SQL database. Notifies the coordinator of these updates
    fn rt_recover_source(&mut self, id: SourceInstanceId) -> HashMap<PartitionId, MzOffset> {
        let ts_updates: Vec<_> = self
            .storage()
            .prepare("SELECT pcount, pid, timestamp, offset FROM timestamps WHERE sid = ? AND vid = ? ORDER BY timestamp")
            .expect("Failed to execute select statement")
            .query_and_then(params![SqlVal(&id.sid), SqlVal(&id.vid)], |row| -> Result<_, failure::Error> {
                let pcount: SqlVal<i32> = row.get(0)?;
                let pid: SqlVal<PartitionId> = match row.get(1) {
                    Ok(val) => val,
                    Err(_err) => {
                        // Historically, pid was an i32 value. If the found value is not of type
                        // PartitionId, try to read an i32.
                        let pid: SqlVal<i32> = row.get(1)?;
                        SqlVal(PartitionId::Kafka(pid.0))
                    },
                };
                let timestamp: SqlVal<u64> = row.get(2)?;
                let offset: SqlVal<i64> = row.get(3)?;
                Ok((pcount.0, pid.0, timestamp.0, offset.0))
            })
            .expect("Failed to parse SQL result")
            .collect();

        let mut max_offset = HashMap::new();
        for row in ts_updates {
            let (partition_count, pid, timestamp, offset) =
                row.expect("Failed to parse SQL result");
            if offset
                > max_offset
                    .entry(pid.clone())
                    .or_insert(MzOffset { offset: 0 })
                    .offset
            {
                max_offset.insert(pid.clone(), MzOffset { offset });
            }
            self.tx
                .unbounded_send(coord::Message::AdvanceSourceTimestamp {
                    id,
                    partition_count,
                    pid,
                    timestamp,
                    offset: MzOffset { offset },
                })
                .expect("Failed to send timestamp update to coordinator");
        }
        max_offset
    }

    /// Query real-time sources for the current max offset that has been generated for that source
    ///
    /// Set the new timestamped offset to min(max_offset, last_offset + increment_size): this ensures
    /// that we never create an overly large batch of messages for the same timestamp (which would
    /// prevent views from becoming visible in a timely fashion)
    fn rt_query_sources(&mut self) -> Vec<(SourceInstanceId, i32, PartitionId, MzOffset)> {
        let mut result = vec![];
        for (id, cons) in self.rt_sources.iter_mut() {
            match &mut cons.connector {
                RtTimestampConnector::Kafka(kc) => {
                    let high_watermarks = kc.state.high_watermarks.lock().expect("lock poisoned");
                    let partition_count: i32 = high_watermarks
                        .len()
                        .try_into()
                        .expect("invalid partition count");
                    for (p, high_watermark) in high_watermarks.iter().enumerate() {
                        let p: i32 = p.try_into().expect("invalid partition id");
                        let current_p_offset = cons
                            .last_partition_offset
                            .entry(PartitionId::Kafka(p))
                            .or_insert(cons.start_offset);
                        let old_offset = *current_p_offset;
                        // high here corresponds to the next available Kafka offset.
                        // Ex: a stream with 0 records will return a high of 0
                        // a stream with one record (written at offset 0) will return a high of 1
                        // high - 1 corresponds the Kafka Offset of the last *currently* available
                        // message
                        let current_max_kafka_offset = KafkaOffset {
                            offset: high_watermark - 1,
                        };
                        *current_p_offset = determine_next_offset(
                            *current_p_offset,
                            current_max_kafka_offset.into(),
                            cons.max_ts_batch,
                        );
                        result.push((
                            *id,
                            partition_count,
                            PartitionId::Kafka(p),
                            *current_p_offset,
                        ));
                        KAFKA_PARTITION_OFFSET_MAX
                            .with_label_values(&[&kc.topic, &id.to_string(), &p.to_string()])
                            .set(*high_watermark);
                        assert!(*current_p_offset >= old_offset);
                    }
                }
                RtTimestampConnector::File(ref mut fc) => {
                    let mut count = 0;
                    let last_offset = cons
                        .last_partition_offset
                        .entry(PartitionId::File)
                        .or_insert(cons.start_offset);
                    while cons.max_ts_batch == 0 || count < cons.max_ts_batch {
                        match fc.stream.try_recv() {
                            Ok(Ok(_)) => {
                                count += 1;
                            }
                            Ok(Err(e)) => {
                                error!("Failed to read file for timestamping: {}", e);
                                break;
                            }
                            Err(TryRecvError::Empty) => break,
                            Err(TryRecvError::Disconnected) => break,
                        }
                    }
                    last_offset.offset += count;
                    result.push((*id, 1, PartitionId::File, *last_offset))
                }
                RtTimestampConnector::Ocf(ref mut fc) => {
                    let last_offset = cons
                        .last_partition_offset
                        .entry(PartitionId::File)
                        .or_insert(cons.start_offset);
                    let mut count = 0;
                    while cons.max_ts_batch == 0 || count < cons.max_ts_batch {
                        match fc.stream.try_recv() {
                            Ok(Ok(_)) => {
                                count += 1;
                            }
                            Ok(Err(e)) => {
                                error!("Failed to read file for timestamping: {}", e);
                                break;
                            }
                            Err(TryRecvError::Empty) => break,
                            Err(TryRecvError::Disconnected) => break,
                        }
                    }
                    last_offset.offset += count;
                    result.push((*id, 1, PartitionId::File, *last_offset))
                }
                RtTimestampConnector::Kinesis(kc) => {
                    // The Kinesis ListShards API is rate limited to 100 transactions per second
                    // per stream. Only hit the ListShards API every 100 Timestamper iterations to update
                    // our cached Shard ids.
                    match &kc.kinesis_client {
                        None => {
                            // KinesisClient was not initialized, skip update.
                            continue;
                        }
                        Some(kinesis_client) => {
                            if kc.timestamper_iteration_count % 100 == 0 {
                                match block_on(aws_util::kinesis::get_shard_ids(kinesis_client, &kc.stream_name)) {
                                    Ok(shard_ids) => {
                                        kc.cached_shard_ids = Some(shard_ids);
                                    }
                                    Err(e) => error!(
                                        "Error listing Shard ids for Kinesis stream {}, using cached Shard ids: {:#?}",
                                        kc.stream_name,
                                        e
                                    ),
                                };
                            }
                            kc.timestamper_iteration_count += 1;
                        }
                    }

                    if let Some(shard_ids) = &kc.cached_shard_ids {
                        let num_shards = i32::try_from(shard_ids.len()).unwrap_or_else(|_| {
                            error!("Unable to convert number of Kinesis shards ({}) for stream {} into i32", shard_ids.len(), kc.stream_name);
                            0
                        });
                        for shard_id in shard_ids {
                            // For now, always just push the current system timestamp.
                            // todo@jldlaughlin #2219
                            result.push((
                                *id,
                                num_shards,
                                PartitionId::Kinesis(shard_id.clone()),
                                MzOffset {
                                    offset: self.current_timestamp as i64,
                                },
                            ));
                        }
                    }
                }
            }
        }
        result
    }

    /// Persist timestamp updates to the underlying storage when using the
    /// real-time timestamping logic.
    fn rt_persist_timestamp(&self, ts_updates: &[(SourceInstanceId, i32, PartitionId, MzOffset)]) {
        let storage = self.storage();
        for (id, pcount, pid, offset) in ts_updates {
            let mut stmt = storage
                .prepare_cached(
                    "INSERT INTO timestamps (sid, vid, pcount, pid, timestamp, offset) VALUES (?, ?, ?, ?, ?, ?)",
                )
                .expect(
                    "Failed to prepare insert statement into persistent store. \
                     Hint: increase the system file descriptor limit.",
                );
            while let Err(e) = stmt.execute(params![
                SqlVal(&id.sid),
                SqlVal(&id.vid),
                SqlVal(&pcount),
                SqlVal(&pid),
                SqlVal(&self.current_timestamp),
                SqlVal(&offset.offset)
            ]) {
                error!(
                    "Failed to insert statement into persistent store: {}. \
                     Hint: increase the system file descriptor limit.",
                    e
                );
                std::thread::sleep(Duration::from_secs(1));
            }
        }
    }

    /// Generates a timestamp that is guaranteed to be monotonically increasing.
    /// This may require multiple calls to the underlying now() system method, which is not
    /// guaranteed to increase monotonically
    fn rt_generate_next_timestamp(&mut self) {
        // TODO[reliability] (brennan) - If someone does something silly like sets their
        // system clock backward by an hour while mz is running,
        // we will hang here for an hour.
        let mut new_ts = 0;
        while new_ts <= self.current_timestamp {
            let start = SystemTime::now();
            new_ts = start
                .duration_since(UNIX_EPOCH)
                .expect("Time went backwards")
                .as_millis() as u64;
        }
        assert!(new_ts > self.current_timestamp);
        self.current_timestamp = new_ts;
    }
}

fn rt_kafka_metadata_fetch_loop(c: RtKafkaConnector, consumer: BaseConsumer, wait: Duration) {
    debug!(
        "Starting realtime Kafka thread for {} (source {})",
        &c.topic, &c.id
    );

    let mut high_watermarks = vec![]; // high watermarks for each partition, in order
    let mut i: usize = 0;
    while !c.state.stop.load(Ordering::SeqCst) {
        // Fetch metadata to check for new partitions. We do this less
        // frequently than every tick because partitions are added very
        // infrequently in production. We do this a bit more frequently than is
        // ideal in order to speed up tests that dynamically add partitions.
        if high_watermarks.len() == 0 || i % 10 == 0 {
            match get_kafka_partitions(&consumer, &c.topic, Duration::from_secs(15)) {
                Ok(partitions) => match partitions.len().cmp(&high_watermarks.len()) {
                    cmp::Ordering::Greater => {
                        let diff = partitions.len() - high_watermarks.len();
                        info!(
                            "Discovered {} new ({} total) kafka partitions for topic {} (source {})",
                            diff, partitions.len(), c.topic, c.id,
                        );
                        high_watermarks.resize(partitions.len(), 0);
                    }
                    cmp::Ordering::Less => {
                        error!(
                            "Ignoring decrease in partitions (from {} to {}) for topic {} (source {})",
                            high_watermarks.len(), partitions.len(), c.topic, c.id,
                        );
                    }
                    cmp::Ordering::Equal => (),
                },
                Err(e) => {
                    error!(
                        "Unable to fetch kafka metadata for topic {} (source {}): {}",
                        c.topic, e, c.id
                    );
                }
            }
        }

        // Fetch the latest offset for each partition.
        //
        // TODO(benesch): Kafka supports fetching these in bulk, but
        // rust-rdkafka does not. That would save us a lot of requests on
        // large topics.
        for i in 0..high_watermarks.len() {
            let pid: i32 = i.try_into().expect("invalid partition id");
            match consumer.fetch_watermarks(&c.topic, pid, Duration::from_secs(15)) {
                Ok((_low, high)) => high_watermarks[i] = high,
                Err(e) => {
                    error!(
                        "Unable to fetch Kafka watermarks for topic {} [{}] ({}): {}",
                        c.topic, pid, c.id, e
                    );
                }
            }
        }

        // Export the offsets to the shared state.
        c.state
            .high_watermarks
            .lock()
            .expect("lock poisoned")
            .clone_from(&high_watermarks);

        i += 1;
        thread::sleep(wait);
    }

    debug!("Terminating realtime Kafka thread for {}", &c.topic);
}
