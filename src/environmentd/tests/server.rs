// Copyright Materialize, Inc. and contributors. All rights reserved.
//
// Use of this software is governed by the Business Source License
// included in the LICENSE file.
//
// As of the Change Date specified in that file, in accordance with
// the Business Source License, use of this software will be governed
// by the Apache License, Version 2.0.

// BEGIN LINT CONFIG
// DO NOT EDIT. Automatically generated by bin/gen-lints.
// Have complaints about the noise? See the note in misc/python/materialize/cli/gen-lints.py first.
#![allow(clippy::style)]
#![allow(clippy::complexity)]
#![allow(clippy::large_enum_variant)]
#![allow(clippy::mutable_key_type)]
#![allow(clippy::stable_sort_primitive)]
#![allow(clippy::map_entry)]
#![allow(clippy::box_default)]
#![warn(clippy::bool_comparison)]
#![warn(clippy::clone_on_ref_ptr)]
#![warn(clippy::no_effect)]
#![warn(clippy::unnecessary_unwrap)]
#![warn(clippy::dbg_macro)]
#![warn(clippy::todo)]
#![warn(clippy::wildcard_dependencies)]
#![warn(clippy::zero_prefixed_literal)]
#![warn(clippy::borrowed_box)]
#![warn(clippy::deref_addrof)]
#![warn(clippy::double_must_use)]
#![warn(clippy::double_parens)]
#![warn(clippy::extra_unused_lifetimes)]
#![warn(clippy::needless_borrow)]
#![warn(clippy::needless_question_mark)]
#![warn(clippy::needless_return)]
#![warn(clippy::redundant_pattern)]
#![warn(clippy::redundant_slicing)]
#![warn(clippy::redundant_static_lifetimes)]
#![warn(clippy::single_component_path_imports)]
#![warn(clippy::unnecessary_cast)]
#![warn(clippy::useless_asref)]
#![warn(clippy::useless_conversion)]
#![warn(clippy::builtin_type_shadow)]
#![warn(clippy::duplicate_underscore_argument)]
#![warn(clippy::double_neg)]
#![warn(clippy::unnecessary_mut_passed)]
#![warn(clippy::wildcard_in_or_patterns)]
#![warn(clippy::crosspointer_transmute)]
#![warn(clippy::excessive_precision)]
#![warn(clippy::overflow_check_conditional)]
#![warn(clippy::as_conversions)]
#![warn(clippy::match_overlapping_arm)]
#![warn(clippy::zero_divided_by_zero)]
#![warn(clippy::must_use_unit)]
#![warn(clippy::suspicious_assignment_formatting)]
#![warn(clippy::suspicious_else_formatting)]
#![warn(clippy::suspicious_unary_op_formatting)]
#![warn(clippy::mut_mutex_lock)]
#![warn(clippy::print_literal)]
#![warn(clippy::same_item_push)]
#![warn(clippy::useless_format)]
#![warn(clippy::write_literal)]
#![warn(clippy::redundant_closure)]
#![warn(clippy::redundant_closure_call)]
#![warn(clippy::unnecessary_lazy_evaluations)]
#![warn(clippy::partialeq_ne_impl)]
#![warn(clippy::redundant_field_names)]
#![warn(clippy::transmutes_expressible_as_ptr_casts)]
#![warn(clippy::unused_async)]
#![warn(clippy::disallowed_methods)]
#![warn(clippy::disallowed_macros)]
#![warn(clippy::disallowed_types)]
#![warn(clippy::from_over_into)]
// END LINT CONFIG

//! Integration tests for Materialize server.

use std::collections::{BTreeMap, VecDeque};
use std::fmt::Write;
use std::net::Ipv4Addr;
use std::process::{Command, Stdio};
use std::sync::{Arc, Mutex};
use std::time::{Duration, Instant};
use std::{iter, thread};

use anyhow::bail;
use chrono::{DateTime, Utc};
use futures::FutureExt;
use http::StatusCode;
use itertools::Itertools;
use mz_environmentd::http::{
    BecomeLeaderResponse, BecomeLeaderResult, LeaderStatus, LeaderStatusResponse,
};
use mz_environmentd::WebSocketResponse;
use mz_ore::cast::CastLossy;
use mz_ore::now::NowFn;
use mz_ore::retry::Retry;
use mz_ore::task;
use mz_pgrepr::UInt8;
use mz_sql::session::user::SYSTEM_USER;
use rand::RngCore;
use reqwest::blocking::Client;
use reqwest::Url;
use serde::{Deserialize, Serialize};
use tempfile::TempDir;
use tokio_postgres::error::SqlState;
use tracing::info;
use tungstenite::error::ProtocolError;
use tungstenite::{Error, Message};

use crate::util::{Listeners, PostgresErrorExt, KAFKA_ADDRS};

pub mod util;

#[mz_ore::test]
fn test_persistence() {
    let data_dir = tempfile::tempdir().unwrap();
    let config = util::Config::default()
        .data_directory(data_dir.path())
        .unsafe_mode();

    {
        let server = util::start_server(config.clone()).unwrap();
        let mut client = server.connect(postgres::NoTls).unwrap();
        client
            .batch_execute(&format!(
                "CREATE CONNECTION kafka_conn TO KAFKA (BROKER '{}')",
                &*KAFKA_ADDRS,
            ))
            .unwrap();
        client
            .batch_execute(
                "CREATE SOURCE src FROM KAFKA CONNECTION kafka_conn (TOPIC 'ignored') FORMAT BYTES",
            )
            .unwrap();
        client
            .batch_execute("CREATE VIEW constant AS SELECT 1")
            .unwrap();
        client.batch_execute(
            "CREATE VIEW mat (a, a_data, c, c_data) AS SELECT 'a', data, 'c' AS c, data FROM src",
        ).unwrap();
        client.batch_execute("CREATE DEFAULT INDEX ON mat").unwrap();
        client.batch_execute("CREATE DATABASE d").unwrap();
        client.batch_execute("CREATE SCHEMA d.s").unwrap();
        client
            .batch_execute("CREATE VIEW d.s.v AS SELECT 1")
            .unwrap();
    }

    let server = util::start_server(config).unwrap();
    let mut client = server.connect(postgres::NoTls).unwrap();
    assert_eq!(
        client
            .query("SHOW VIEWS", &[])
            .unwrap()
            .into_iter()
            .map(|row| row.get(0))
            .collect::<Vec<String>>(),
        &["constant", "mat"]
    );
    assert_eq!(
        client
            .query_one("SHOW INDEXES ON mat", &[])
            .unwrap()
            .get::<_, Vec<String>>("key"),
        &["a", "a_data", "c", "c_data"],
    );
    assert_eq!(
        client
            .query("SHOW VIEWS FROM d.s", &[])
            .unwrap()
            .into_iter()
            .map(|row| row.get(0))
            .collect::<Vec<String>>(),
        &["v"]
    );

    // Test that catalog recovery correctly populates `mz_objects`.
    assert_eq!(
        client
            .query(
                "SELECT id FROM mz_objects WHERE id LIKE 'u%' ORDER BY 1",
                &[]
            )
            .unwrap()
            .into_iter()
            .map(|row| row.get(0))
            .collect::<Vec<String>>(),
        vec!["u1", "u2", "u3", "u4", "u5", "u6", "u7"]
    );
}

// Test that sources and sinks require an explicit `SIZE` parameter outside of
// unsafe mode.
#[mz_ore::test]
fn test_source_sink_size_required() {
    let server = util::start_server(util::Config::default()).unwrap();
    let mut client = server.connect(postgres::NoTls).unwrap();

    // Sources bail without an explicit size.
    let result = client.batch_execute("CREATE SOURCE lg FROM LOAD GENERATOR COUNTER");
    assert_eq!(
        result.unwrap_err().unwrap_db_error().message(),
        "must specify either cluster or size option"
    );

    // Sources work with an explicit size.
    client
        .batch_execute("CREATE SOURCE lg FROM LOAD GENERATOR COUNTER WITH (SIZE '1')")
        .unwrap();

    // `ALTER SOURCE ... RESET SIZE` is banned.
    let result = client.batch_execute("ALTER SOURCE lg RESET (SIZE)");
    assert_eq!(
        result.unwrap_err().unwrap_db_error().message(),
        "must specify either cluster or size option"
    );

    client
        .batch_execute(&format!(
            "CREATE CONNECTION conn TO KAFKA (BROKER '{}')",
            &*KAFKA_ADDRS,
        ))
        .unwrap();

    // Sinks bail without an explicit size.
    let result = client.batch_execute("CREATE SINK snk FROM mz_sources INTO KAFKA CONNECTION conn (TOPIC 'foo') FORMAT JSON ENVELOPE DEBEZIUM");
    assert_eq!(
        result.unwrap_err().unwrap_db_error().message(),
        "must specify either cluster or size option"
    );

    // Sinks work with an explicit size.
    client.batch_execute("CREATE SINK snk FROM mz_sources INTO KAFKA CONNECTION conn (TOPIC 'foo') FORMAT JSON ENVELOPE DEBEZIUM WITH (SIZE '1')").unwrap();

    // `ALTER SINK ... RESET SIZE` is banned.
    let result = client.batch_execute("ALTER SINK snk RESET (SIZE)");
    assert_eq!(
        result.unwrap_err().unwrap_db_error().message(),
        "must specify either cluster or size option"
    );
}

// Test the POST and WS server endpoints.
#[mz_ore::test]
#[cfg_attr(miri, ignore)] // unsupported operation: can't call foreign function `epoll_wait` on OS `linux`
fn test_http_sql() {
    // Datadriven directives for WebSocket are "ws-text" and "ws-binary" to send
    // text or binary websocket messages that are the input. Output is
    // everything until and including the next ReadyForQuery message. An
    // optional "rows=N" argument can be given in the directive to produce
    // datadriven output after N rows. Any directive with rows=N should be the
    // final directive in a file, since it leaves the websocket in a
    // mid-statement state. A "fixtimestamp=true" argument can be given to
    // replace timestamps with "<TIMESTAMP>".
    //
    // Datadriven directive for HTTP POST is "http". Input and output are the
    // documented JSON formats.

    let fixtimestamp_re = regex::Regex::new("\\d{13}(\\.0)?").unwrap();
    let fixtimestamp_replace = "<TIMESTAMP>";

    datadriven::walk("tests/testdata/http", |f| {
        let server = util::start_server(util::Config::default()).unwrap();
        let ws_url = server.ws_addr();
        let http_url = Url::parse(&format!(
            "http://{}/api/sql",
            server.inner.http_local_addr()
        ))
        .unwrap();
        let (mut ws, _resp) = tungstenite::connect(ws_url).unwrap();
        let ws_init = util::auth_with_ws(&mut ws, BTreeMap::default()).unwrap();

        // Verify ws_init contains roughly what we expect. This varies (rng secret and version
        // numbers), so easier to test here instead of in the ws file.
        assert!(
            ws_init
                .iter()
                .filter(|m| matches!(m, WebSocketResponse::ParameterStatus(_)))
                .count()
                > 1
        );
        assert!(matches!(
            ws_init.last(),
            Some(WebSocketResponse::BackendKeyData(_))
        ));

        f.run(|tc| {
            let msg = match tc.directive.as_str() {
                "ws-text" => Message::Text(tc.input.clone()),
                "ws-binary" => Message::Binary(tc.input.as_bytes().to_vec()),
                "http" => {
                    let json: serde_json::Value = serde_json::from_str(&tc.input).unwrap();
                    let res = Client::new()
                        .post(http_url.clone())
                        .json(&json)
                        .send()
                        .unwrap();
                    return format!("{}\n{}\n", res.status(), res.text().unwrap());
                }
                _ => panic!("unknown directive {}", tc.directive),
            };
            let mut rows = tc
                .args
                .get("rows")
                .map(|rows| rows.get(0).map(|row| row.parse::<usize>().unwrap()))
                .flatten();
            let fixtimestamp = tc.args.get("fixtimestamp").is_some();
            ws.write_message(msg).unwrap();
            let mut responses = String::new();
            loop {
                let resp = ws.read_message().unwrap();
                match resp {
                    Message::Text(mut msg) => {
                        if fixtimestamp {
                            msg = fixtimestamp_re
                                .replace_all(&msg, fixtimestamp_replace)
                                .into();
                        }
                        let msg: WebSocketResponse = serde_json::from_str(&msg).unwrap();
                        write!(&mut responses, "{}\n", serde_json::to_string(&msg).unwrap())
                            .unwrap();
                        match msg {
                            WebSocketResponse::ReadyForQuery(_) => return responses,
                            WebSocketResponse::Row(_) => {
                                if let Some(rows) = rows.as_mut() {
                                    *rows -= 1;
                                    if *rows == 0 {
                                        return responses;
                                    }
                                }
                            }
                            _ => {}
                        }
                    }
                    Message::Ping(_) => continue,
                    _ => panic!("unexpected response: {:?}", resp),
                }
            }
        });
    });
}

// Test that the server properly handles cancellation requests.
#[mz_ore::test]
#[cfg_attr(miri, ignore)] // unsupported operation: can't call foreign function `epoll_wait` on OS `linux`
fn test_cancel_long_running_query() {
    let config = util::Config::default().unsafe_mode();
    let server = util::start_server(config).unwrap();

    let mut client = server.connect(postgres::NoTls).unwrap();
    let cancel_token = client.cancel_token();

    client.batch_execute("CREATE TABLE t (i INT)").unwrap();
    let (shutdown_tx, shutdown_rx) = std::sync::mpsc::channel();

    let handle = thread::spawn(move || {
        // Repeatedly attempt to abort the query because we're not sure exactly
        // when the SELECT will arrive.
        loop {
            thread::sleep(Duration::from_secs(1));
            match shutdown_rx.try_recv() {
                Ok(()) => return,
                Err(std::sync::mpsc::TryRecvError::Empty) => {
                    let _ = cancel_token.cancel_query(postgres::NoTls);
                }
                _ => panic!("unexpected"),
            }
        }
    });

    match client.simple_query("SELECT * FROM t AS OF 18446744073709551615") {
        Err(e) if e.code() == Some(&postgres::error::SqlState::QUERY_CANCELED) => {}
        Err(e) => panic!("expected error SqlState::QUERY_CANCELED, but got {:?}", e),
        Ok(_) => panic!("expected error SqlState::QUERY_CANCELED, but query succeeded"),
    }

    // Wait for the cancellation thread to stop.
    shutdown_tx.send(()).unwrap();
    handle.join().unwrap();

    client
        .simple_query("SELECT 1")
        .expect("simple query succeeds after cancellation");
}

fn test_cancellation_cancels_dataflows(query: &str) {
    let config = util::Config::default().unsafe_mode();
    let server = util::start_server(config).unwrap();
    server.enable_feature_flags(&["enable_with_mutually_recursive"]);

    let mut client1 = server.connect(postgres::NoTls).unwrap();
    let mut client2 = server.connect(postgres::NoTls).unwrap();
    let cancel_token = client1.cancel_token();

    client1.batch_execute("CREATE TABLE t (i INT)").unwrap();
    // No dataflows expected at startup.
    assert_eq!(
        client1
            .query_one(
                "SELECT count(*) FROM mz_internal.mz_dataflow_operators",
                &[]
            )
            .unwrap()
            .get::<_, i64>(0),
        0
    );

    thread::spawn(move || {
        // Wait until we see the expected dataflow.
        Retry::default()
            .retry(|_state| {
                let count: i64 = client2
                    .query_one(
                        "SELECT count(*) FROM mz_internal.mz_dataflow_operators",
                        &[],
                    )
                    .map_err(|_| ())
                    .unwrap()
                    .get(0);
                if count == 0 {
                    Err(())
                } else {
                    Ok(())
                }
            })
            .unwrap();
        cancel_token.cancel_query(postgres::NoTls).unwrap();
    });

    match client1.simple_query(query) {
        Err(e) if e.code() == Some(&postgres::error::SqlState::QUERY_CANCELED) => {}
        Err(e) => panic!("expected error SqlState::QUERY_CANCELED, but got {:?}", e),
        Ok(_) => panic!("expected error SqlState::QUERY_CANCELED, but query succeeded"),
    }
    // Expect the dataflows to shut down.
    Retry::default()
        .retry(|_state| {
            let count: i64 = client1
                .query_one(
                    "SELECT count(*) FROM mz_internal.mz_dataflow_operators",
                    &[],
                )
                .map_err(|_| ())
                .unwrap()
                .get(0);
            if count == 0 {
                Ok(())
            } else {
                Err(())
            }
        })
        .unwrap();
}

// Test that dataflow uninstalls cancelled peeks.
#[mz_ore::test]
#[cfg_attr(miri, ignore)] // unsupported operation: can't call foreign function `epoll_wait` on OS `linux`
fn test_cancel_dataflow_removal() {
    test_cancellation_cancels_dataflows("SELECT * FROM t AS OF 9223372036854775807");
}

#[mz_ore::test]
#[cfg_attr(miri, ignore)] // unsupported operation: can't call foreign function `epoll_wait` on OS `linux`
fn test_cancel_long_select() {
    test_cancellation_cancels_dataflows("WITH MUTUALLY RECURSIVE flip(x INTEGER) AS (VALUES(1) EXCEPT ALL SELECT * FROM flip) SELECT * FROM flip;");
}

#[mz_ore::test]
#[cfg_attr(miri, ignore)] // unsupported operation: can't call foreign function `epoll_wait` on OS `linux`
fn test_cancel_insert_select() {
    test_cancellation_cancels_dataflows("INSERT INTO t WITH MUTUALLY RECURSIVE flip(x INTEGER) AS (VALUES(1) EXCEPT ALL SELECT * FROM flip) SELECT * FROM flip;");
}

fn test_closing_connection_cancels_dataflows(query: String) {
    let config = util::Config::default().unsafe_mode();
    let server = util::start_server(config).unwrap();
    server.enable_feature_flags(&["enable_with_mutually_recursive"]);

    let mut cmd = Command::new("psql");
    let cmd = cmd
        .args([
            // Ignore .psqlrc so that local execution of testdrive isn't
            // affected by it.
            "--no-psqlrc",
            &format!(
                "postgres://{}:{}/materialize",
                Ipv4Addr::LOCALHOST,
                server.inner.sql_local_addr().port()
            ),
        ])
        .stdin(Stdio::piped());
    tracing::info!("spawning: {cmd:#?}");
    let mut child = cmd.spawn().expect("failed to spawn psql");
    let mut stdin = child.stdin.take().expect("failed to open stdin");
    thread::spawn(move || {
        use std::io::Write;
        stdin
            .write_all("SET STATEMENT_TIMEOUT = \"120s\";".as_bytes())
            .unwrap();
        stdin.write_all(query.as_bytes()).unwrap();
    });

    let spawned_psql = Instant::now();

    let mut client = server.connect(postgres::NoTls).unwrap();

    // Wait until we see the expected dataflow.
    Retry::default()
        .retry(|_state| {
            if spawned_psql.elapsed() > Duration::from_secs(30) {
                panic!("waited too long for psql to send the query");
            }
            let count: i64 = client
                .query_one(
                    "SELECT count(*) FROM mz_internal.mz_dataflow_operators",
                    &[],
                )
                .map_err(|_| ())
                .unwrap()
                .get(0);
            if count == 0 {
                Err(())
            } else {
                Ok(())
            }
        })
        .unwrap();

    let started = Instant::now();
    if let Some(wait_status) = child.try_wait().expect("wait shouldn't error") {
        panic!("child should still be running, it exitted with {wait_status}");
    }
    child.kill().expect("killing psql child");

    // Expect the dataflows to shut down.
    Retry::default()
        .retry(|_state| {
            if started.elapsed() > Duration::from_secs(30) {
                // this has to be less than statement timeout
                panic!("waited too long for dataflow cancellation");
            }
            let count: i64 = client
                .query_one(
                    "SELECT count(*) FROM mz_internal.mz_dataflow_operators",
                    &[],
                )
                .map_err(|_| ())
                .unwrap()
                .get(0);
            if count == 0 {
                Ok(())
            } else {
                Err(())
            }
        })
        .unwrap();
    info!(
        "Took {:#?} until dataflows were cancelled",
        started.elapsed()
    );
}

#[mz_ore::test]
#[cfg_attr(miri, ignore)] // unsupported operation: can't call foreign function `epoll_wait` on OS `linux`
fn test_closing_connection_for_long_select() {
    test_closing_connection_cancels_dataflows("WITH MUTUALLY RECURSIVE flip(x INTEGER) AS (VALUES(1) EXCEPT ALL SELECT * FROM flip) SELECT * FROM flip;".to_string())
}

#[mz_ore::test]
#[cfg_attr(miri, ignore)] // unsupported operation: can't call foreign function `epoll_wait` on OS `linux`
fn test_closing_connection_for_insert_select() {
    test_closing_connection_cancels_dataflows("CREATE TABLE t1 (a int); INSERT INTO t1 WITH MUTUALLY RECURSIVE flip(x INTEGER) AS (VALUES(1) EXCEPT ALL SELECT * FROM flip) SELECT * FROM flip;".to_string())
}

#[mz_ore::test]
fn test_storage_usage_collection_interval() {
    /// Waits for the next storage collection to occur, then returns the
    /// timestamp at which the collection occurred. The timestamp of the last
    /// collection must be provided
    fn wait_for_next_collection(
        client: &mut postgres::Client,
        last_timestamp: DateTime<Utc>,
    ) -> DateTime<Utc> {
        info!("waiting for next storage usage collection");
        let ts = Retry::default()
            .max_duration(Duration::from_secs(10))
            .retry(|_| {
                let row = client.query_one(
                    "SELECT max(collection_timestamp)
                    FROM mz_internal.mz_storage_usage_by_shard",
                    &[],
                )?;
                // mz_storage_usage_by_shard may not be populated yet, which would result in a NULL ts.
                let ts = row.try_get::<_, DateTime<Utc>>("max")?;
                if ts <= last_timestamp {
                    bail!("next collection has not yet occurred")
                }
                Ok(ts)
            })
            .unwrap();
        info!(%ts, "detected storage usage collection");
        ts
    }

    fn get_shard_id(client: &mut postgres::Client, name: &str) -> String {
        let row = Retry::default()
            .max_duration(Duration::from_secs(10))
            .retry(|_| {
                client.query_one(
                    "SELECT shard_id
                     FROM mz_internal.mz_storage_shards s
                     JOIN mz_objects o ON o.id = s.object_id
                     WHERE o.name = $1",
                    &[&name],
                )
            })
            .unwrap();
        row.get("shard_id")
    }

    fn get_storage_usage(
        client: &mut postgres::Client,
        shard_id: &str,
        collection_timestamp: DateTime<Utc>,
    ) -> u64 {
        let row = Retry::default()
            .max_duration(Duration::from_secs(10))
            .retry(|_| {
                client.query_one(
                    "SELECT coalesce(sum(size_bytes), 0)::uint8 AS size
                     FROM mz_internal.mz_storage_usage_by_shard
                     WHERE shard_id = $1 AND collection_timestamp = $2",
                    &[&shard_id, &collection_timestamp],
                )
            })
            .unwrap();
        row.get::<_, UInt8>("size").0
    }

    let config =
        util::Config::default().with_storage_usage_collection_interval(Duration::from_secs(1));
    let server = util::start_server(config).unwrap();
    let mut client = server.connect(postgres::NoTls).unwrap();

    // Wait for the initial storage usage collection to occur.
    let timestamp = wait_for_next_collection(&mut client, DateTime::<Utc>::MIN_UTC);

    // Create a table with no data.
    client
        .batch_execute("CREATE TABLE usage_test (a int)")
        .unwrap();
    let shard_id = get_shard_id(&mut client, "usage_test");
    info!(%shard_id, "created table");

    // Test that the storage usage for the table was zero before it was
    // created.
    let pre_create_storage_usage = get_storage_usage(&mut client, &shard_id, timestamp);
    info!(%pre_create_storage_usage);
    assert_eq!(pre_create_storage_usage, 0);

    // Test that the storage usage for the table is nonzero after it is created
    // (there is some overhead even for empty tables). We wait out two storage
    // collection intervals (here and below) because the next storage collection
    // may have been concurrent with the previous operation.
    let timestamp = wait_for_next_collection(&mut client, timestamp);
    let timestamp = wait_for_next_collection(&mut client, timestamp);
    let post_create_storage_usage = get_storage_usage(&mut client, &shard_id, timestamp);
    info!(%post_create_storage_usage);
    assert!(post_create_storage_usage > 0);

    // Insert some data into the table.
    for _ in 0..3 {
        client
            .batch_execute("INSERT INTO usage_test VALUES (1)")
            .unwrap();
    }

    // Test that the storage usage for the table is larger than it was before.
    let timestamp = wait_for_next_collection(&mut client, timestamp);
    let timestamp = wait_for_next_collection(&mut client, timestamp);
    let after_insert_storage_usage = get_storage_usage(&mut client, &shard_id, timestamp);
    info!(%after_insert_storage_usage);
    assert!(after_insert_storage_usage > post_create_storage_usage);

    // Drop the table.
    client.batch_execute("DROP TABLE usage_test").unwrap();

    // Test that the storage usage is reported as zero.
    let timestamp = wait_for_next_collection(&mut client, timestamp);
    let timestamp = wait_for_next_collection(&mut client, timestamp);
    let after_drop_storage_usage = get_storage_usage(&mut client, &shard_id, timestamp);
    info!(%after_drop_storage_usage);
    assert_eq!(after_drop_storage_usage, 0);
}

#[mz_ore::test]
fn test_storage_usage_updates_between_restarts() {
    let data_dir = tempfile::tempdir().unwrap();
    let storage_usage_collection_interval = Duration::from_secs(3);
    let config = util::Config::default()
        .with_storage_usage_collection_interval(storage_usage_collection_interval)
        .data_directory(data_dir.path());

    // Wait for initial storage usage collection.
    let initial_timestamp: f64 = {
        let server = util::start_server(config.clone()).unwrap();
        let mut client = server.connect(postgres::NoTls).unwrap();
        // Retry because it may take some time for the initial snapshot to be taken.
        Retry::default().max_duration(Duration::from_secs(60)).retry(|_| {
            client
                    .query_one(
                        "SELECT EXTRACT(EPOCH FROM MAX(collection_timestamp))::float8 FROM mz_catalog.mz_storage_usage;",
                        &[],
                    )
                    .map_err(|e| e.to_string()).unwrap()
                    .try_get::<_, f64>(0)
                    .map_err(|e| e.to_string())
        }).unwrap()
    };

    std::thread::sleep(storage_usage_collection_interval);

    // Another storage usage collection should be scheduled immediately.
    {
        let server = util::start_server(config).unwrap();
        let mut client = server.connect(postgres::NoTls).unwrap();

        // Retry until storage usage is updated.
        Retry::default().max_duration(Duration::from_secs(60)).retry(|_| {
            let updated_timestamp = client
                .query_one("SELECT EXTRACT(EPOCH FROM MAX(collection_timestamp))::float8 FROM mz_catalog.mz_storage_usage;", &[])
                .map_err(|e| e.to_string()).unwrap()
                .try_get::<_, f64>(0)
                .map_err(|e| e.to_string()).unwrap();

            if updated_timestamp > initial_timestamp {
                Ok(())
            } else {
                Err(format!("updated storage collection timestamp {updated_timestamp} is not greater than initial timestamp {initial_timestamp}"))
            }
        }).unwrap();
    }
}

#[mz_ore::test]
#[cfg_attr(coverage, ignore)] // https://github.com/MaterializeInc/materialize/issues/18896
fn test_storage_usage_doesnt_update_between_restarts() {
    let data_dir = tempfile::tempdir().unwrap();
    let storage_usage_collection_interval = Duration::from_secs(10);
    let config = util::Config::default()
        .with_storage_usage_collection_interval(storage_usage_collection_interval)
        .data_directory(data_dir.path());

    // Wait for initial storage usage collection.
    let initial_timestamp = {
        let server = util::start_server(config.clone()).unwrap();
        let mut client = server.connect(postgres::NoTls).unwrap();
        // Retry because it may take some time for the initial snapshot to be taken.
        Retry::default().max_duration(Duration::from_secs(60)).retry(|_| {
                client
                    .query_one(
                        "SELECT DISTINCT(EXTRACT(EPOCH FROM MAX(collection_timestamp))::float8) FROM mz_catalog.mz_storage_usage;",
                        &[],
                    )
                    .map_err(|e| e.to_string()).unwrap()
                    .try_get::<_, f64>(0)
                    .map_err(|e| e.to_string())
            }).unwrap()
    };

    // Another storage usage collection should not be scheduled immediately.
    {
        // Give plenty of time so we don't accidentally do another collection if this test is slow.
        let config = config.with_storage_usage_collection_interval(Duration::from_secs(60 * 1000));
        let server = util::start_server(config).unwrap();
        let mut client = server.connect(postgres::NoTls).unwrap();

        let collection_timestamps = client
            .query(
                "SELECT DISTINCT(EXTRACT(EPOCH FROM collection_timestamp)::float8) as epoch FROM mz_catalog.mz_storage_usage ORDER BY epoch DESC LIMIT 2;",
                &[],
            ).unwrap();
        match collection_timestamps.len() {
            0 => panic!("storage usage disappeared"),
            1 => assert_eq!(initial_timestamp, collection_timestamps[0].get::<_, f64>(0)),
            // It's possible that after collecting the first usage timestamp but before shutting the
            // server down, we collect another usage timestamp.
            2 => {
                let most_recent_timestamp = collection_timestamps[0].get::<_, f64>(0);
                let second_most_recent_timestamp = collection_timestamps[1].get::<_, f64>(0);

                let actual_collection_interval =
                    most_recent_timestamp - second_most_recent_timestamp;
                let expected_collection_interval: f64 =
                    f64::cast_lossy(storage_usage_collection_interval.as_secs());

                assert!(actual_collection_interval >= expected_collection_interval);
            }
            _ => unreachable!("query is limited to 2"),
        }
    }
}

#[mz_ore::test]
fn test_storage_usage_collection_interval_timestamps() {
    let config =
        util::Config::default().with_storage_usage_collection_interval(Duration::from_secs(5));
    let server = util::start_server(config).unwrap();
    let mut client = server.connect(postgres::NoTls).unwrap();

    // Retry because it may take some time for the initial snapshot to be taken.
    Retry::default().max_duration(Duration::from_secs(10)).retry(|_| {
        let rows = client
            .query(
                "SELECT collection_timestamp, SUM(size_bytes)::int8 FROM mz_catalog.mz_storage_usage GROUP BY collection_timestamp ORDER BY collection_timestamp;",
                &[],
            )
            .map_err(|e| e.to_string()).unwrap();
        if rows.len() == 1 {
            Ok(())
        } else {
            Err(format!("expected a single timestamp, instead found {}", rows.len()))
        }
    }).unwrap();
}

#[mz_ore::test]
fn test_old_storage_usage_records_are_reaped_on_restart() {
    let now = Arc::new(Mutex::new(0));
    let now_fn = {
        let timestamp = Arc::clone(&now);
        NowFn::from(move || *timestamp.lock().expect("lock poisoned"))
    };
    let data_dir = tempfile::tempdir().unwrap();
    let collection_interval = Duration::from_secs(1);
    let retention_period = Duration::from_millis(1100);
    let config = util::Config::default()
        .with_now(now_fn)
        .with_storage_usage_collection_interval(collection_interval)
        .with_storage_usage_retention_period(retention_period)
        .data_directory(data_dir.path());

    let initial_timestamp = {
        let server = util::start_server(config.clone()).unwrap();
        let mut client = server.connect(postgres::NoTls).unwrap();
        // Create a table with no data, which should have some overhead and therefore some storage usage
        client
            .batch_execute("CREATE TABLE usage_test (a int)")
            .unwrap();

        *now.lock().expect("lock poisoned") +=
            u64::try_from(collection_interval.as_millis()).expect("known to fit") + 1;

        // Wait for initial storage usage collection, to be sure records are present.
        let initial_timestamp = Retry::default().max_duration(Duration::from_secs(5)).retry(|_| {
            client
                    .query_one(
                        "SELECT EXTRACT(EPOCH FROM MAX(collection_timestamp))::integer FROM mz_internal.mz_storage_usage_by_shard;",
                        &[],
                    )
                    .map_err(|e| e.to_string()).unwrap()
                    .try_get::<_, i32>(0)
                    .map_err(|e| e.to_string())
        }).expect("Could not fetch initial timestamp");

        let initial_server_usage_records = client
            .query_one(
                "SELECT COUNT(*)::integer AS number
                     FROM mz_internal.mz_storage_usage_by_shard",
                &[],
            )
            .unwrap()
            .try_get::<_, i32>(0)
            .expect("Could not get initial count of records");

        info!(%initial_timestamp, %initial_server_usage_records);
        assert!(
            initial_server_usage_records >= 1,
            "No initial server usage records!"
        );

        initial_timestamp
    };

    // Push time forward, start a new server, and assert that the previous storage records have been reaped
    *now.lock().expect("lock poisoned") = u64::try_from(initial_timestamp)
        .expect("negative timestamps are impossible")
        + u64::try_from(retention_period.as_millis()).expect("known to fit")
        + 1;

    {
        let server = util::start_server(config).unwrap();
        let mut client = server.connect(postgres::NoTls).unwrap();

        *now.lock().expect("lock poisoned") +=
            u64::try_from(collection_interval.as_millis()).expect("known to fit") + 1;

        let subsequent_initial_timestamp = Retry::default().max_duration(Duration::from_secs(5)).retry(|_| {
            client
                .query_one(
                    "SELECT EXTRACT(EPOCH FROM MIN(collection_timestamp))::integer FROM mz_internal.mz_storage_usage_by_shard;",
                    &[],
                )
                .map_err(|e| e.to_string()).unwrap()
                .try_get::<_, i32>(0)
                .map_err(|e| e.to_string())
        }).expect("Could not fetch initial timestamp");

        info!(%subsequent_initial_timestamp);
        assert!(
            subsequent_initial_timestamp > initial_timestamp,
            "Records were not reaped!"
        );
    };
}

#[mz_ore::test]
#[cfg_attr(miri, ignore)] // unsupported operation: can't call foreign function `epoll_wait` on OS `linux`
fn test_default_cluster_sizes() {
    let config = util::Config::default()
        .with_builtin_cluster_replica_size("1".to_string())
        .with_default_cluster_replica_size("2".to_string());
    let server = util::start_server(config).unwrap();
    let mut client = server.connect(postgres::NoTls).unwrap();

    let builtin_size: String = client
        .query(
            "SELECT size FROM (SHOW CLUSTER REPLICAS WHERE cluster LIKE 'mz_%')",
            &[],
        )
        .unwrap()
        .get(0)
        .unwrap()
        .get(0);
    assert_eq!(builtin_size, "1");

    let builtin_size: String = client
        .query(
            "SELECT size FROM (SHOW CLUSTER REPLICAS WHERE cluster = 'default')",
            &[],
        )
        .unwrap()
        .get(0)
        .unwrap()
        .get(0);
    assert_eq!(builtin_size, "2");
}

#[mz_ore::test]
#[cfg_attr(miri, ignore)] // unsupported operation: can't call foreign function `epoll_wait` on OS `linux`
fn test_max_request_size() {
    let statement = "SELECT $1::text";
    let statement_size = statement.bytes().count();
    let server = util::start_server(util::Config::default()).unwrap();

    // pgwire
    {
        let param_size = mz_pgwire::MAX_REQUEST_SIZE - statement_size + 1;
        let param = std::iter::repeat("1").take(param_size).join("");
        let mut client = server.connect(postgres::NoTls).unwrap();

        // The specific error isn't forwarded to the client, the connection is just closed.
        assert!(client.query(statement, &[&param]).is_err());
        assert!(client.is_valid(Duration::from_secs(2)).is_err());
    }

    // http
    {
        let param_size = mz_environmentd::http::MAX_REQUEST_SIZE - statement_size + 1;
        let param = std::iter::repeat("1").take(param_size).join("");
        let http_url = Url::parse(&format!(
            "http://{}/api/sql",
            server.inner.http_local_addr()
        ))
        .unwrap();
        let json = format!("{{\"queries\":[{{\"query\":\"{statement}\",\"params\":[{param}]}}]}}");
        let json: serde_json::Value = serde_json::from_str(&json).unwrap();
        let res = Client::new().post(http_url).json(&json).send().unwrap();
        assert_eq!(res.status(), StatusCode::PAYLOAD_TOO_LARGE);
    }

    // ws
    {
        let param_size = mz_environmentd::http::MAX_REQUEST_SIZE - statement_size + 1;
        let param = std::iter::repeat("1").take(param_size).join("");
        let ws_url = server.ws_addr();
        let (mut ws, _resp) = tungstenite::connect(ws_url).unwrap();
        util::auth_with_ws(&mut ws, BTreeMap::default()).unwrap();
        let json =
            format!("{{\"queries\":[{{\"query\":\"{statement}\",\"params\":[\"{param}\"]}}]}}");
        let json: serde_json::Value = serde_json::from_str(&json).unwrap();
        ws.write_message(Message::Text(json.to_string())).unwrap();

        // The specific error isn't forwarded to the client, the connection is just closed.
        let err = ws.read_message().unwrap_err();
        assert!(matches!(
            err,
            Error::Protocol(ProtocolError::ResetWithoutClosingHandshake)
        ));
    }
}

#[mz_ore::test]
#[cfg_attr(miri, ignore)] // too slow
fn test_max_statement_batch_size() {
    let statement = "SELECT 1;";
    let statement_size = statement.bytes().count();
    let max_statement_size = mz_sql_parser::parser::MAX_STATEMENT_BATCH_SIZE;
    let max_statement_count = max_statement_size / statement_size + 1;
    let statements = iter::repeat(statement).take(max_statement_count).join("");
    let server = util::start_server(util::Config::default()).unwrap();

    // pgwire
    {
        let mut client = server.connect(postgres::NoTls).unwrap();

        let err = client
            .batch_execute(&statements)
            .expect_err("statement should be too large")
            .unwrap_db_error();
        assert_eq!(&SqlState::PROGRAM_LIMIT_EXCEEDED, err.code());
        assert!(
            err.message().contains("statement batch size cannot exceed"),
            "error should indicate that the statement was too large: {}",
            err.message()
        );
    }

    // http
    {
        let http_url = Url::parse(&format!(
            "http://{}/api/sql",
            server.inner.http_local_addr()
        ))
        .unwrap();
        let json = format!("{{\"query\":\"{statements}\"}}");
        let json: serde_json::Value = serde_json::from_str(&json).unwrap();

        let res = Client::new().post(http_url).json(&json).send().unwrap();
        assert!(
            res.status().is_client_error(),
            "statement should result in an error: {res:?}"
        );
        let text = res.text().unwrap();
        assert!(
            text.contains("statement batch size cannot exceed"),
            "error should indicate that the statement was too large: {}",
            text
        );
    }

    // ws
    {
        let ws_url = server.ws_addr();
        let (mut ws, _resp) = tungstenite::connect(ws_url).unwrap();
        util::auth_with_ws(&mut ws, BTreeMap::default()).unwrap();
        let json = format!("{{\"query\":\"{statements}\"}}");
        let json: serde_json::Value = serde_json::from_str(&json).unwrap();
        ws.write_message(Message::Text(json.to_string())).unwrap();

        let msg = ws.read_message().unwrap();
        let msg = msg.into_text().expect("response should be text");
        let msg: WebSocketResponse = serde_json::from_str(&msg).unwrap();
        match msg {
            WebSocketResponse::Error(err) => assert!(
                err.message.contains("statement batch size cannot exceed"),
                "error should indicate that the statement was too large: {}",
                err.message,
            ),
            msg => {
                panic!("response should be error: {msg:?}")
            }
        }
    }
}

#[mz_ore::test]
#[cfg_attr(miri, ignore)] // unsupported operation: can't call foreign function `epoll_wait` on OS `linux`
fn test_mz_system_user_admin() {
    let config = util::Config::default();
    let server = util::start_server(config).unwrap();
    let mut client = server
        .pg_config_internal()
        .user(&SYSTEM_USER.name)
        .connect(postgres::NoTls)
        .unwrap();
    assert_eq!(
        "on".to_string(),
        client
            .query_one("SHOW is_superuser;", &[])
            .unwrap()
            .get::<_, String>(0)
    );
}

#[mz_ore::test]
#[cfg_attr(miri, ignore)] // too slow
fn test_ws_passes_options() {
    let server = util::start_server(util::Config::default()).unwrap();

    // Create our WebSocket.
    let ws_url = server.ws_addr();
    let (mut ws, _resp) = tungstenite::connect(ws_url).unwrap();
    let options = BTreeMap::from([(
        "application_name".to_string(),
        "billion_dollar_idea".to_string(),
    )]);
    util::auth_with_ws(&mut ws, options).unwrap();

    // Query to make sure we get back the correct session var, which should be
    // set from the options map we passed with the auth.
    let json = "{\"query\":\"SHOW application_name;\"}";
    let json: serde_json::Value = serde_json::from_str(json).unwrap();
    ws.write_message(Message::Text(json.to_string())).unwrap();

    let mut read_msg = || -> WebSocketResponse {
        let msg = ws.read_message().unwrap();
        let msg = msg.into_text().expect("response should be text");
        serde_json::from_str(&msg).unwrap()
    };
    let starting = read_msg();
    let columns = read_msg();
    let row_val = read_msg();

    if !matches!(starting, WebSocketResponse::CommandStarting(_)) {
        panic!("wrong message!, {starting:?}");
    };

    if let WebSocketResponse::Rows(rows) = columns {
        let names: Vec<&str> = rows.columns.iter().map(|c| c.name.as_str()).collect();
        assert_eq!(names, ["application_name"]);
    } else {
        panic!("wrong message!, {columns:?}");
    };

    if let WebSocketResponse::Row(row) = row_val {
        let expected = serde_json::Value::String("billion_dollar_idea".to_string());
        assert_eq!(row, [expected]);
    } else {
        panic!("wrong message!, {row_val:?}");
    }
}

#[mz_ore::test]
#[cfg_attr(miri, ignore)] // too slow
fn test_ws_notifies_for_bad_options() {
    let server = util::start_server(util::Config::default()).unwrap();

    // Create our WebSocket.
    let ws_url = server.ws_addr();
    let (mut ws, _resp) = tungstenite::connect(ws_url).unwrap();
    let options = BTreeMap::from([("bad_var_name".to_string(), "i_do_not_exist".to_string())]);
    util::auth_with_ws(&mut ws, options).unwrap();

    let mut read_msg = || -> WebSocketResponse {
        let msg = ws.read_message().unwrap();
        let msg = msg.into_text().expect("response should be text");
        serde_json::from_str(&msg).unwrap()
    };
    let notice = read_msg();

    // After startup, we should get a notice that our var name was not set.
    if let WebSocketResponse::Notice(notice) = notice {
        let msg = notice.message();
        assert!(msg.starts_with("startup setting bad_var_name not set"));
    } else {
        panic!("wrong message!, {notice:?}");
    };
}

#[derive(Debug, Deserialize)]
struct HttpResponse<R> {
    results: Vec<R>,
}

#[derive(Debug, Deserialize)]
struct HttpRows {
    rows: Vec<Vec<serde_json::Value>>,
    desc: HttpResponseDesc,
    notices: Vec<Notice>,
}

#[derive(Debug, Deserialize)]
struct HttpResponseDesc {
    columns: Vec<HttpResponseColumn>,
}

#[derive(Debug, Deserialize)]
struct HttpResponseColumn {
    name: String,
}

#[derive(Debug, Deserialize)]
struct Notice {
    message: String,
    #[serde(rename = "severity")]
    _severity: String,
}

#[mz_ore::test]
#[cfg_attr(miri, ignore)] // too slow
fn test_http_options_param() {
    let server = util::start_server(util::Config::default()).unwrap();

    #[derive(Debug, Serialize)]
    struct Params {
        options: String,
    }

    let make_request = |params| {
        let http_url = Url::parse(&format!(
            "http://{}/api/sql?{}",
            server.inner.http_local_addr(),
            params
        ))
        .unwrap();

        let json = r#"{ "query": "SHOW application_name;" }"#;
        let json: serde_json::Value = serde_json::from_str(json).unwrap();
        Client::new().post(http_url).json(&json).send().unwrap()
    };

    //
    // Happy path, valid and correctly formatted options dictionary.
    //
    let options = BTreeMap::from([("application_name", "yet_another_client")]);
    let options = serde_json::to_string(&options).unwrap();
    let params = serde_urlencoded::to_string(Params { options }).unwrap();

    let resp = make_request(params);
    assert!(resp.status().is_success());

    let mut result: HttpResponse<HttpRows> = resp.json().unwrap();
    assert_eq!(result.results.len(), 1);

    let mut rows = result.results.pop().unwrap();
    assert!(rows.notices.is_empty());

    let row = rows.rows.pop().unwrap().pop().unwrap();
    let col = rows.desc.columns.pop().unwrap().name;

    assert_eq!(col, "application_name");
    assert_eq!(row, "yet_another_client");

    //
    // Malformed options dictionary.
    //
    let params = "options=not_a_urlencoded_json_object".to_string();
    let resp = make_request(params);
    assert_eq!(resp.status(), StatusCode::BAD_REQUEST);

    //
    // Correctly formed options dictionary, but invalid param.
    //
    let options = BTreeMap::from([("not_a_session_var", "hmmmm")]);
    let options = serde_json::to_string(&options).unwrap();
    let params = serde_urlencoded::to_string(Params { options }).unwrap();

    let resp = make_request(params);
    assert!(resp.status().is_success());

    let result: HttpResponse<HttpRows> = resp.json().unwrap();
    assert_eq!(result.results.len(), 1);
    assert_eq!(result.results[0].notices.len(), 1);

    let notice = &result.results[0].notices[0];
    assert!(notice
        .message
        .contains(r#"startup setting not_a_session_var not set"#));
}

#[mz_ore::test]
#[cfg_attr(miri, ignore)] // too slow
fn test_max_connections_on_all_interfaces() {
    let query = "SELECT 1";
    let server = util::start_server(util::Config::default().unsafe_mode()).unwrap();

    let mut mz_client = server
        .pg_config_internal()
        .user(&SYSTEM_USER.name)
        .connect(postgres::NoTls)
        .unwrap();
    mz_client
        .batch_execute("ALTER SYSTEM SET max_connections = 1")
        .unwrap();

    let client = server.connect(postgres::NoTls).unwrap();

    let ws_url = server.ws_addr();
    let http_url = Url::parse(&format!(
        "http://{}/api/sql",
        server.inner.http_local_addr()
    ))
    .unwrap();
    let json = format!("{{\"query\":\"{query}\"}}");
    let json: serde_json::Value = serde_json::from_str(&json).unwrap();

    {
        // while postgres client is connected, http connections error out
        let res = Client::new()
            .post(http_url.clone())
            .json(&json)
            .send()
            .unwrap();
        let status = res.status();
        let text = res.text().expect("no body?");
        assert_eq!(status, StatusCode::INTERNAL_SERVER_ERROR);
        assert_eq!(text, "creating connection would violate max_connections limit (desired: 2, limit: 1, current: 1)");
    }

    {
        // while postgres client is connected, websockets can't auth
        let (mut ws, _resp) = tungstenite::connect(ws_url.clone()).unwrap();
        let err = util::auth_with_ws(&mut ws, BTreeMap::default()).unwrap_err();
        assert!(err.to_string().contains("creating connection would violate max_connections limit (desired: 2, limit: 1, current: 1)"), "{err}");
    }

    tracing::info!("closing postgres client");
    client.close().unwrap();
    tracing::info!("closed postgres client");

    tracing::info!("waiting for postgres client to close so that the query goes through");
    Retry::default().max_tries(10).retry(|_state| {
            let res = Client::new().post(http_url.clone()).json(&json).send().unwrap();
            let status = res.status();
            if status == StatusCode::INTERNAL_SERVER_ERROR {
                assert_eq!(res.text().expect("expect body"), "creating connection would violate max_connections limit (desired: 2, limit: 1, current: 1)");
                return Err(());
            }
            assert_eq!(status, StatusCode::OK);
            let result: HttpResponse<HttpRows> = res.json().unwrap();
            assert_eq!(result.results.len(), 1);
            assert_eq!(result.results[0].rows, vec![vec![1]]);
            Ok(())
        }).unwrap();

    tracing::info!("http query succeeded");
    let (mut ws, _resp) = tungstenite::connect(ws_url).unwrap();
    util::auth_with_ws(&mut ws, BTreeMap::default()).unwrap();
    let json = format!("{{\"query\":\"{query}\"}}");
    let json: serde_json::Value = serde_json::from_str(&json).unwrap();
    ws.write_message(Message::Text(json.to_string())).unwrap();

    // The specific error isn't forwarded to the client, the connection is just closed.
    match ws.read_message() {
        Ok(Message::Text(msg)) => {
            assert_eq!(
                msg,
                r#"{"type":"CommandStarting","payload":{"has_rows":true,"is_streaming":false}}"#
            );
            assert_eq!(
                ws.read_message().unwrap(),
                Message::Text("{\"type\":\"Rows\",\"payload\":{\"columns\":[{\"name\":\"?column?\",\"type_oid\":23,\"type_len\":4,\"type_mod\":-1}]}}".to_string())
            );
            assert_eq!(
                ws.read_message().unwrap(),
                Message::Text("{\"type\":\"Row\",\"payload\":[1]}".to_string())
            );
            tracing::info!("data: {:?}", ws.read_message().unwrap());
        }
        Ok(msg) => panic!("unexpected msg: {msg:?}"),
        Err(e) => panic!("{e}"),
    }

    // While the websocket connection is still open, http requests fail
    let res = Client::new().post(http_url).json(&json).send().unwrap();
    tracing::info!("res: {:#?}", res);
    let status = res.status();
    let text = res.text().expect("no body?");
    assert_eq!(status, StatusCode::INTERNAL_SERVER_ERROR);
    assert_eq!(text, "creating connection would violate max_connections limit (desired: 2, limit: 1, current: 1)");
}

#[mz_ore::test]
#[cfg_attr(miri, ignore)] // too slow
fn test_concurrent_id_reuse() {
    let server = util::start_server(util::Config::default()).unwrap();

    server.runtime.block_on(async {
        {
            let (client, conn) = server.connect_async(postgres::NoTls).await.unwrap();
            task::spawn(|| "conn await", async {
                conn.await.unwrap();
            });
            client
                .batch_execute("CREATE TABLE t (a INT);")
                .await
                .unwrap();
        }

        let http_url = Url::parse(&format!(
            "http://{}/api/sql",
            server.inner.http_local_addr()
        ))
        .unwrap();
        let select_json = "{\"queries\":[{\"query\":\"SELECT * FROM t;\",\"params\":[]}]}";
        let select_json: serde_json::Value = serde_json::from_str(select_json).unwrap();

        let insert_json = "{\"queries\":[{\"query\":\"INSERT INTO t VALUES (1);\",\"params\":[]}]}";
        let insert_json: serde_json::Value = serde_json::from_str(insert_json).unwrap();

        // The goal here is to start some connection `B`, after another connection `A` has
        // terminated, but while connection `A` still has some asynchronous work in flight. Then
        // connection `A` will terminate it's session after connection `B` has started it's own
        // session. If they use the same connection ID, then `A` will accidentally tear down `B`'s
        // state and `B` will panic at any point it tries to access it's state. If they don't use
        // the same connection ID, then everything will be fine.
        fail::cfg("async_prepare", "return(true)").unwrap();
        for i in 0..100 {
            let http_url = http_url.clone();
            if i % 2 == 0 {
                let fut = reqwest::Client::new()
                    .post(http_url)
                    .json(&select_json)
                    .send();
                let time = tokio::time::sleep(Duration::from_millis(500));
                futures::select! {
                    _ = fut.fuse() => {},
                    _ = time.fuse() => {},
                }
            } else {
                reqwest::Client::new()
                    .post(http_url)
                    .json(&insert_json)
                    .send()
                    .await
                    .unwrap();
            }
        }
    });

    let mut client = server.connect(postgres::NoTls).unwrap();
    client.batch_execute("SELECT 1").unwrap();
}

#[mz_ore::test]
#[cfg_attr(miri, ignore)] // too slow
fn test_leader_promotion() {
    let tmpdir = TempDir::new().unwrap();
    let config = util::Config::default()
        .unsafe_mode()
        .data_directory(tmpdir.path());
    {
        // start with a stash with no deploy generation to match current production
        let server = util::start_server(config.clone()).unwrap();
        let mut client = server.connect(postgres::NoTls).unwrap();
        client.simple_query("SELECT 1").unwrap();
    }
    {
        // propose a deploy generation for the first time
        let server = util::start_server(config.clone()).unwrap();
        let mut client = server.connect(postgres::NoTls).unwrap();
        client.simple_query("SELECT 1").unwrap();

        // make sure asking about the leader and promoting don't panic
        let res = Client::new()
            .get(
                Url::parse(&format!(
                    "http://{}/api/leader/status",
                    server.inner.internal_http_local_addr()
                ))
                .unwrap(),
            )
            .send()
            .unwrap();
        tracing::info!("response: {res:?}");
        assert_eq!(
            res.status(),
            StatusCode::OK,
            "{:?}",
            res.json::<serde_json::Value>()
        );

        let res = Client::new()
            .post(
                Url::parse(&format!(
                    "http://{}/api/leader/promote",
                    server.inner.internal_http_local_addr()
                ))
                .unwrap(),
            )
            .send()
            .unwrap();
        tracing::info!("response: {res:?}");
        assert_eq!(
            res.status(),
            StatusCode::OK,
            "{:?}",
            res.json::<serde_json::Value>()
        );
    }
    let config = config.with_deploy_generation(Some(2));
    {
        // start with different deploy generation
        let config = config.with_deploy_generation(Some(3));
        thread::scope(|s| {
            let listeners = Listeners::new().unwrap();
            let internal_http_addr = listeners.inner.internal_http_local_addr();
            let server_handle = s.spawn(|| listeners.serve(config).unwrap());

            let status_http_url =
                Url::parse(&format!("http://{}/api/leader/status", internal_http_addr)).unwrap();

            Retry::default()
                .max_tries(10)
                .retry(|_state| {
                    let res = Client::new().get(status_http_url.clone()).send().unwrap();
                    assert_eq!(res.status(), StatusCode::OK);
                    let response: LeaderStatusResponse = res.json().unwrap();
                    assert_ne!(response.status, LeaderStatus::IsLeader);
                    if response.status == LeaderStatus::ReadyToPromote {
                        Ok(())
                    } else {
                        Err(())
                    }
                })
                .unwrap();

            let promote_http_url =
                Url::parse(&format!("http://{}/api/leader/promote", internal_http_addr)).unwrap();

            let res = Client::new().post(promote_http_url.clone()).send().unwrap();
            assert_eq!(res.status(), StatusCode::OK);
            let response: BecomeLeaderResponse = res.json().unwrap();
            assert_eq!(
                response,
                BecomeLeaderResponse {
                    result: BecomeLeaderResult::Success
                }
            );

            let server = server_handle.join().unwrap();
            let mut client = server.connect(postgres::NoTls).unwrap();
            client.simple_query("SELECT 1").unwrap();

            // check that we're the leader and promotion doesn't do anything
            let res = Client::new().get(status_http_url).send().unwrap();
            assert_eq!(res.status(), StatusCode::OK);
            let response: LeaderStatusResponse = res.json().unwrap();
            assert_eq!(response.status, LeaderStatus::IsLeader);

            let res = Client::new().post(promote_http_url).send().unwrap();
            assert_eq!(res.status(), StatusCode::OK);
            let response: BecomeLeaderResponse = res.json().unwrap();
            assert_eq!(
                response,
                BecomeLeaderResponse {
                    result: BecomeLeaderResult::Success
                }
            );
        });
    }
}

#[mz_ore::test]
#[cfg_attr(miri, ignore)] // too slow
fn test_leader_promotion_always_using_deploy_generation() {
    let tmpdir = TempDir::new().unwrap();
    let config = util::Config::default()
        .unsafe_mode()
        .data_directory(tmpdir.path())
        .with_deploy_generation(Some(2));
    {
        // propose a deploy generation for the first time
        let server = util::start_server(config.clone()).unwrap();
        let mut client = server.connect(postgres::NoTls).unwrap();
        client.simple_query("SELECT 1").unwrap();
    }
    {
        // keep it the same, no need to promote the leader
        let listeners = Listeners::new().unwrap();
        let internal_http_addr = listeners.inner.internal_http_local_addr();
        let server = listeners.serve(config).unwrap();
        let mut client = server.connect(postgres::NoTls).unwrap();
        client.simple_query("SELECT 1").unwrap();

        // check that we're the leader and promotion doesn't do anything
        let status_http_url =
            Url::parse(&format!("http://{}/api/leader/status", internal_http_addr)).unwrap();
        let res = Client::new().get(status_http_url).send().unwrap();
        assert_eq!(res.status(), StatusCode::OK);
        let response: LeaderStatusResponse = res.json().unwrap();
        assert_eq!(response.status, LeaderStatus::IsLeader);

        let promote_http_url =
            Url::parse(&format!("http://{}/api/leader/promote", internal_http_addr)).unwrap();
        let res = Client::new().post(promote_http_url).send().unwrap();
        assert_eq!(res.status(), StatusCode::OK);
        let response: BecomeLeaderResponse = res.json().unwrap();
        assert_eq!(
            response,
            BecomeLeaderResponse {
                result: BecomeLeaderResult::Success
            }
        );
    }
}

// Test that websockets observe cancellation.
#[mz_ore::test]
#[cfg_attr(miri, ignore)] // unsupported operation: can't call foreign function `epoll_wait` on OS `linux`
fn test_cancel_ws() {
    let server = util::start_server(util::Config::default()).unwrap();
    let mut client = server.connect(postgres::NoTls).unwrap();
    client.batch_execute("CREATE TABLE t (i INT);").unwrap();

    // Start a thread to perform the cancel while the SUBSCRIBE is running in this thread.
    let handle = thread::spawn(move || {
        // Wait for the subscription to start.
        let conn_id = Retry::default()
            .retry(|_| {
                let conn_id: String = client
                    .query_one(
                        "SELECT session_id::text FROM mz_internal.mz_subscriptions",
                        &[],
                    )?
                    .get(0);
                Ok::<_, postgres::Error>(conn_id)
            })
            .unwrap();

        client
            .query_one(&format!("SELECT pg_cancel_backend({conn_id})"), &[])
            .unwrap();
    });

    let (mut ws, _resp) = tungstenite::connect(server.ws_addr()).unwrap();
    util::auth_with_ws(&mut ws, BTreeMap::default()).unwrap();
    let json = r#"{"queries":[{"query":"SUBSCRIBE t"}]}"#;
    let json: serde_json::Value = serde_json::from_str(json).unwrap();
    ws.write_message(Message::Text(json.to_string())).unwrap();

    loop {
        let msg = ws.read_message().unwrap();
        if let Ok(msg) = msg.into_text() {
            if msg.contains("query canceled") {
                break;
            }
        }
    }
    handle.join().unwrap();
}

#[mz_ore::test]
#[cfg_attr(miri, ignore)] // too slow
fn smoketest_webhook_source() {
    let server = util::start_server(util::Config::default()).unwrap();
    server.enable_feature_flags(&["enable_webhook_sources"]);

    let mut client = server.connect(postgres::NoTls).unwrap();

    #[derive(Debug, PartialEq, Eq, Deserialize, Serialize)]
    struct WebhookEvent {
        ts: u128,
        name: String,
        attrs: Vec<String>,
    }

    // Create a webhook source.
    client
        .execute(
            "CREATE SOURCE webhook_json FROM WEBHOOK BODY FORMAT JSON",
            &[],
        )
        .expect("failed to create source");

    let now = || {
        std::time::SystemTime::now()
            .duration_since(std::time::UNIX_EPOCH)
            .expect("time went backwards")
            .as_nanos()
    };

    // Generate some events.
    const NUM_EVENTS: i64 = 100;
    let events: Vec<_> = (0..NUM_EVENTS)
        .map(|i| WebhookEvent {
            ts: now(),
            name: format!("event_{i}"),
            attrs: (0..i).map(|j| format!("attr_{j}")).collect(),
        })
        .collect();

    let http_client = Client::new();
    let webhook_url = format!(
        "http://{}/api/webhook/materialize/public/webhook_json",
        server.inner.http_local_addr()
    );
    // Send all of our events to our webhook source.
    for event in &events {
        let resp = http_client
            .post(&webhook_url)
            .json(event)
            .send()
            .expect("failed to POST event");
        assert!(resp.status().is_success());
    }

    // Wait for the events to be persisted.
    mz_ore::retry::Retry::default()
        .max_tries(10)
        .retry(|_| {
            let cnt: i64 = client
                .query_one("SELECT COUNT(*) FROM webhook_json", &[])
                .expect("failed to get count")
                .get(0);

            if cnt != 100 {
                Err(anyhow::anyhow!("not all rows present"))
            } else {
                Ok(())
            }
        })
        .expect("failed to read events!");

    // Read all of our events back.
    let events_roundtrip: Vec<WebhookEvent> = client
        .query("SELECT * FROM webhook_json", &[])
        .expect("failed to query source")
        .into_iter()
        .map(|row| serde_json::from_value(row.get("body")).expect("failed to deserialize"))
        .collect();

    similar_asserts::assert_eq!(events, events_roundtrip);
}

#[mz_ore::test]
#[cfg_attr(miri, ignore)] // too slow
fn test_invalid_webhook_body() {
    let server = util::start_server(util::Config::default()).unwrap();
    server.enable_feature_flags(&["enable_webhook_sources"]);

    let mut client = server.connect(postgres::NoTls).unwrap();
    let http_client = Client::new();

    // Create a webhook source with a body format of text.
    client
        .execute(
            "CREATE SOURCE webhook_text FROM WEBHOOK BODY FORMAT TEXT",
            &[],
        )
        .expect("failed to create source");
    let webhook_url = format!(
        "http://{}/api/webhook/materialize/public/webhook_text",
        server.inner.http_local_addr()
    );

    // Send non-UTF8 text which will fail to get deserialized.
    let non_utf8 = vec![255, 255, 255, 255];
    assert!(std::str::from_utf8(&non_utf8).is_err());

    let resp = http_client
        .post(webhook_url)
        .body(non_utf8)
        .send()
        .expect("failed to POST event");
    assert_eq!(resp.status(), StatusCode::BAD_REQUEST);

    // Create a webhook source with a body format of JSON.
    client
        .execute(
            "CREATE SOURCE webhook_json FROM WEBHOOK BODY FORMAT JSON",
            &[],
        )
        .expect("failed to create source");
    let webhook_url = format!(
        "http://{}/api/webhook/materialize/public/webhook_json",
        server.inner.http_local_addr()
    );

    // Send invalid JSON which will fail to get deserialized.
    let resp = http_client
        .post(webhook_url)
        .body("a")
        .send()
        .expect("failed to POST event");
    assert_eq!(resp.status(), StatusCode::BAD_REQUEST);

    // Create a webhook source with a body format of bytes.
    client
        .execute(
            "CREATE SOURCE webhook_bytes FROM WEBHOOK BODY FORMAT BYTES",
            &[],
        )
        .expect("failed to create source");
    let webhook_url = format!(
        "http://{}/api/webhook/materialize/public/webhook_bytes",
        server.inner.http_local_addr()
    );

    // No matter what is in the body, we should always succeed.
    let mut data = [0u8; 128];
    rand::thread_rng().fill_bytes(&mut data);
    println!("Random bytes: {data:?}");
    let resp = http_client
        .post(webhook_url)
        .body(data.to_vec())
        .send()
        .expect("failed to POST event");
    assert!(resp.status().is_success());
}

#[mz_ore::test]
#[cfg_attr(miri, ignore)] // too slow
fn test_webhook_duplicate_headers() {
    let server = util::start_server(util::Config::default()).unwrap();
    server.enable_feature_flags(&["enable_webhook_sources"]);

    let mut client = server.connect(postgres::NoTls).unwrap();
    let http_client = Client::new();

    // Create a webhook source that includes headers.
    client
        .execute(
            "CREATE SOURCE webhook_text FROM WEBHOOK BODY FORMAT TEXT INCLUDE HEADERS",
            &[],
        )
        .expect("failed to create source");
    let webhook_url = format!(
        "http://{}/api/webhook/materialize/public/webhook_text",
        server.inner.http_local_addr()
    );

    // Send a request with duplicate headers.
    let resp = http_client
        .post(webhook_url)
        .body("test")
        .header("dupe", "first")
        .header("DUPE", "second")
        .header("dUpE", "third")
        .header("dUpE", "final")
        .send()
        .expect("failed to POST event");
    assert!(resp.status().is_success());

    // Wait for the events to be persisted.
    mz_ore::retry::Retry::default()
        .max_tries(10)
        .retry(|_| {
            let cnt: i64 = client
                .query_one("SELECT COUNT(*) FROM webhook_text", &[])
                .expect("failed to get count")
                .get(0);

            if cnt == 0 {
                Err(anyhow::anyhow!("no rows present"))
            } else {
                Ok(())
            }
        })
        .expect("failed to read events!");

    // Query for a row where our final header was applied.
    let body: String = client
        .query_one(
            "SELECT body FROM webhook_text WHERE headers -> 'dupe' = 'final'",
            &[],
        )
        .expect("failed to read row")
        .get("body");
    assert_eq!(body, "test");

    // Assert only one row was inserted.
    let cnt: i64 = client
        .query_one("SELECT COUNT(*) FROM webhook_text", &[])
        .expect("failed to get count")
        .get(0);
    assert_eq!(cnt, 1);
}

// Test that websockets observe cancellation and leave the transaction in an idle state.
#[mz_ore::test]
#[cfg_attr(miri, ignore)] // unsupported operation: can't call foreign function `epoll_wait` on OS `linux`
fn test_github_20262() {
    let server = util::start_server(util::Config::default()).unwrap();
    let mut client = server.connect(postgres::NoTls).unwrap();
    client.batch_execute("CREATE TABLE t (i INT);").unwrap();

    let mut cancel = || {
        // Wait for the subscription to start.
        let conn_id = Retry::default()
            .retry(|_| {
                let conn_id: String = client
                    .query_one(
                        "SELECT session_id::text FROM mz_internal.mz_subscriptions",
                        &[],
                    )?
                    .get(0);
                Ok::<_, postgres::Error>(conn_id)
            })
            .unwrap();
        client
            .query_one(&format!("SELECT pg_cancel_backend({conn_id})"), &[])
            .unwrap();
    };

    let subscribe: serde_json::Value =
        serde_json::from_str(r#"{"queries":[{"query":"SUBSCRIBE t"}]}"#).unwrap();
    let subscribe = subscribe.to_string();
    let commit: serde_json::Value =
        serde_json::from_str(r#"{"queries":[{"query":"COMMIT"}]}"#).unwrap();
    let commit = commit.to_string();
    let select: serde_json::Value =
        serde_json::from_str(r#"{"queries":[{"query":"SELECT 1"}]}"#).unwrap();
    let select = select.to_string();

    let (mut ws, _resp) = tungstenite::connect(server.ws_addr()).unwrap();
    util::auth_with_ws(&mut ws, BTreeMap::default()).unwrap();
    ws.write_message(Message::Text(subscribe)).unwrap();
    cancel();
    ws.write_message(Message::Text(commit)).unwrap();
    ws.write_message(Message::Text(select)).unwrap();

    let mut expect = VecDeque::from([
        r#"{"type":"CommandStarting","payload":{"has_rows":true,"is_streaming":true}}"#,
        r#"{"type":"Rows","payload":{"columns":[{"name":"mz_timestamp","type_oid":1700,"type_len":-1,"type_mod":2555908},{"name":"mz_diff","type_oid":20,"type_len":8,"type_mod":-1},{"name":"i","type_oid":23,"type_len":4,"type_mod":-1}]}}"#,
        r#"{"type":"Error","payload":{"message":"query canceled","code":"XX000"}}"#,
        r#"{"type":"ReadyForQuery","payload":"I"}"#,
        r#"{"type":"CommandStarting","payload":{"has_rows":false,"is_streaming":false}}"#,
        r#"{"type":"CommandComplete","payload":"COMMIT"}"#,
        r#"{"type":"Notice","payload":{"message":"there is no transaction in progress","severity":"warning"}}"#,
        r#"{"type":"ReadyForQuery","payload":"I"}"#,
        r#"{"type":"CommandStarting","payload":{"has_rows":true,"is_streaming":false}}"#,
        r#"{"type":"Rows","payload":{"columns":[{"name":"?column?","type_oid":23,"type_len":4,"type_mod":-1}]}}"#,
        r#"{"type":"Row","payload":[1]}"#,
        r#"{"type":"CommandComplete","payload":"SELECT 1"}"#,
        r#"{"type":"Notice","payload":{"message":"query was automatically run on the \"mz_introspection\" cluster","severity":"debug"}}"#,
        r#"{"type":"ReadyForQuery","payload":"I"}"#,
    ]);
    while !expect.is_empty() {
        if let Message::Text(text) = ws.read_message().unwrap() {
            let next = expect.pop_front().unwrap();
            assert_eq!(text, next);
        }
    }
}
