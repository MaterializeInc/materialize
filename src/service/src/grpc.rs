// Copyright Materialize, Inc. and contributors. All rights reserved.
//
// Use of this software is governed by the Business Source License
// included in the LICENSE file.
//
// As of the Change Date specified in that file, in accordance with
// the Business Source License, use of this software will be governed
// by the Apache License, Version 2.0.

//! gRPC transport for the [client](crate::client) module.

use std::cmp;
use std::convert::Infallible;
use std::fmt;
use std::net::ToSocketAddrs;
use std::pin::Pin;
use std::sync::Arc;
use std::time::Duration;

use async_trait::async_trait;
use futures::stream::{Stream, StreamExt};
use tokio::sync::mpsc::{self, UnboundedReceiver, UnboundedSender};
use tokio::sync::{Mutex, Notify};
use tokio::time::{self, Instant};
use tonic::body::BoxBody;
use tonic::transport::{Body, NamedService, Server};
use tonic::{Status, Streaming};
use tower::Service;
use tracing::{debug, error, info, warn};

use mz_proto::{ProtoType, RustType};

use crate::client::{FromAddr, GenericClient, Reconnect};

pub type ResponseStream<PR> = Pin<Box<dyn Stream<Item = Result<PR, Status>> + Send>>;

/// A client to a remote dataflow server using gRPC and protobuf based
/// communication.
///
/// The client opens a connection using the proto client stubs that are
/// generated by tonic from a service definition. After creation, the client is
/// in disconnected state. To connect it, `connect` has to be called. Once the
/// client is connected, it will call automatically the only RPC defined in the
/// service description, encapsulated by the `BidiProtoClient` trait. This trait
/// bound is not on the `Client` type parameter here, but it IS on the impl
/// blocks. Bidirectional protobuf RPC sets up two streams that persist after
/// the RPC has returned: A Request (Command) stream (for us, backed by a
/// unbounded mpsc queue) going from this instance to the server and a response
/// stream coming back (represented directly as a Streaming<Response> instance).
/// The recv and send functions interact with the two mpsc channels or the
/// streaming instance respectively.
#[derive(Debug)]
pub struct GrpcClient<Client, PC, PR> {
    addr: String,
    state: GrpcTcpConn<Client, PC, PR>,
    backoff: Duration,
}

/// The connection state of the GrpcClient.
#[derive(Debug)]
enum GrpcTcpConn<Client, PC, PR> {
    // Initial, disconnected state
    Disconnected,

    // We have a TCP client connection, but we have to wait for RPC answer for setting up the streams
    AwaitResponse(Client),

    // Ready to go!
    Connected((UnboundedSender<PC>, Streaming<PR>)),

    // Unable to connect, wait on next connect
    Backoff(Instant),
}

impl<Client, PC, PR> FromAddr for GrpcClient<Client, PC, PR> {
    fn from_addr(addr: String) -> Self {
        GrpcClient {
            addr: format!("http://{}", addr),
            state: GrpcTcpConn::Disconnected,
            backoff: Duration::from_millis(10),
        }
    }
}

impl<Client, PC, PR> GrpcClient<Client, PC, PR>
where
    Client: BidiProtoClient<ProtoCommand = PC, ProtoResponse = PR>,
{
    // This is and must be cancellation safe
    pub async fn connect(&mut self) -> () {
        loop {
            match &mut self.state {
                GrpcTcpConn::Disconnected => {
                    debug!("GrpcClient {}: Attempt to connect", &self.addr);
                    match Client::connect(self.addr.clone()).await {
                        Ok(client) => {
                            self.backoff = Duration::from_millis(10);
                            self.state = GrpcTcpConn::AwaitResponse(client);
                        }
                        Err(e) => {
                            self.backoff = cmp::min(self.backoff * 2, Duration::from_secs(1));
                            warn!(
                                "GrpcClient {}: Connection refused: {}. Backoff {}ms",
                                &self.addr,
                                e,
                                self.backoff.as_millis()
                            );
                            self.state = GrpcTcpConn::Backoff(Instant::now() + self.backoff);
                        }
                    }
                }
                GrpcTcpConn::AwaitResponse(client) => {
                    // The channel size is a arbitrary, but it should be a
                    // small bounded channel such that backpressure is applied early.
                    let (tx, rx) = mpsc::unbounded_channel();
                    match client.create_stream(rx).await {
                        Ok(stream) => {
                            info!("GrpcClient {}: connected", &self.addr);
                            self.state = GrpcTcpConn::Connected((tx, stream));
                        }
                        Err(err) => {
                            debug!("GrpcClient {}: Connection refused: {}", &self.addr, err);
                            self.state = GrpcTcpConn::Disconnected;
                        }
                    }
                }
                GrpcTcpConn::Connected(_) => break,
                GrpcTcpConn::Backoff(deadline) => {
                    time::sleep_until(*deadline).await;
                    self.state = GrpcTcpConn::Disconnected;
                }
            }
        }
    }
}

#[async_trait]
impl<Client, C, R, PC, PR> GenericClient<C, R> for GrpcClient<Client, PC, PR>
where
    C: RustType<PC> + Send + Sync + 'static,
    R: RustType<PR> + Send + Sync,
    Client: BidiProtoClient<ProtoCommand = PC, ProtoResponse = PR> + Send + fmt::Debug,
    PC: Send + Sync + fmt::Debug,
    PR: Send + Sync + fmt::Debug,
{
    async fn send(&mut self, cmd: C) -> Result<(), anyhow::Error> {
        let sender = if let GrpcTcpConn::Connected((sender, _)) = &self.state {
            sender
        } else {
            return Err(anyhow::anyhow!("Sent into disconnected channel"));
        };
        if sender.send(cmd.into_proto()).is_err() {
            self.state = GrpcTcpConn::Disconnected;
            Err(anyhow::anyhow!("Sent into disconnected channel"))
        } else {
            Ok(())
        }
    }

    async fn recv(&mut self) -> Result<Option<R>, anyhow::Error> {
        if let GrpcTcpConn::Connected(channels) = &mut self.state {
            match channels.1.next().await {
                Some(Ok(x)) => match x.into_rust() {
                    Ok(r) => return Ok(Some(r)),
                    Err(e) => {
                        error!(
                            "could not decode protobuf message, terminating connection: {}",
                            e
                        );
                        self.state = GrpcTcpConn::Disconnected;
                        anyhow::bail!("Connection severed");
                    }
                },
                other => {
                    match other {
                        Some(Ok(_)) => unreachable!("handled above"),
                        None => error!("connection unexpectedly terminated cleanly"),
                        Some(Err(e)) => error!("connection unexpectedly errored: {}", e),
                    }

                    self.state = GrpcTcpConn::Disconnected;
                    anyhow::bail!("Connection severed")
                }
            }
        } else {
            Err(anyhow::anyhow!("Connection severed"))
        }
    }
}

#[async_trait]
impl<Client, PC, PR> Reconnect for GrpcClient<Client, PC, PR>
where
    Client: BidiProtoClient<ProtoCommand = PC, ProtoResponse = PR> + Send + Sync,
    PC: Send,
{
    fn disconnect(&mut self) {
        debug!("GrpcClient {}: disconnect called", &self.addr);
        self.state = GrpcTcpConn::Disconnected;
    }

    async fn reconnect(&mut self) {
        debug!("GrpcClient {}: reconnect called", &self.addr);
        self.connect().await
    }
}

/// The server side gRPC implementation that will run in the service process.
///
/// There are two main tasks involved: The gRPC callback implementations will
/// execute in their own tasks, receive commands from the network and send
/// responses to the network. Upon reception of a command, the implementation
/// will put it in a mpsc queue, out of which the consumer (running in another
/// task) will read it with a recv call. The same goes for the send path: The
/// consumer calls `send` which puts the response in a mpsc queue, from which
/// the gRPC stubs will read and send it over the network.
///
/// If an error occurs, the consumer receives an error from the recv call. If no
/// client is connected recv will block until a client is available. To
/// implement the "waiting for client" the queue_change notification is used.
/// recv will check first if a client is connected using queue. If queue is
/// None, no client is connected and recv will await on the queue_change
/// notification. The server does this vice-versa. Upon connection of a client,
/// it will insert the endpoints into queue and trigger the queue_change, which
/// will wake up the waiting clients.
///
/// This is the shared datastructure between server and consumer.
///
pub struct GrpcShared<PC, PR> {
    // These are endpoints for the consumer. The other end of these queues is
    // consumed by the stream implementation. `queue` is None if no client is
    // connected, otherwise the endpoints are forwarded to the single client. If
    // a client is connecting but a client is already connected, this mutex is
    // used to block. If the consumer calls send or recv and queue is None, the
    // consumer will wait on the queue_change notification to proceed only when
    // a client has connected.
    pub queue: Mutex<Option<(Streaming<PC>, UnboundedSender<PR>)>>,

    // If queue changes, the server side will publish a notification here.
    pub queue_change: Notify,
}

/// Server side implementation.
pub struct GrpcServer<PC, PR> {
    pub shared: Arc<GrpcShared<PC, PR>>,
}

/// Consumer side functions such as send and recv. These will not directly
/// interact with the network, but put messages in a mpsc queue which will be
/// read and sent to the network in a separate server task.
pub struct GrpcServerInterface<PC, PR> {
    shared: Arc<GrpcShared<PC, PR>>,
}

impl<PC: Send + Sync, PR: Send + Sync + fmt::Debug + 'static> GrpcServerInterface<PC, PR> {
    pub async fn send<R: RustType<PR> + Send + Sync>(&self, resp: R) -> Result<(), anyhow::Error> {
        loop {
            let res = match self.shared.queue.lock().await.as_ref() {
                Some(x) => Some(x.1.send((&resp).into_proto()).map_err(Into::into)),

                // Other end absent, wait for connection, can't inline queue reset
                // here as we are still holding the lock.
                None => None,
            };

            match res {
                Some(r) => {
                    if r.is_err() {
                        *self.shared.queue.lock().await = None;
                    }
                    return r;
                }
                None => self.shared.queue_change.notified().await,
            }
        }
    }

    // Recv returns an error if a faulty connection is detected (for example when
    // the other queue endpoint has been dropped).
    // If there is no current connection, it will await a connection from the coordinator.
    //
    // This function is and must be cancellation safe.
    // If a message is received from the streaming instance and it does not cause an error,
    // the message will be delivered, as there are no await points on the path.
    // If there is an error more await point will be passed, however the hope is that in the
    // error case, the next interaction with the queue will produce another error, such that
    // the errors are not lost due to cancellation.
    //
    // There is no race between the creation of new queue pair, as the Notify will store
    // internally a permit: If a notifier comes first, and this function calls notified().await,
    // it will immediately return (and clear the permit).
    // See https://docs.rs/tokio/0.2.12/tokio/sync/struct.Notify.html
    pub async fn recv<C: RustType<PC>>(&mut self) -> Result<C, anyhow::Error> {
        loop {
            let res = match self.shared.queue.lock().await.as_mut() {
                Some(x) => {
                    let res = match x.0.next().await {
                        Some(Ok(x)) => x.into_rust().map_err(Into::into),
                        Some(Err(e)) => {
                            info!("Connection severed: {}", e);
                            Err(e.into())
                        }
                        None => {
                            info!("Connection severed: Endpoint gone");
                            Err(anyhow::anyhow!("Connection severed: Endpoint gone"))
                        }
                    };
                    Some(res)
                }

                // Other end absent, wait for connection
                None => {
                    debug!("recv called while no coordinator connected, waiting for coordinator connection.");
                    None
                }
            };

            // Don't move queue reset in block above as we are still holding the queue lock
            // there.
            match res {
                None => {
                    // No queue
                    self.shared.queue_change.notified().await;
                }
                Some(res) => {
                    if res.is_err() {
                        *self.shared.queue.lock().await = None;
                    }
                    return res;
                }
            }
        }
    }
}

/// Creates a gRPC server that can receive `PC` and sends `PR`. Returns a tuple
/// of GrpcServerInterface that can be used to recv and send from. As well as a
/// shutdown signal, which should be used to terminate the mainloop if a message
/// is sent.
///
/// The trait bounds here are intimidating, but the `f` parameter is a function
/// that turns a `GrpcServer<ProtoCommandType, ProtoResponseType>` into a
/// [`Service`] that represents a gRPC server. This is _always_ encapsulated by
/// the `ExportedFromTonicServer::new` for a specific protobuf service.
pub fn grpc_server<PC, PR, F, S>(listen_addr: String, f: F) -> GrpcServerInterface<PC, PR>
where
    PC: Send + 'static,
    PR: Send + 'static,
    S: tower::Service<
            http::request::Request<Body>,
            Response = http::response::Response<BoxBody>,
            Error = Infallible,
        > + NamedService
        + Clone
        + Send
        + 'static,
    S::Future: Send + 'static,
    F: FnOnce(GrpcServer<PC, PR>) -> S + Send + 'static,
{
    // The channel size is a arbitrary, but it should be a
    // small bounded channel such that backpressure is applied early.
    let shared = Arc::new(GrpcShared {
        queue: Mutex::new(None),
        queue_change: Notify::new(),
    });

    let server = GrpcServer {
        shared: Arc::clone(&shared),
    };

    mz_ore::task::spawn(|| S::NAME, async move {
        info!("Starting to listen on {}", listen_addr);
        Server::builder()
            .add_service(f(server))
            .serve(listen_addr.to_socket_addrs().unwrap().next().unwrap())
            .await
            .unwrap();
    });

    GrpcServerInterface { shared }
}

/// Encapsulates the core functionality of a prost client for a bidirectional
/// stream.
///
/// The implementations for this trait MUST be identical minus the types.
//
// TODO(guswynn): if prost ever presents the client API as a trait, use it
// instead of requiring an implementation of this trait.
#[async_trait]
pub trait BidiProtoClient {
    type ProtoCommand;
    type ProtoResponse;

    async fn connect(addr: String) -> Result<Self, anyhow::Error>
    where
        Self: Sized;

    async fn create_stream(
        &mut self,
        rx: UnboundedReceiver<Self::ProtoCommand>,
    ) -> Result<Streaming<Self::ProtoResponse>, anyhow::Error>;
}

/// Configuration required for [`serve`].
pub struct ServeConfig {
    /// The network address to bind.
    pub listen_addr: String,
    /// Whether or not to continue running if the client disconnects.
    pub linger: bool,
}

/// Start a gRPC server that forwards commands and responses to `client`.
///
/// The trait bounds here are intimidating, but the `f` parameter is a function
/// that turns a `GrpcServer<ProtoCommandType, ProtoResponseType>` into a
/// [`Service`] that represents a gRPC server. This is _always_ encapsulated by
/// the `ExportedFromTonicServer::new` for a specific protobuf service.
pub async fn serve<C, R, PC, PR, S, F, G>(
    config: ServeConfig,
    mut client: G,
    f: F,
) -> Result<(), anyhow::Error>
where
    PC: Send + Sync + 'static + fmt::Debug,
    PR: Send + Sync + 'static + fmt::Debug,
    C: RustType<PC> + Send + Sync + 'static,
    R: RustType<PR> + Send + Sync,
    G: GenericClient<C, R>,
    S: Service<
            http::Request<Body>,
            Response = http::Response<BoxBody>,
            Error = std::convert::Infallible,
        > + NamedService
        + Clone
        + Send
        + 'static,
    S::Future: Send + 'static,
    F: FnOnce(GrpcServer<PC, PR>) -> S + Send + 'static,
{
    let mut grpc_serve = grpc_server(config.listen_addr, f);

    loop {
        // This select implies that the .recv functions of the clients must be cancellation safe.
        loop {
            tokio::select! {
                res = grpc_serve.recv() => {
                    match res {
                        Ok(cmd) => client.send(cmd).await.unwrap(),
                        Err(err) => {
                            tracing::warn!("Lost connection: {}", err);
                            break;
                        }
                    }
                },
                res = client.recv() => {
                    match res.unwrap() {
                        None => {},
                        Some(response) => {
                            match grpc_serve.send(response).await {
                                Ok(_) => {},
                                Err(err) => {
                                    tracing::warn!("Lost connection: {}", err);
                                    break;
                                }
                            }
                        }
                    }
                },
            }
        }
        if !config.linger {
            tracing::info!("client gRPC connection gone; terminating");
            break;
        }
        tracing::info!("client gRPC connection gone; waiting for reconnect");
    }

    Ok(())
}
