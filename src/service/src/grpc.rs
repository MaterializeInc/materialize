// Copyright Materialize, Inc. and contributors. All rights reserved.
//
// Use of this software is governed by the Business Source License
// included in the LICENSE file.
//
// As of the Change Date specified in that file, in accordance with
// the Business Source License, use of this software will be governed
// by the Apache License, Version 2.0.

//! gRPC transport for the [client](crate::client) module.

use std::convert::Infallible;
use std::fmt;
use std::marker::PhantomData;
use std::net::ToSocketAddrs;
use std::pin::Pin;
use std::sync::Arc;

use anyhow::anyhow;
use async_trait::async_trait;
use futures::stream::{Stream, StreamExt, TryStreamExt};
use tokio::sync::mpsc::{self, UnboundedReceiver, UnboundedSender};
use tokio::sync::{Mutex, Notify};
use tonic::body::BoxBody;
use tonic::transport::{Body, NamedService, Server};
use tonic::{Status, Streaming};
use tower::Service;
use tracing::{debug, info};

use mz_proto::{ProtoType, RustType};

use crate::client::{GenericClient, Partitionable, Partitioned, Reconnect};

pub type ResponseStream<PR> = Pin<Box<dyn Stream<Item = Result<PR, Status>> + Send>>;

/// A client to a remote dataflow server using gRPC and protobuf based
/// communication.
///
/// The client opens a connection using the proto client stubs that are
/// generated by tonic from a service definition. After creation, the client is
/// in disconnected state. To connect it, `connect` has to be called. Once the
/// client is connected, it will call automatically the only RPC defined in the
/// service description, encapsulated by the `BidiProtoClient` trait. This trait
/// bound is not on the `Client` type parameter here, but it IS on the impl
/// blocks. Bidirectional protobuf RPC sets up two streams that persist after
/// the RPC has returned: A Request (Command) stream (for us, backed by a
/// unbounded mpsc queue) going from this instance to the server and a response
/// stream coming back (represented directly as a Streaming<Response> instance).
/// The recv and send functions interact with the two mpsc channels or the
/// streaming instance respectively.
#[derive(Debug)]
pub struct GrpcClient<Client, PC, PR> {
    addr: String,
    state: GrpcConn<PC, PR>,
    _client: PhantomData<Client>,
}

/// The connection state of a [`GrpcClient`].
#[derive(Debug)]
enum GrpcConn<PC, PR> {
    /// Disconnected state.
    Disconnected,
    /// Connected state.
    Connected {
        /// The sender for commands.
        tx: UnboundedSender<PC>,
        /// The receiver for responses.
        rx: Streaming<PR>,
    },
}

impl<Client, PC, PR> GrpcClient<Client, PC, PR> {
    pub fn new(addr: String) -> Self {
        GrpcClient {
            addr: format!("http://{}", addr),
            state: GrpcConn::Disconnected,
            _client: PhantomData,
        }
    }

    pub fn new_partitioned<C, R>(addrs: Vec<String>) -> Partitioned<Self, C, R>
    where
        (C, R): Partitionable<C, R>,
    {
        let clients = addrs.into_iter().map(Self::new).collect();
        Partitioned::new(clients)
    }
}

#[async_trait]
impl<Client, C, R, PC, PR> GenericClient<C, R> for GrpcClient<Client, PC, PR>
where
    C: RustType<PC> + Send + Sync + 'static,
    R: RustType<PR> + Send + Sync,
    Client: BidiProtoClient<ProtoCommand = PC, ProtoResponse = PR> + Send + fmt::Debug,
    PC: Send + Sync + fmt::Debug + 'static,
    PR: Send + Sync + fmt::Debug,
{
    async fn send(&mut self, cmd: C) -> Result<(), anyhow::Error> {
        if let GrpcConn::Connected { tx, .. } = &self.state {
            tx.send(cmd.into_proto())?;
            Ok(())
        } else {
            Err(anyhow!("Connection severed"))
        }
    }

    async fn recv(&mut self) -> Result<Option<R>, anyhow::Error> {
        if let GrpcConn::Connected { rx, .. } = &mut self.state {
            match rx.try_next().await? {
                None => Ok(None),
                Some(response) => Ok(Some(response.into_rust()?)),
            }
        } else {
            Err(anyhow::anyhow!("Connection severed"))
        }
    }
}

#[async_trait]
impl<Client, PC, PR> Reconnect for GrpcClient<Client, PC, PR>
where
    Client: BidiProtoClient<ProtoCommand = PC, ProtoResponse = PR> + Send + Sync,
    PC: Send,
{
    async fn reconnect(&mut self) -> Result<(), anyhow::Error> {
        debug!("GrpcClient {}: Attempt to connect", &self.addr);
        self.state = GrpcConn::Disconnected;
        let (tx, rx) = mpsc::unbounded_channel();
        let mut client = Client::connect(self.addr.clone()).await?;
        let rx = client.create_stream(rx).await?;
        self.state = GrpcConn::Connected { tx, rx };
        info!("GrpcClient {}: connected", &self.addr);
        Ok(())
    }
}

/// The server side gRPC implementation that will run in the service process.
///
/// There are two main tasks involved: The gRPC callback implementations will
/// execute in their own tasks, receive commands from the network and send
/// responses to the network. Upon reception of a command, the implementation
/// will put it in a mpsc queue, out of which the consumer (running in another
/// task) will read it with a recv call. The same goes for the send path: The
/// consumer calls `send` which puts the response in a mpsc queue, from which
/// the gRPC stubs will read and send it over the network.
///
/// If an error occurs, the consumer receives an error from the recv call. If no
/// client is connected recv will block until a client is available. To
/// implement the "waiting for client" the queue_change notification is used.
/// recv will check first if a client is connected using queue. If queue is
/// None, no client is connected and recv will await on the queue_change
/// notification. The server does this vice-versa. Upon connection of a client,
/// it will insert the endpoints into queue and trigger the queue_change, which
/// will wake up the waiting clients.
///
/// This is the shared datastructure between server and consumer.
///
pub struct GrpcShared<PC, PR> {
    // These are endpoints for the consumer. The other end of these queues is
    // consumed by the stream implementation. `queue` is None if no client is
    // connected, otherwise the endpoints are forwarded to the single client. If
    // a client is connecting but a client is already connected, this mutex is
    // used to block. If the consumer calls send or recv and queue is None, the
    // consumer will wait on the queue_change notification to proceed only when
    // a client has connected.
    pub queue: Mutex<Option<(Streaming<PC>, UnboundedSender<PR>)>>,

    // If queue changes, the server side will publish a notification here.
    pub queue_change: Notify,
}

/// Server side implementation.
pub struct GrpcServer<PC, PR> {
    pub shared: Arc<GrpcShared<PC, PR>>,
}

/// Consumer side functions such as send and recv. These will not directly
/// interact with the network, but put messages in a mpsc queue which will be
/// read and sent to the network in a separate server task.
pub struct GrpcServerInterface<PC, PR> {
    shared: Arc<GrpcShared<PC, PR>>,
}

impl<PC: Send + Sync, PR: Send + Sync + fmt::Debug + 'static> GrpcServerInterface<PC, PR> {
    pub async fn send<R: RustType<PR> + Send + Sync>(&self, resp: R) -> Result<(), anyhow::Error> {
        loop {
            let res = match self.shared.queue.lock().await.as_ref() {
                Some(x) => Some(x.1.send((&resp).into_proto()).map_err(Into::into)),

                // Other end absent, wait for connection, can't inline queue reset
                // here as we are still holding the lock.
                None => None,
            };

            match res {
                Some(r) => {
                    if r.is_err() {
                        *self.shared.queue.lock().await = None;
                    }
                    return r;
                }
                None => self.shared.queue_change.notified().await,
            }
        }
    }

    // Recv returns an error if a faulty connection is detected (for example when
    // the other queue endpoint has been dropped).
    // If there is no current connection, it will await a connection from the coordinator.
    //
    // This function is and must be cancellation safe.
    // If a message is received from the streaming instance and it does not cause an error,
    // the message will be delivered, as there are no await points on the path.
    // If there is an error more await point will be passed, however the hope is that in the
    // error case, the next interaction with the queue will produce another error, such that
    // the errors are not lost due to cancellation.
    //
    // There is no race between the creation of new queue pair, as the Notify will store
    // internally a permit: If a notifier comes first, and this function calls notified().await,
    // it will immediately return (and clear the permit).
    // See https://docs.rs/tokio/0.2.12/tokio/sync/struct.Notify.html
    pub async fn recv<C: RustType<PC>>(&mut self) -> Result<C, anyhow::Error> {
        loop {
            let res = match self.shared.queue.lock().await.as_mut() {
                Some(x) => {
                    let res = match x.0.next().await {
                        Some(Ok(x)) => x.into_rust().map_err(Into::into),
                        Some(Err(e)) => {
                            info!("Connection severed: {}", e);
                            Err(e.into())
                        }
                        None => {
                            info!("Connection severed: Endpoint gone");
                            Err(anyhow::anyhow!("Connection severed: Endpoint gone"))
                        }
                    };
                    Some(res)
                }

                // Other end absent, wait for connection
                None => {
                    debug!("recv called while no coordinator connected, waiting for coordinator connection.");
                    None
                }
            };

            // Don't move queue reset in block above as we are still holding the queue lock
            // there.
            match res {
                None => {
                    // No queue
                    self.shared.queue_change.notified().await;
                }
                Some(res) => {
                    if res.is_err() {
                        *self.shared.queue.lock().await = None;
                    }
                    return res;
                }
            }
        }
    }
}

/// Creates a gRPC server that can receive `PC` and sends `PR`. Returns a tuple
/// of GrpcServerInterface that can be used to recv and send from. As well as a
/// shutdown signal, which should be used to terminate the mainloop if a message
/// is sent.
///
/// The trait bounds here are intimidating, but the `f` parameter is a function
/// that turns a `GrpcServer<ProtoCommandType, ProtoResponseType>` into a
/// [`Service`] that represents a gRPC server. This is _always_ encapsulated by
/// the `ExportedFromTonicServer::new` for a specific protobuf service.
pub fn grpc_server<PC, PR, F, S>(listen_addr: String, f: F) -> GrpcServerInterface<PC, PR>
where
    PC: Send + 'static,
    PR: Send + 'static,
    S: tower::Service<
            http::request::Request<Body>,
            Response = http::response::Response<BoxBody>,
            Error = Infallible,
        > + NamedService
        + Clone
        + Send
        + 'static,
    S::Future: Send + 'static,
    F: FnOnce(GrpcServer<PC, PR>) -> S + Send + 'static,
{
    // The channel size is a arbitrary, but it should be a
    // small bounded channel such that backpressure is applied early.
    let shared = Arc::new(GrpcShared {
        queue: Mutex::new(None),
        queue_change: Notify::new(),
    });

    let server = GrpcServer {
        shared: Arc::clone(&shared),
    };

    mz_ore::task::spawn(|| S::NAME, async move {
        info!("Starting to listen on {}", listen_addr);
        Server::builder()
            .add_service(f(server))
            .serve(listen_addr.to_socket_addrs().unwrap().next().unwrap())
            .await
            .unwrap();
    });

    GrpcServerInterface { shared }
}

/// Encapsulates the core functionality of a prost client for a bidirectional
/// stream.
///
/// The implementations for this trait MUST be identical minus the types.
//
// TODO(guswynn): if prost ever presents the client API as a trait, use it
// instead of requiring an implementation of this trait.
#[async_trait]
pub trait BidiProtoClient {
    type ProtoCommand;
    type ProtoResponse;

    async fn connect(addr: String) -> Result<Self, anyhow::Error>
    where
        Self: Sized;

    async fn create_stream(
        &mut self,
        rx: UnboundedReceiver<Self::ProtoCommand>,
    ) -> Result<Streaming<Self::ProtoResponse>, anyhow::Error>;
}

/// Configuration required for [`serve`].
pub struct ServeConfig {
    /// The network address to bind.
    pub listen_addr: String,
    /// Whether or not to continue running if the client disconnects.
    pub linger: bool,
}

/// Start a gRPC server that forwards commands and responses to `client`.
///
/// The trait bounds here are intimidating, but the `f` parameter is a function
/// that turns a `GrpcServer<ProtoCommandType, ProtoResponseType>` into a
/// [`Service`] that represents a gRPC server. This is _always_ encapsulated by
/// the `ExportedFromTonicServer::new` for a specific protobuf service.
pub async fn serve<C, R, PC, PR, S, F, G>(
    config: ServeConfig,
    mut client: G,
    f: F,
) -> Result<(), anyhow::Error>
where
    PC: Send + Sync + 'static + fmt::Debug,
    PR: Send + Sync + 'static + fmt::Debug,
    C: RustType<PC> + Send + Sync + 'static,
    R: RustType<PR> + Send + Sync,
    G: GenericClient<C, R>,
    S: Service<
            http::Request<Body>,
            Response = http::Response<BoxBody>,
            Error = std::convert::Infallible,
        > + NamedService
        + Clone
        + Send
        + 'static,
    S::Future: Send + 'static,
    F: FnOnce(GrpcServer<PC, PR>) -> S + Send + 'static,
{
    let mut grpc_serve = grpc_server(config.listen_addr, f);

    loop {
        // This select implies that the .recv functions of the clients must be cancellation safe.
        loop {
            tokio::select! {
                res = grpc_serve.recv() => {
                    match res {
                        Ok(cmd) => client.send(cmd).await.unwrap(),
                        Err(err) => {
                            tracing::warn!("Lost connection: {}", err);
                            break;
                        }
                    }
                },
                res = client.recv() => {
                    match res.unwrap() {
                        None => {},
                        Some(response) => {
                            match grpc_serve.send(response).await {
                                Ok(_) => {},
                                Err(err) => {
                                    tracing::warn!("Lost connection: {}", err);
                                    break;
                                }
                            }
                        }
                    }
                },
            }
        }
        if !config.linger {
            tracing::info!("client gRPC connection gone; terminating");
            break;
        }
        tracing::info!("client gRPC connection gone; waiting for reconnect");
    }

    Ok(())
}
