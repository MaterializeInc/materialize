// Copyright Materialize, Inc. and contributors. All rights reserved.
//
// Use of this software is governed by the Business Source License
// included in the LICENSE file.
//
// As of the Change Date specified in that file, in accordance with
// the Business Source License, use of this software will be governed
// by the Apache License, Version 2.0.

// BEGIN LINT CONFIG
// DO NOT EDIT. Automatically generated by bin/gen-lints.
// Have complaints about the noise? See the note in misc/python/materialize/cli/gen-lints.py first.
#![allow(clippy::style)]
#![allow(clippy::complexity)]
#![allow(clippy::large_enum_variant)]
#![allow(clippy::mutable_key_type)]
#![allow(clippy::stable_sort_primitive)]
#![allow(clippy::map_entry)]
#![allow(clippy::box_default)]
#![warn(clippy::bool_comparison)]
#![warn(clippy::clone_on_ref_ptr)]
#![warn(clippy::no_effect)]
#![warn(clippy::unnecessary_unwrap)]
#![warn(clippy::dbg_macro)]
#![warn(clippy::todo)]
#![warn(clippy::wildcard_dependencies)]
#![warn(clippy::zero_prefixed_literal)]
#![warn(clippy::borrowed_box)]
#![warn(clippy::deref_addrof)]
#![warn(clippy::double_must_use)]
#![warn(clippy::double_parens)]
#![warn(clippy::extra_unused_lifetimes)]
#![warn(clippy::needless_borrow)]
#![warn(clippy::needless_question_mark)]
#![warn(clippy::needless_return)]
#![warn(clippy::redundant_pattern)]
#![warn(clippy::redundant_slicing)]
#![warn(clippy::redundant_static_lifetimes)]
#![warn(clippy::single_component_path_imports)]
#![warn(clippy::unnecessary_cast)]
#![warn(clippy::useless_asref)]
#![warn(clippy::useless_conversion)]
#![warn(clippy::builtin_type_shadow)]
#![warn(clippy::duplicate_underscore_argument)]
#![warn(clippy::double_neg)]
#![warn(clippy::unnecessary_mut_passed)]
#![warn(clippy::wildcard_in_or_patterns)]
#![warn(clippy::collapsible_if)]
#![warn(clippy::collapsible_else_if)]
#![warn(clippy::crosspointer_transmute)]
#![warn(clippy::excessive_precision)]
#![warn(clippy::overflow_check_conditional)]
#![warn(clippy::as_conversions)]
#![warn(clippy::match_overlapping_arm)]
#![warn(clippy::zero_divided_by_zero)]
#![warn(clippy::must_use_unit)]
#![warn(clippy::suspicious_assignment_formatting)]
#![warn(clippy::suspicious_else_formatting)]
#![warn(clippy::suspicious_unary_op_formatting)]
#![warn(clippy::mut_mutex_lock)]
#![warn(clippy::print_literal)]
#![warn(clippy::same_item_push)]
#![warn(clippy::useless_format)]
#![warn(clippy::write_literal)]
#![warn(clippy::redundant_closure)]
#![warn(clippy::redundant_closure_call)]
#![warn(clippy::unnecessary_lazy_evaluations)]
#![warn(clippy::partialeq_ne_impl)]
#![warn(clippy::redundant_field_names)]
#![warn(clippy::transmutes_expressible_as_ptr_casts)]
#![warn(clippy::unused_async)]
#![warn(clippy::disallowed_methods)]
#![warn(clippy::disallowed_macros)]
#![warn(clippy::disallowed_types)]
#![warn(clippy::from_over_into)]
// END LINT CONFIG

use std::{
    collections::{BTreeMap, BTreeSet},
    convert::Infallible,
    time::Duration,
};

use futures::{Future, StreamExt};
use postgres_openssl::MakeTlsConnector;
use serde::{Deserialize, Serialize};
use serde_json::json;
use timely::progress::Antichain;
use tokio::sync::oneshot;
use tokio_postgres::Config;

use mz_ore::{
    assert_contains, collections::CollectionExt, metrics::MetricsRegistry, retry::Retry,
    task::spawn,
};

use mz_stash::{
    Stash, StashCollection, StashError, StashFactory, TableTransaction, Timestamp, TypedCollection,
};

pub static C1: TypedCollection<i64, i64> = TypedCollection::new("c1");
pub static C2: TypedCollection<i64, i64> = TypedCollection::new("c2");
pub static C_SAVEPOINT: TypedCollection<i64, i64> = TypedCollection::new("c_savepoint");

#[tokio::test]
#[cfg_attr(miri, ignore)] // unsupported operation: can't call foreign function `TLS_client_method` on OS `linux`
async fn test_stash_postgres() {
    mz_ore::test::init_logging();

    let tls = mz_postgres_util::make_tls(&Config::new()).unwrap();
    let factory = StashFactory::new(&MetricsRegistry::new());

    {
        // Verify invalid URLs fail on connect.
        assert!(factory
            .open("host=invalid".into(), None, tls.clone(),)
            .await
            .unwrap_err()
            .to_string()
            .contains("stash error: postgres: error connecting to server"));
    }

    let connstr = std::env::var("COCKROACH_URL").expect("COCKROACH_URL must be set");
    async fn connect(
        factory: &StashFactory,
        connstr: &str,
        tls: MakeTlsConnector,
        clear: bool,
    ) -> Stash {
        if clear {
            Stash::clear(connstr, tls.clone()).await.unwrap();
        }
        factory.open(connstr.to_string(), None, tls).await.unwrap()
    }
    {
        connect(&factory, &connstr, tls.clone(), true).await;
        let stash =
            test_stash(|| async { connect(&factory, &connstr, tls.clone(), false).await }).await;
        stash.verify().await.unwrap();
    }
    {
        connect(&factory, &connstr, tls.clone(), true).await;
        let stash =
            test_append(|| async { connect(&factory, &connstr, tls.clone(), false).await }).await;
        stash.verify().await.unwrap();
    }
    // Test the fence.
    {
        let mut conn1 = connect(&factory, &connstr, tls.clone(), true).await;
        // Don't clear the stash tables.
        let mut conn2 = connect(&factory, &connstr, tls.clone(), false).await;
        assert!(match conn1.collection::<String, String>("c").await {
            Err(e) => e.is_unrecoverable(),
            _ => panic!("expected error"),
        });
        let _: StashCollection<String, String> = conn2.collection("c").await.unwrap();
    }
    // Test failures after commit.
    {
        let mut stash = connect(&factory, &connstr, tls.clone(), true).await;
        let col = stash.collection::<i64, i64>("c1").await.unwrap();
        let mut batch = col.make_batch(&mut stash).await.unwrap();
        col.append_to_batch(&mut batch, &1, &2, 1);
        stash.append(vec![batch]).await.unwrap();
        assert_eq!(
            C1.peek_one(&mut stash).await.unwrap(),
            BTreeMap::from([(1, 2)])
        );
        let mut batch = col.make_batch(&mut stash).await.unwrap();
        col.append_to_batch(&mut batch, &1, &2, -1);

        fail::cfg("stash_commit_pre", "return(commit failpoint)").unwrap();
        fail::cfg("stash_commit_post", "return(commit failpoint)").unwrap();
        // Because the commit error will either retry or discover it succeeded,
        // it never returns an error. Thus, we need to re-enable the failpoint
        // in another thread. Use both a pre and post commit error to test both
        // commit success and fail paths. Use a channel to check that we haven't
        // succeeded unexpectedly.
        let (tx, mut rx) = oneshot::channel();
        let handle = spawn(|| "stash_commit_enable", async {
            tokio::time::sleep(Duration::from_millis(100)).await;
            // Assert no success yet.
            rx.try_recv().unwrap_err();
            fail::cfg("stash_commit_post", "off").unwrap();
            tokio::time::sleep(Duration::from_millis(100)).await;
            // Assert no success yet.
            rx.try_recv().unwrap_err();
            fail::cfg("stash_commit_pre", "off").unwrap();
            rx.await.unwrap();
        });
        stash.append(vec![batch.clone()]).await.unwrap();
        assert_eq!(C1.peek_one(&mut stash).await.unwrap(), BTreeMap::new());
        tx.send(()).unwrap();
        handle.await.unwrap();
    }
    // Test collection_fix_unconsolidated_rows.
    {
        let mut stash = connect(&factory, &connstr, tls.clone(), true).await;
        #[derive(Debug, Serialize, Deserialize, PartialOrd, PartialEq, Ord, Eq, Hash)]
        struct S1 {
            a: i64,
        }
        #[derive(Debug, Serialize, Deserialize, PartialOrd, PartialEq, Ord, Eq, Hash)]
        struct S2 {
            a: i64,
            b: Option<i64>,
        }
        // Use the same collection name!
        static CS1: TypedCollection<S1, i64> = TypedCollection::new("c");
        static CS2: TypedCollection<S2, i64> = TypedCollection::new("c");
        let (col1, mut batch) = CS1.make_batch(&mut stash).await.unwrap();
        col1.append_to_batch(&mut batch, &S1 { a: 1 }, &2, 1);
        stash.append(vec![batch]).await.unwrap();
        assert_eq!(
            CS1.peek_one(&mut stash).await.unwrap(),
            BTreeMap::from([(S1 { a: 1 }, 2)])
        );

        // Remove the row using the new struct.
        stash
            .with_transaction(move |tx| {
                Box::pin(async move {
                    let col = CS2.from_tx(&tx).await?;
                    let raw_rows = tx.peek_raw(col.id).await?.collect::<Vec<_>>();
                    // Returned JSON values should not contain column b.
                    assert_eq!(
                        raw_rows,
                        vec![(
                            (
                                json!({
                                    "a": 1,
                                }),
                                json!(2),
                            ),
                            1
                        )]
                    );
                    let col_rows = tx.peek_one(col).await?;
                    let mut row = col_rows.into_element();
                    // None is in the new struct for field b.
                    assert_eq!(row, (S2 { a: 1, b: None }, 2));

                    // Attempt to remove the old row and add the current one, simulating a catalog
                    // migration.
                    let mut batch = col.make_batch_tx(&tx).await?;
                    col.append_to_batch(&mut batch, &row.0, &row.1, -1);
                    row.0.b = Some(3);
                    col.append_to_batch(&mut batch, &row.0, &row.1, 1);
                    tx.append(vec![batch]).await?;

                    // Refetch the raw rows, asserting that the unmatched retraction is present.
                    let raw_rows = tx.peek_raw(col.id).await?.collect::<Vec<_>>();
                    assert_eq!(
                        raw_rows,
                        vec![
                            // The original row.
                            (
                                (
                                    json!({
                                        "a": 1,
                                    }),
                                    json!(2),
                                ),
                                1
                            ),
                            // The unmatched retraction.
                            (
                                (
                                    json!({
                                        "a": 1,
                                        "b": null,
                                    }),
                                    json!(2),
                                ),
                                -1
                            ),
                            // The new row.
                            (
                                (
                                    json!({
                                        "a": 1,
                                        "b": 3,
                                    }),
                                    json!(2),
                                ),
                                1
                            ),
                        ]
                    );

                    // But peek_one only sees a single row due to Rust consolidation.
                    assert_eq!(
                        tx.peek_one(col).await.unwrap(),
                        BTreeMap::from([(S2 { a: 1, b: Some(3) }, 2)])
                    );

                    // Attempt to fix.
                    tx.collection_fix_unconsolidated_rows(col).await?;
                    Ok(())
                })
            })
            .await
            .unwrap();

        // Commit the transaction and wait for background consolidation to occur, so
        // retry in a loop until peek_raw only sees a single row.
        let stream = Retry::default().into_retry_stream();
        tokio::pin!(stream);
        let mut ok = false;
        while stream.next().await.is_some() {
            let (raw_rows, rows) = stash
                .with_transaction(move |tx| {
                    Box::pin(async move {
                        let col = CS2.from_tx(&tx).await?;
                        let raw_rows = tx.peek_raw(col.id).await?.collect::<Vec<_>>();
                        let rows = tx.peek_one(col).await.unwrap();
                        Ok((raw_rows, rows))
                    })
                })
                .await
                .unwrap();

            // There should be exactly one raw row.
            if raw_rows.len() != 1 {
                continue;
            }
            let expect = BTreeMap::from([(S2 { a: 1, b: Some(3) }, 2)]);
            if rows != expect {
                continue;
            }
            ok = true;
            break;
        }
        assert!(ok);
    }
    // Test readonly.
    {
        Stash::clear(&connstr, tls.clone()).await.unwrap();
        let mut stash_rw = factory
            .open(connstr.to_string(), None, tls.clone())
            .await
            .unwrap();
        let col_rw = stash_rw.collection::<i64, i64>("c1").await.unwrap();
        let mut batch = col_rw.make_batch(&mut stash_rw).await.unwrap();
        col_rw.append_to_batch(&mut batch, &1, &2, 1);
        stash_rw.append(vec![batch]).await.unwrap();

        // Now make a readonly stash. We should fail to create new collections,
        // but be able to read existing collections.
        let mut stash_ro = factory
            .open_readonly(connstr.to_string(), None, tls.clone())
            .await
            .unwrap();
        let res = stash_ro.collection::<i64, i64>("c2").await;
        assert_contains!(
            res.unwrap_err().to_string(),
            "cannot execute INSERT in a read-only transaction"
        );
        assert_eq!(
            C1.peek_one(&mut stash_ro).await.unwrap(),
            BTreeMap::from([(1, 2)])
        );

        // The previous stash should still be the leader.
        assert!(stash_rw.confirm_leadership().await.is_ok());
        stash_rw.verify().await.unwrap();
    }
    // Test savepoint.
    {
        let mut stash_rw = factory
            .open(connstr.to_string(), None, tls.clone())
            .await
            .unwrap();
        // Data still present from previous test.

        // Now make a savepoint stash. We should be allowed to create anything
        // we want, but it shouldn't be viewable to other stashes.
        let mut stash_sp = factory
            .open_savepoint(connstr.to_string(), tls)
            .await
            .unwrap();
        let c1_sp = stash_rw.collection::<i64, i64>("c1").await.unwrap();
        let mut batch = c1_sp.make_batch(&mut stash_sp).await.unwrap();
        c1_sp.append_to_batch(&mut batch, &5, &6, 1);
        stash_sp.append(vec![batch]).await.unwrap();
        assert_eq!(
            C1.peek_one(&mut stash_sp).await.unwrap(),
            BTreeMap::from([(1, 2), (5, 6)]),
        );
        // RW collection can't see the new row.
        assert_eq!(
            C1.peek_one(&mut stash_rw).await.unwrap(),
            BTreeMap::from([(1, 2)])
        );

        // SP stash can create a new collection, append to it, peek it.
        let c_savepoint = stash_sp
            .collection::<i64, i64>("c_savepoint")
            .await
            .unwrap();
        let mut batch = c_savepoint.make_batch(&mut stash_sp).await.unwrap();
        c_savepoint.append_to_batch(&mut batch, &3, &4, 1);
        stash_sp.append(vec![batch]).await.unwrap();
        assert_eq!(
            C_SAVEPOINT.peek_one(&mut stash_sp).await.unwrap(),
            BTreeMap::from([(3, 4)])
        );
        // But the RW collection can't see it.
        assert_eq!(
            BTreeSet::from_iter(stash_rw.collections().await.unwrap().into_values()),
            BTreeSet::from(["c1".to_string()])
        );

        drop(stash_sp);

        // The previous stash should still be the leader.
        assert!(stash_rw.confirm_leadership().await.is_ok());
        // Verify c1 didn't change.
        assert_eq!(
            C1.peek_one(&mut stash_rw).await.unwrap(),
            BTreeMap::from([(1, 2)])
        );
        stash_rw.verify().await.unwrap();
    }
}

async fn test_append<F, O>(f: F) -> Stash
where
    O: Future<Output = Stash>,
    F: Fn() -> O,
{
    const TYPED: TypedCollection<String, String> = TypedCollection::new("typed");

    let mut stash = f().await;

    // Can't peek if since == upper.
    assert!(TYPED
        .peek_one(&mut stash)
        .await
        .unwrap_err()
        .to_string()
        .contains("since {-9223372036854775808} is not less than upper {-9223372036854775808}"));
    TYPED
        .upsert_key(&mut stash, "k1".to_string(), |_| {
            Ok::<_, Infallible>("v1".to_string())
        })
        .await
        .unwrap()
        .unwrap();
    assert_eq!(
        TYPED.peek_one(&mut stash).await.unwrap(),
        BTreeMap::from([("k1".to_string(), "v1".to_string())])
    );
    TYPED
        .upsert_key(&mut stash, "k1".to_string(), |_| {
            Ok::<_, Infallible>("v2".to_string())
        })
        .await
        .unwrap()
        .unwrap();
    assert_eq!(
        TYPED.peek_one(&mut stash).await.unwrap(),
        BTreeMap::from([("k1".to_string(), "v2".to_string())])
    );
    assert_eq!(
        TYPED
            .peek_key_one(&mut stash, "k1".to_string())
            .await
            .unwrap(),
        Some("v2".to_string()),
    );
    assert_eq!(
        TYPED
            .peek_key_one(&mut stash, "k2".to_string())
            .await
            .unwrap(),
        None
    );
    TYPED
        .upsert(
            &mut stash,
            vec![
                ("k1".to_string(), "v3".to_string()),
                ("k2".to_string(), "v4".to_string()),
            ],
        )
        .await
        .unwrap();
    assert_eq!(
        TYPED.peek_one(&mut stash).await.unwrap(),
        BTreeMap::from([
            ("k1".to_string(), "v3".to_string()),
            ("k2".to_string(), "v4".to_string())
        ])
    );

    // Test append across collections.
    let orders = stash.collection::<String, String>("orders").await.unwrap();
    let other = stash.collection::<String, String>("other").await.unwrap();

    stash
        .with_transaction(move |tx| {
            Box::pin(async move {
                // Seal so we can invalidate the upper below.
                tx.seal(other.id, Antichain::from_elem(1), None)
                    .await
                    .unwrap();
                let mut orders_batch = orders.make_batch_tx(&tx).await.unwrap();
                orders.append_to_batch(&mut orders_batch, &"k1".to_string(), &"v1".to_string(), 1);
                let mut other_batch = other.make_batch_tx(&tx).await.unwrap();
                other.append_to_batch(&mut other_batch, &"k2".to_string(), &"v2".to_string(), 1);

                // Invalidate one upper and ensure append doesn't commit partial batches.
                let other_upper = other_batch.upper;
                other_batch.upper = Antichain::from_elem(Timestamp::MIN);
                assert_contains!(
                    tx.append(vec![orders_batch.clone(), other_batch.clone()])
                        .await
                        .unwrap_err()
                        .to_string(),
                    "{-9223372036854775808}",
                );
                // Test batches in the other direction too.
                assert_contains!(
                    tx.append(vec![other_batch.clone(), orders_batch.clone()])
                        .await
                        .unwrap_err()
                        .to_string(),
                    "{-9223372036854775808}",
                );

                // Fix the upper, append should work now.
                other_batch.upper = other_upper;
                tx.append(vec![other_batch, orders_batch]).await.unwrap();
                assert_eq!(
                    tx.iter(orders).await.unwrap(),
                    &[(("k1".into(), "v1".into()), -9223372036854775808, 1),]
                );
                assert_eq!(
                    tx.iter(other).await.unwrap(),
                    &[(("k2".into(), "v2".into()), 1, 1),]
                );
                assert_eq!(
                    tx.peek_one(orders).await.unwrap(),
                    BTreeMap::from([("k1".to_string(), "v1".to_string())])
                );
                assert_eq!(
                    tx.peek_one(other).await.unwrap(),
                    BTreeMap::from([("k2".to_string(), "v2".to_string())])
                );

                // Verify the upper got bumped.
                assert_eq!(
                    tx.since(orders.id).await.unwrap().into_option().unwrap(),
                    tx.upper(orders.id).await.unwrap().into_option().unwrap() - 1
                );
                // Multiple empty batches should bump the upper and the since because append
                // must also compact and consolidate.
                for _ in 0..5 {
                    let orders_batch = orders.make_batch_tx(&tx).await.unwrap();
                    tx.append(vec![orders_batch]).await.unwrap();
                    assert_eq!(
                        tx.since(orders.id).await.unwrap().into_option().unwrap(),
                        tx.upper(orders.id).await.unwrap().into_option().unwrap() - 1
                    );
                }
                Ok(())
            })
        })
        .await
        .unwrap();

    // Remake the stash and ensure data remains.
    let mut stash = f().await;
    stash
        .with_transaction(move |tx| {
            Box::pin(async move {
                assert_eq!(
                    tx.peek_one(orders).await.unwrap(),
                    BTreeMap::from([("k1".to_string(), "v1".to_string())])
                );
                assert_eq!(
                    tx.peek_one(other).await.unwrap(),
                    BTreeMap::from([("k2".to_string(), "v2".to_string())])
                );
                Ok(())
            })
        })
        .await
        .unwrap();

    // Remake again, mutate before reading, then read.
    let mut stash = f().await;
    stash
        .with_transaction(move |tx| {
            Box::pin(async move {
                tx.update_savepoint(orders.id, &[(("k3".into(), "v3".into()), 1, 1)], None)
                    .await
                    .unwrap();
                tx.seal(orders.id, Antichain::from_elem(2), None)
                    .await
                    .unwrap();

                assert_eq!(
                    tx.peek_one(orders).await.unwrap(),
                    BTreeMap::from([
                        ("k1".to_string(), "v1".to_string()),
                        ("k3".to_string(), "v3".to_string())
                    ])
                );
                Ok(())
            })
        })
        .await
        .unwrap();

    // Remake the stash, mutate, then read.
    let mut stash = f().await;
    stash
        .with_transaction(move |tx| {
            Box::pin(async move {
                let mut orders_batch = orders.make_batch_tx(&tx).await.unwrap();
                orders.append_to_batch(&mut orders_batch, &"k4".to_string(), &"v4".to_string(), 1);
                tx.append(vec![orders_batch]).await.unwrap();
                assert_eq!(
                    tx.peek_one(orders).await.unwrap(),
                    BTreeMap::from([
                        ("k1".to_string(), "v1".to_string()),
                        ("k3".to_string(), "v3".to_string()),
                        ("k4".to_string(), "v4".to_string())
                    ])
                );
                Ok(())
            })
        })
        .await
        .unwrap();

    // Remake and read again.
    let mut stash = f().await;
    stash
        .with_transaction(move |tx| {
            Box::pin(async move {
                assert_eq!(
                    tx.peek_one(orders).await.unwrap(),
                    BTreeMap::from([
                        ("k1".to_string(), "v1".to_string()),
                        ("k3".to_string(), "v3".to_string()),
                        ("k4".to_string(), "v4".to_string())
                    ])
                );
                Ok(())
            })
        })
        .await
        .unwrap();

    test_stash_table(&mut stash).await;

    stash
}

async fn test_stash<F, O>(f: F) -> Stash
where
    O: Future<Output = Stash>,
    F: Fn() -> O,
{
    let mut stash = f().await;
    stash
        .with_transaction(move |tx| {
            Box::pin(async move {
                // Create an arrangement, write some data into it, then read it back.
                let orders = tx.collection::<String, String>("orders").await.unwrap();
                tx.update_savepoint(orders.id, &[(("widgets".into(), "1".into()), 1, 1)], None)
                    .await
                    .unwrap();
                tx.update_savepoint(orders.id, &[(("wombats".into(), "2".into()), 1, 2)], None)
                    .await
                    .unwrap();
                // Move this before iter to better test the memory tx's iter_key.
                assert_eq!(
                    tx.iter_key(orders, &"widgets".to_string()).await.unwrap(),
                    &[("1".into(), 1, 1)]
                );
                assert_eq!(
                    tx.iter(orders).await.unwrap(),
                    &[
                        (("widgets".into(), "1".into()), 1, 1),
                        (("wombats".into(), "2".into()), 1, 2),
                    ]
                );
                assert_eq!(
                    tx.iter_key(orders, &"wombats".to_string()).await.unwrap(),
                    &[("2".into(), 1, 2)]
                );

                // Write to another arrangement and ensure the data stays separate.
                let other = tx.collection::<String, String>("other").await.unwrap();
                tx.update_savepoint(other.id, &[(("foo".into(), "bar".into()), 1, 1)], None)
                    .await
                    .unwrap();
                assert_eq!(
                    tx.iter(other).await.unwrap(),
                    &[(("foo".into(), "bar".into()), 1, 1)],
                );
                assert_eq!(
                    tx.iter(orders).await.unwrap(),
                    &[
                        (("widgets".into(), "1".into()), 1, 1),
                        (("wombats".into(), "2".into()), 1, 2),
                    ]
                );

                // Check that consolidation happens immediately...
                tx.update_savepoint(orders.id, &[(("wombats".into(), "2".into()), 1, -1)], None)
                    .await
                    .unwrap();
                assert_eq!(
                    tx.iter(orders).await.unwrap(),
                    &[
                        (("widgets".into(), "1".into()), 1, 1),
                        (("wombats".into(), "2".into()), 1, 1),
                    ]
                );

                // ...even when it results in a entry's removal.
                tx.update_savepoint(orders.id, &[(("wombats".into(), "2".into()), 1, -1)], None)
                    .await
                    .unwrap();
                assert_eq!(
                    tx.iter(orders).await.unwrap(),
                    &[(("widgets".into(), "1".into()), 1, 1),]
                );

                // Check that logical compaction applies immediately.
                tx.update_savepoint(
                    orders.id,
                    &[
                        (("widgets".into(), "1".into()), 2, 1),
                        (("widgets".into(), "1".into()), 3, 1),
                        (("widgets".into(), "1".into()), 4, 1),
                    ],
                    None,
                )
                .await
                .unwrap();
                tx.seal(orders.id, Antichain::from_elem(3), None)
                    .await
                    .unwrap();
                // Peek should not observe widgets from timestamps 3 or 4.
                assert_eq!(tx.peek_timestamp(orders).await.unwrap(), 2);
                assert_eq!(
                    tx.peek(orders).await.unwrap(),
                    vec![("widgets".into(), "1".into(), 2)]
                );
                assert_eq!(
                    tx.peek_one(orders).await.unwrap_err().to_string(),
                    "stash error: unexpected peek multiplicity"
                );
                tx.compact(orders.id, &Antichain::from_elem(3), None)
                    .await
                    .unwrap();
                assert_eq!(
                    tx.iter(orders).await.unwrap(),
                    &[
                        (("widgets".into(), "1".into()), 3, 3),
                        (("widgets".into(), "1".into()), 4, 1),
                    ]
                );

                // Check that physical compaction does not change the collection's contents.
                tx.consolidate(orders.id).await.unwrap();
                assert_eq!(
                    tx.iter(orders).await.unwrap(),
                    &[
                        (("widgets".into(), "1".into()), 3, 3),
                        (("widgets".into(), "1".into()), 4, 1),
                    ]
                );

                // Test invalid seals, compactions, and updates.
                assert_eq!(
                    tx.seal(orders.id, Antichain::from_elem(2), None)
                        .await
                        .unwrap_err()
                        .to_string(),
                    "stash error: seal request {2} is less than the current upper frontier {3}",
                );
                assert_eq!(
                    tx.compact(orders.id, &Antichain::from_elem(2), None)
                        .await
                        .unwrap_err()
                        .to_string(),
                    "stash error: compact request {2} is less than the current since frontier {3}",
                );
                assert_eq!(
                    tx.compact(orders.id, &Antichain::from_elem(4), None)
                        .await
                        .unwrap_err()
                        .to_string(),
                    "stash error: compact request {4} is greater than the current upper frontier {3}",
                );
                assert_eq!(
                    tx.update_savepoint(orders.id, &[(("wodgets".into(), "1".into()), 2, 1)], None)
                        .await
                        .unwrap_err()
                        .to_string(),
                    "stash error: entry time 2 is less than the current upper frontier {3}",
                );

                // Test advancing since and upper to the empty frontier.
                tx.seal(orders.id, Antichain::new(), None).await.unwrap();
                tx.compact(orders.id, &Antichain::new(), None)
                    .await
                    .unwrap();
                assert_eq!(
                    match tx.iter(orders).await {
                        Ok(_) => panic!("call to iter unexpectedly succeeded"),
                        Err(e) => e.to_string(),
                    },
                    "stash error: cannot iterate collection with empty since frontier",
                );
                assert_eq!(
                    match tx.iter_key(orders, &"wombats".to_string()).await {
                        Ok(_) => panic!("call to iter_key unexpectedly succeeded"),
                        Err(e) => e.to_string(),
                    },
                    "stash error: cannot iterate collection with empty since frontier",
                );
                tx.consolidate(orders.id).await.unwrap();

                // Double check that the other collection is still untouched.
                assert_eq!(
                    tx.iter(other).await.unwrap(),
                    &[(("foo".into(), "bar".into()), 1, 1)],
                );
                assert_eq!(
                    tx.since(other.id).await.unwrap(),
                    Antichain::from_elem(Timestamp::MIN)
                );
                assert_eq!(
                    tx.upper(other.id).await.unwrap(),
                    Antichain::from_elem(Timestamp::MIN)
                );

                // Test peek_one.
                tx.seal(other.id, Antichain::from_elem(2), None)
                    .await
                    .unwrap();
                assert_eq!(
                    tx.peek_one(other).await.unwrap(),
                    BTreeMap::from([("foo".to_string(), "bar".to_string())])
                );
                assert_eq!(
                    tx.peek_key_one(other, &"foo".to_string()).await.unwrap(),
                    Some("bar".to_string())
                );
                Ok(())
            })
        })
        .await
        .unwrap();

    stash
}

async fn test_stash_table(stash: &mut Stash) {
    const TABLE: TypedCollection<Vec<u8>, String> = TypedCollection::new("table");
    fn uniqueness_violation(a: &String, b: &String) -> bool {
        a == b
    }
    let collection = TABLE.get(stash).await.unwrap();

    async fn commit(
        stash: &mut Stash,
        collection: StashCollection<Vec<u8>, String>,
        pending: Vec<(Vec<u8>, String, i64)>,
    ) -> Result<(), StashError> {
        let mut batch = collection.make_batch(stash).await.unwrap();
        for (k, v, diff) in pending {
            collection.append_to_batch(&mut batch, &k, &v, diff);
        }
        stash.append(vec![batch]).await.unwrap();
        Ok(())
    }

    TABLE
        .upsert_key(stash, 1i64.to_le_bytes().to_vec(), |_| {
            Ok::<_, Infallible>("v1".to_string())
        })
        .await
        .unwrap()
        .unwrap();
    TABLE
        .upsert(stash, vec![(2i64.to_le_bytes().to_vec(), "v2".to_string())])
        .await
        .unwrap();
    let mut table =
        TableTransaction::new(TABLE.peek_one(stash).await.unwrap(), uniqueness_violation);
    assert_eq!(
        table.items(),
        BTreeMap::from([
            (1i64.to_le_bytes().to_vec(), "v1".to_string()),
            (2i64.to_le_bytes().to_vec(), "v2".to_string())
        ])
    );
    assert_eq!(table.delete(|_k, _v| false).len(), 0);
    assert_eq!(table.delete(|_k, v| v == "v2").len(), 1);
    assert_eq!(
        table.items(),
        BTreeMap::from([(1i64.to_le_bytes().to_vec(), "v1".to_string())])
    );
    assert_eq!(table.update(|_k, _v| Some("v3".to_string())).unwrap(), 1);

    // Uniqueness violation.
    table
        .insert(3i64.to_le_bytes().to_vec(), "v3".to_string())
        .unwrap_err();

    table
        .insert(3i64.to_le_bytes().to_vec(), "v4".to_string())
        .unwrap();
    assert_eq!(
        table.items(),
        BTreeMap::from([
            (1i64.to_le_bytes().to_vec(), "v3".to_string()),
            (3i64.to_le_bytes().to_vec(), "v4".to_string()),
        ])
    );
    assert_eq!(
        table
            .update(|_k, _v| Some("v1".to_string()))
            .unwrap_err()
            .to_string(),
        "stash error: uniqueness violation"
    );
    let pending = table.pending();
    assert_eq!(
        pending,
        vec![
            (1i64.to_le_bytes().to_vec(), "v1".to_string(), -1),
            (1i64.to_le_bytes().to_vec(), "v3".to_string(), 1),
            (2i64.to_le_bytes().to_vec(), "v2".to_string(), -1),
            (3i64.to_le_bytes().to_vec(), "v4".to_string(), 1),
        ]
    );
    commit(stash, collection, pending).await.unwrap();
    let items = TABLE.peek_one(stash).await.unwrap();
    assert_eq!(
        items,
        BTreeMap::from([
            (1i64.to_le_bytes().to_vec(), "v3".to_string()),
            (3i64.to_le_bytes().to_vec(), "v4".to_string())
        ])
    );

    let mut table = TableTransaction::new(items, uniqueness_violation);
    // Deleting then creating an item that has a uniqueness violation should work.
    assert_eq!(table.delete(|k, _v| k == &1i64.to_le_bytes()).len(), 1);
    table
        .insert(1i64.to_le_bytes().to_vec(), "v3".to_string())
        .unwrap();
    // Uniqueness violation in value.
    table
        .insert(5i64.to_le_bytes().to_vec(), "v3".to_string())
        .unwrap_err();
    // Key already exists, expect error.
    table
        .insert(1i64.to_le_bytes().to_vec(), "v5".to_string())
        .unwrap_err();
    assert_eq!(table.delete(|k, _v| k == &1i64.to_le_bytes()).len(), 1);
    // Both the inserts work now because the key and uniqueness violation are gone.
    table
        .insert(5i64.to_le_bytes().to_vec(), "v3".to_string())
        .unwrap();
    table
        .insert(1i64.to_le_bytes().to_vec(), "v5".to_string())
        .unwrap();
    let pending = table.pending();
    assert_eq!(
        pending,
        vec![
            (1i64.to_le_bytes().to_vec(), "v3".to_string(), -1),
            (1i64.to_le_bytes().to_vec(), "v5".to_string(), 1),
            (5i64.to_le_bytes().to_vec(), "v3".to_string(), 1),
        ]
    );
    commit(stash, collection, pending).await.unwrap();
    let items = TABLE.peek_one(stash).await.unwrap();
    assert_eq!(
        items,
        BTreeMap::from([
            (1i64.to_le_bytes().to_vec(), "v5".to_string()),
            (3i64.to_le_bytes().to_vec(), "v4".to_string()),
            (5i64.to_le_bytes().to_vec(), "v3".to_string()),
        ])
    );

    let mut table = TableTransaction::new(items, uniqueness_violation);
    assert_eq!(table.delete(|_k, _v| true).len(), 3);
    table
        .insert(1i64.to_le_bytes().to_vec(), "v1".to_string())
        .unwrap();

    commit(stash, collection, table.pending()).await.unwrap();
    let items = TABLE.peek_one(stash).await.unwrap();
    assert_eq!(
        items,
        BTreeMap::from([(1i64.to_le_bytes().to_vec(), "v1".to_string()),])
    );

    let mut table = TableTransaction::new(items, uniqueness_violation);
    assert_eq!(table.delete(|_k, _v| true).len(), 1);
    table
        .insert(1i64.to_le_bytes().to_vec(), "v2".to_string())
        .unwrap();
    commit(stash, collection, table.pending()).await.unwrap();
    let items = TABLE.peek_one(stash).await.unwrap();
    assert_eq!(
        items,
        BTreeMap::from([(1i64.to_le_bytes().to_vec(), "v2".to_string()),])
    );

    // Verify we don't try to delete v3 or v4 during commit.
    let mut table = TableTransaction::new(items, uniqueness_violation);
    assert_eq!(table.delete(|_k, _v| true).len(), 1);
    table
        .insert(1i64.to_le_bytes().to_vec(), "v3".to_string())
        .unwrap();
    table
        .insert(1i64.to_le_bytes().to_vec(), "v4".to_string())
        .unwrap_err();
    assert_eq!(table.delete(|_k, _v| true).len(), 1);
    table
        .insert(1i64.to_le_bytes().to_vec(), "v5".to_string())
        .unwrap();
    commit(stash, collection, table.pending()).await.unwrap();
    let items = TABLE.peek(stash).await.unwrap();
    assert_eq!(
        items,
        vec![(1i64.to_le_bytes().to_vec(), "v5".to_string(), 1)]
    );

    // Test `set`.
    let items = TABLE.peek_one(stash).await.unwrap();
    let mut table = TableTransaction::new(items, uniqueness_violation);
    // Uniqueness violation.
    table
        .set(2i64.to_le_bytes().to_vec(), Some("v5".to_string()))
        .unwrap_err();
    table
        .set(3i64.to_le_bytes().to_vec(), Some("v6".to_string()))
        .unwrap();
    table.set(2i64.to_le_bytes().to_vec(), None).unwrap();
    table.set(1i64.to_le_bytes().to_vec(), None).unwrap();
    let pending = table.pending();
    assert_eq!(
        pending,
        vec![
            (1i64.to_le_bytes().to_vec(), "v5".to_string(), -1),
            (3i64.to_le_bytes().to_vec(), "v6".to_string(), 1),
        ]
    );
    commit(stash, collection, pending).await.unwrap();
    let items = TABLE.peek_one(stash).await.unwrap();
    assert_eq!(
        items,
        BTreeMap::from([(3i64.to_le_bytes().to_vec(), "v6".to_string())])
    );
}

#[test]
fn test_table() {
    fn uniqueness_violation(a: &String, b: &String) -> bool {
        a == b
    }
    let mut table = TableTransaction::new(
        BTreeMap::from([(1i64.to_le_bytes().to_vec(), "a".to_string())]),
        uniqueness_violation,
    );

    table
        .insert(2i64.to_le_bytes().to_vec(), "b".to_string())
        .unwrap();
    table
        .insert(3i64.to_le_bytes().to_vec(), "c".to_string())
        .unwrap();
}
