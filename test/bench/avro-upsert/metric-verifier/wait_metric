#!/usr/bin/env python3

# Copyright Materialize, Inc. All rights reserved.
#
# Use of this software is governed by the Business Source License
# included in the LICENSE file at the root of this repository.
#
# As of the Change Date specified in that file, in accordance with
# the Business Source License, use of this software will be governed
# by the Apache License, Version 2.0.

import argparse
import dateparser
import datetime
import io
import math
import os
import pathlib
import sys
import time
import typing
import urllib
import uuid

import confluent_kafka.admin
import confluent_kafka.avro
import confluent_kafka.schema_registry
import prometheus_api_client
import prometheus_api_client.utils
import requests


METRICS_DIRECTORY = "/usr/share/benchmarks/schemas"


def create_topics(args: argparse.Namespace) -> None:
    """Create the topics for each metric and exit."""

    csr_config = {'url': f'http://{args.csr_host}:{args.csr_port}'}
    csr = confluent_kafka.schema_registry.SchemaRegistryClient(csr_config)

    kafka_config = {'bootstrap.servers': f"{args.kafka_host}:{args.kafka_port}"}
    admin_client = confluent_kafka.admin.AdminClient(kafka_config)

    for metric_dir in pathlib.Path(args.metrics_directory).iterdir():

        key_schema_str = pathlib.Path(metric_dir, 'key-schema.avsc').read_text().strip()
        key_schema = confluent_kafka.schema_registry.Schema(key_schema_str, 'AVRO')
        value_schema_str = pathlib.Path(metric_dir, 'value-schema.avsc').read_text().strip()
        value_schema = confluent_kafka.schema_registry.Schema(value_schema_str, 'AVRO')

        csr.register_schema(f"{metric_dir.name}-key", key_schema)
        csr.register_schema(f"{metric_dir.name}-value", value_schema)

        # Create topics takes and returns a list of futures but we're going to call create topic
        # while iterating on each directory, so these are just lists of 1
        topics = [confluent_kafka.admin.NewTopic(metric_dir.name, num_partitions=10, replication_factor=1)]
        # Don't bother trying to catch the error, let's just fail startup and raise the error
        [future.result() for future in admin_client.create_topics(topics).values()]


class Producer:

    def __init__(self, topic_dir: pathlib.Path, kafka_config: typing.Dict[str, str]):
        """Construct a producer for writing to the given topic, using the given config."""
        self.topic_name = topic_dir.name

        key_schema = confluent_kafka.avro.load(pathlib.Path(topic_dir, 'key-schema.avsc'))
        value_schema = confluent_kafka.avro.load(pathlib.Path(topic_dir, 'value-schema.avsc'))

        # Namespace for both schemas should be the name of the topic
        assert key_schema.namespace == value_schema.namespace
        assert self.topic_name == key_schema.namespace

        self.producer = confluent_kafka.avro.AvroProducer(kafka_config,
                                                default_key_schema=key_schema,
                                                default_value_schema=value_schema)

    def write_metric(self, key: typing.Any, value: typing.Any) -> None:
        """Encode key and value using Avro and send event to Kafka."""
        self.producer.produce(topic=self.topic_name, key=key, value=value)
        self.producer.flush()


class BenchmarkMetrics:

    def __init__(self, args: argparse.Namespace, schema_directory: str):
        """Create an instance responsible for recording benchmark metrics."""
        self.args = args
        self.schema_directory = schema_directory
        self.producers: typing.Dict[str, Producer] = {}

        # See https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md
        kafka_config = {'bootstrap.servers': f"{args.kafka_host}:{args.kafka_port}",
                        'schema.registry.url': f'http://{args.csr_host}:{args.csr_port}'}
        for topic_dir in pathlib.Path(self.schema_directory).iterdir():
            self.producers[topic_dir.name] = Producer(topic_dir, kafka_config)

    def write_metric(self, topic: str, key: typing.Any, value: typing.Any) -> None:
        """Encode key and value using Avro and send event to Kafka."""
        self.producers[topic].write_metric(key, value)


def scrape(args: argparse.Namespace) -> int:
    """Wait for the query to settle or timeout and then dump ingest metrics."""

    start = dateparser.parse('now')
    metrics = BenchmarkMetrics(args, args.metrics_directory)

    # Record this as two distinct metrics. Ideally, the first would be recorded by mzbench
    # when it's populating the list of benchmarks to run. Then second would then be recorded by
    # this runner, which may or may not be the same process / machine as the one enumerating the
    # parameter space
    metrics.write_metric('dev.mtrlz.benchmarks.runs.params.v0',
                         {"run_id": args.mzbench_run_id},
                         {"benchmark_id": args.mzbench_id,
                          "git_ref": args.mzbench_git_ref,
                          "mz_workers": args.mz_workers})
    metrics.write_metric('dev.mtrlz.benchmarks.runs.begin.v0',
                         {"run_id": args.mzbench_run_id},
                         {"start_ms": int(time.time() * 1000)})

    try:
        return wait_metric(args, metrics, start)
    except:
        record_results(args, metrics, start, result="error")
        raise


def record_results(args: argparse.Namespace, metrics: BenchmarkMetrics, start: datetime.datetime,
        rows_per_second: int = 0, result: str = "passed"):
    """Write the results of this benchmark to the Kafka topic"""
    metrics.write_metric('dev.mtrlz.benchmarks.runs.results.v0',
                         {"run_id": args.mzbench_run_id},
                         {"end_ms": int(time.time() * 1000),
                          "rows_per_second": rows_per_second,
                          "result": result})

    # TODO: Construct the URL externally from this script, maintain for compat with mzbench
    # Create parameters to see a dashboard with the metrics from this benchmark run
    # Add padding to make the charts nicer to read
    # Grafana expects timestamps with milliseconds
    path = '/d/materialize-overview/materialize-overview'
    query = urllib.parse.urlencode( {
              "from": round((start - datetime.timedelta(seconds=30)).timestamp()) * 1000,
                "to": round(dateparser.parse('in 30 seconds').timestamp()) * 1000,
                "tz": "UTC"
             })
    dashboard_url = urllib.parse.urlunparse(('http', args.grafana_location, path, '', query, ''))

    print(f'Grafana URL: {dashboard_url}')


def wait_metric(args: argparse.Namespace, metrics: BenchmarkMetrics, start: datetime.datetime) -> int:
    """Wait for the given metric, returning desired exit code (0 is success)."""
    prom = prometheus_api_client.PrometheusConnect(f'http://{args.prometheus_host}:{args.prometheus_port}')
    time_so_far = 0
    begin = int(time.monotonic())
    while time_so_far < args.timeout_seconds:
        current_values = prom.custom_query(args.prom_query)
        if args.verbose:
            print(current_values)

        if len(current_values) > 1:
            print('ERROR: Prometheus query must only return a zero or one results!')
            prometheus_api_client.utils.pretty_print_metric(current_values)
            return 1

        # We aren't running query_range, so there should only be a single timestamp and point in the reponse
        if current_values:
            (ts, point) = [float(i) for i in current_values[0]['value']]
            if point == args.expected_value:
                rate = round(point / max(time_so_far, 1))
                print(f"SUCCESS! seconds_taken={time_so_far} rows_per_sec={rate}")
                record_results(args, metrics, start, rate)
                return 0

        time.sleep(1)
        time_so_far = int(time.monotonic()) - begin

    # Check this last because it's okay to have a 1-2 second grace period and we want the
    # ability to print the most recent result
    print(f"FAILED! Query response is '{point}' after {time_so_far} seconds")
    return 1


def run(args: argparse.Namespace) -> None:
    """Run the desired command."""
    return args.action(args)


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser()

    parser.add_argument(
        "-v",
        "--verbose",
        action="store_true",
        help="Enable verbose logging to print the results of each prometheus query",
    )

    parser.add_argument(
        "--metrics-directory",
        type=str,
        default=METRICS_DIRECTORY,
        help="Directory containing metrics definitions and source queries",
    )

    parser.add_argument(
        "--kafka-host",
        help="Name of the kafka broker",
        type=str,
        default="kafka",
    )

    parser.add_argument(
        "--kafka-port", help="Port the connect to the broker over", type=int, default=9092
    )

    parser.add_argument(
        "--csr-host", help="Hostname of the schema registry", type=str,
        default="schema-registry"
    )
    parser.add_argument(
        "--csr-port", help="Port that schema registry is listening on", type=int,
        default=8081
    )

    sub_parsers = parser.add_subparsers()

    # Create topics subcommand and flags
    create_topic_parser = sub_parsers.add_parser("create-topics")
    create_topic_parser.set_defaults(action=create_topics)

    # Scrape subcommand and flags
    scrape_parser = sub_parsers.add_parser("scrape")
    scrape_parser.set_defaults(action=scrape)

    scrape_parser.add_argument(
        "--grafana-location",
        type=str,
        default='localhost:3000',
        help="Default URL net location (host and port) for Grafana",
    )

    scrape_parser.add_argument(
        "--mz-workers",
        type=int,
        default=os.environ.get('MZ_WORKERS', 0),
        help="How many workers materialized is configured to use",
    )

    scrape_parser.add_argument(
        "--mzbench-git-ref",
        type=str,
        default=os.environ.get('MZBENCH_GIT_REF', 'undefined'),
        help="The materialized image tag",
    )

    scrape_parser.add_argument(
        "--mzbench-id",
        type=str,
        default=os.environ.get('MZBENCH_ID', str(uuid.uuid4())),
        help="Unique string that identifies the grouping of benchmark runs",
    )

    scrape_parser.add_argument(
        "--mzbench-run-id",
        type=str,
        default=os.environ.get('MZBENCH_RUN_ID', str(uuid.uuid4())),
        help="Unique string that identifies this particular benchmark run",
    )

    scrape_parser.add_argument(
        "--prometheus-host",
        type=str,
        default='dashboard',
        help="Hostname of the prometheus instance to query",
    )

    scrape_parser.add_argument(
        "--prometheus-port",
        type=int,
        default=9090,
        help="Port on which the prometheus instance is running",
    )

    scrape_parser.add_argument(
        "-t",
        "--timeout-seconds",
        type=int,
        default=900,
        help="Length of time to wait until the metric reaches the specified value",
    )

    scrape_parser.add_argument(
        "prom_query",
        type=str,
        help="Prometheus query to run",
    )

    scrape_parser.add_argument(
        "expected_value",
        type=float,
        help="Expected value of the metric queried",
    )

    args = parser.parse_args()
    return args

if __name__ == '__main__':
    sys.exit(run(parse_args()))
