# Copyright Materialize, Inc. and contributors. All rights reserved.
#
# Use of this software is governed by the Business Source License
# included in the LICENSE file at the root of this repository.
#
# As of the Change Date specified in that file, in accordance with
# the Business Source License, use of this software will be governed
# by the Apache License, Version 2.0.


# Map from host-port:internal port
#
# This mostly just shows all the ports that are available to the host system, if you want
# to change these you must restart the docker-compose cluster.
x-port-mappings:
  - &kafka-internal ${KAFKA_PORT:-9092}
  - &kafka-external 9093:9093
  - &schema-registry ${SR_PORT:-8081:8081}
  - &materialized ${MZ_PORT:-6875}
  - &grafana 3000:3000
  - &prometheus 9090
  - &perf-metrics ${PERF_METRICS_PORT:-8675}
  - &perf-dash-web ${PERF_DASH_PORT:-8875:8875}

version: '3.7'
services:
  create-views:
    mzbuild: avro-upsert-create-views
  perf-dash-metrics:
    mzbuild: materialized
    ports:
      - *perf-metrics
    command:
      - --disable-telemetry
      - --listen-addr=0.0.0.0:${PERF_METRICS_PORT:-8675}
      - --logical-compaction-window=1ms
    environment:
      - MZ_DEV=1
  perf-dash-scraper:
    mzbuild: perf-dash-scraper
    command: scrape
  perf-dash-create-views:
    mzbuild: perf-dash-create-views
  perf-dash-web:
    mzbuild: perf-dash-web
    ports:
      - *perf-dash-web
  materialized:
    mzbuild: materialized
    ports:
      - *materialized
    command:
      - --workers=${MZ_WORKERS:-16}
      - --logical-compaction-window=1ms
      # We want this to eventually count up to the size of the largest batch in
      # an arrangement. This number represents a tradeoff between proactive
      # merging (which takes time) and low latency.
      #
      # 1000 was chosen by fair dice roll.
      - --differential-idle-merge-effort=1000
      - --timely-progress-mode=${MZ_TIMELY_PROGRESS_MODE:-demand}
      - --disable-telemetry
      - --experimental
      - --persistent-kafka-upsert-source
    environment:
      # You can, for example, add `pgwire=trace` or change `info` to `debug` to
      # get more verbose logs.
      - MZ_LOG_FILTER=pgwire=debug,info
      - MZ_DEV=1
  zookeeper:
    image: confluentinc/cp-zookeeper:5.5.4
    environment:
      - ZOOKEEPER_CLIENT_PORT=2181
  kafka:
    image: confluentinc/cp-enterprise-kafka:5.5.4
    ports:
      - *kafka-internal
      - *kafka-external
    depends_on: [zookeeper]
    environment:
      - KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181
      - KAFKA_LISTENER_SECURITY_PROTOCOL_MAP=PLAINTEXT:PLAINTEXT,EXTERNAL:PLAINTEXT
      - KAFKA_LISTENERS=PLAINTEXT://0.0.0.0:9092,EXTERNAL://0.0.0.0:9093
      - KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092,EXTERNAL://${KAFKA_HOST:-kafka}:9093
      - KAFKA_METRIC_REPORTERS=io.confluent.metrics.reporter.ConfluentMetricsReporter
      - KAFKA_BROKER_ID=1
      - KAFKA_LOG_RETENTION_HOURS=-1
      - KAFKA_NUM_PARTITIONS=30
      - KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1
      - CONFLUENT_METRICS_REPORTER_BOOTSTRAP_SERVERS=kafka:9092
      - CONFLUENT_METRICS_REPORTER_TOPIC_REPLICAS=1
      - CONFLUENT_METRICS_REPORTER_TOPIC_CREATE=false
      - KAFKA_JMX_PORT=9991
  schema-registry:
    image: confluentinc/cp-schema-registry:5.5.4
    ports:
      - *schema-registry
    environment:
     - SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL=zookeeper:2181
     - SCHEMA_REGISTRY_HOST_NAME=schema-registry
     - SCHEMA_REGISTRY_LISTENERS=http://schema-registry:8081,http://localhost:8081
    depends_on: [zookeeper, kafka]
  kafka-avro-generator:
    mzbuild: kafka-avro-generator
  metric-verifier:
    mzbuild: metric-verifier
    environment:
      - MZ_WORKERS=${MZ_WORKERS:-0}
      - MZBENCH_GIT_REF=${MZBENCH_GIT_REF:-None}
      - MZBENCH_ID=${MZBENCH_ID:-0}
  # All monitoring
  dashboard:
    mzbuild: dashboard
    propagate_uid_gid: true
    environment:
      - 'MATERIALIZED_URL=materialized:6875'
    ports:
      - *grafana
      - *prometheus
    volumes:
      # ensure that data doesn't get lost across restarts
      # data will be lost if you remove docker volumes (using nuke, for example)
      - prometheus:/prometheus
      # specialized configurations
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml

volumes:
  prometheus:

mzworkflows:

  start-services:
    steps:
    - step: start-services
      services: [dashboard]
    - step: start-services
      services: [kafka, schema-registry, perf-dash-metrics]
    - step: wait-for-tcp
      host: kafka
      port: 9092
    - step: wait-for-tcp
      host: schema-registry
      port: 8081
    - step: run
      service: perf-dash-scraper
      command: >-
        create-topics
    - step: run
      service: metric-verifier
      command: >-
        create-topics
    - step: run
      service: perf-dash-create-views
    - step: start-services
      services: [perf-dash-web]

  setup-benchmark:
    env:
      UPSERT_GENERATOR_DISTRIBUTION: ${UPSERT_GENERATOR_DISTRIBUTION:-benchmark}
      UPSERT_GENERATOR_NUM_RECORDS: ${UPSERT_GENERATOR_NUM_RECORDS:-400000000}
      UPSERT_GENERATOR_PARALLELISM: ${UPSERT_GENERATOR_PARALLELISM:-40}
    steps:
    - step: workflow
      workflow: start-services
    - step: run
      service: kafka-avro-generator
      command: >-
        --parallelism=${UPSERT_GENERATOR_PARALLELISM}
        --num-records=${UPSERT_GENERATOR_NUM_RECORDS}
        --distribution=${UPSERT_GENERATOR_DISTRIBUTION}
        --topic=upsertavrotest

  run-benchmark:
    env:
      MZ_WORKERS: ${MZ_WORKERS:-16}
      METRIC_VERIFIER_TIMEOUT: ${METRIC_VERIFIER_TIMEOUT:-3600}
      UPSERT_GENERATOR_NUM_RECORDS: ${UPSERT_GENERATOR_NUM_RECORDS:-400000000}
    steps:
    - step: remove-services
      services: [materialized, perf-dash-scraper]
      destroy_volumes: true
    - step: start-services
      services: [materialized, perf-dash-scraper]
    - step: run
      service: metric-verifier
      daemon: true
      command: >-
        scrape
        --timeout-seconds=${METRIC_VERIFIER_TIMEOUT}
        "sum(mz_messages_ingested)"
        ${UPSERT_GENERATOR_NUM_RECORDS}
    - step: run
      service: create-views
    - step: wait
      service: metric-verifier
      expected_return_code: 0
      print_logs: true

  # The benchmark that we run in the cloud
  benchmark-large:
    steps:
    - step: workflow
      workflow: setup-benchmark-large
    - step: workflow
      workflow: run-benchmark-large

  setup-benchmark-large:
    env:
      UPSERT_GENERATOR_DISTRIBUTION: benchmark
      UPSERT_GENERATOR_NUM_RECORDS: 400000000
    steps:
    - step: workflow
      workflow: setup-benchmark

  run-benchmark-large:
    env:
      MZ_WORKERS: ${MZ_WORKERS:-16}
      UPSERT_GENERATOR_NUM_RECORDS: 400000000
    steps:
    - step: workflow
      workflow: run-benchmark

  # The benchmark that developers can run on their laptop
  benchmark-medium:
    steps:
    - step: workflow
      workflow: setup-benchmark-medium
    - step: workflow
      workflow: run-benchmark-medium

  setup-benchmark-medium:
    env:
      UPSERT_GENERATOR_DISTRIBUTION: medium
      UPSERT_GENERATOR_NUM_RECORDS: 20000000
      UPSERT_GENERATOR_PARALLELISM: 8
    steps:
    - step: workflow
      workflow: setup-benchmark

  run-benchmark-medium:
    env:
      MZ_WORKERS: ${MZ_WORKERS:-8}
      METRIC_VERIFIER_TIMEOUT: 900
      UPSERT_GENERATOR_NUM_RECORDS: 20000000
    steps:
    - step: workflow
      workflow: run-benchmark

  # The smoketest benchmark that we run in CI
  ci:
    steps:
    - step: workflow
      workflow: setup-benchmark-ci
    - step: workflow
      workflow: run-benchmark-ci

  benchmark-ci:
    steps:
      - step: worfklow
        workflow: ci

  setup-benchmark-ci:
    env:
      UPSERT_GENERATOR_DISTRIBUTION: small
      UPSERT_GENERATOR_NUM_RECORDS: 100000
      UPSERT_GENERATOR_PARALLELISM: 4
    steps:
    - step: workflow
      workflow: setup-benchmark

  run-benchmark-ci:
    env:
      MZ_WORKERS: ${MZ_WORKERS:-8}
      METRIC_VERIFIER_TIMEOUT: 30
      UPSERT_GENERATOR_NUM_RECORDS: 100000
    steps:
    - step: workflow
      workflow: run-benchmark

  run-generator:
    steps:
    - step: run
      service: kafka-avro-generator

  rerun-benchmark:
    steps:
    - step: remove-services
      services: [materialized, perf-dash-scraper]
      destroy_volumes: true
    - step: start-services
      services: [materialized, perf-dash-scraper]
    - step: run
      service: create-views
