#!/usr/bin/env python3

# Copyright Materialize, Inc. All rights reserved.
#
# Use of this software is governed by the Business Source License
# included in the LICENSE file at the root of this repository.
#
# As of the Change Date specified in that file, in accordance with
# the Business Source License, use of this software will be governed
# by the Apache License, Version 2.0.

import argparse
import dateparser
import datetime
import io
import pathlib
import sys
import time
import typing
import urllib

import confluent_kafka.avro
import prometheus_api_client
import prometheus_api_client.utils
import random_name
import requests


SCHEMAS_DIRECTORY = "/usr/share/benchmarks/schemas"


class Producer:

    def __init__(self, topic_dir: pathlib.Path, kafka_config: typing.Dict[str, str]):
        """Construct a producer for writing to the given topic, using the given config."""
        self.topic_name = topic_dir.name

        key_schema = confluent_kafka.avro.load(pathlib.Path(topic_dir, 'key-schema.avsc'))
        value_schema = confluent_kafka.avro.load(pathlib.Path(topic_dir, 'value-schema.avsc'))

        # Namespace for both schemas should be the name of the topic
        assert key_schema.namespace == value_schema.namespace
        assert self.topic_name == key_schema.namespace

        self.producer = confluent_kafka.avro.AvroProducer(kafka_config,
                                                default_key_schema=key_schema,
                                                default_value_schema=value_schema)

    def write_metric(self, key: typing.Any, value: typing.Any) -> None:
        """Encode key and value using Avro and send event to Kafka."""
        self.producer.produce(topic=self.topic_name, key=key, value=value)
        self.producer.flush()


class BenchmarkMetrics:

    def __init__(self, args: argparse.Namespace, schema_directory: str):
        """Create an instance responsible for recording benchmark metrics."""
        self.args = args
        self.schema_directory = schema_directory
        self.producers: typing.Dict[str, Producer] = {}

        # See https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md
        kafka_config = {'bootstrap.servers': f"{args.kafka_host}:{args.kafka_port}",
                        'schema.registry.url': f'http://{args.csr_host}:{args.csr_port}'}
        for topic_dir in pathlib.Path(self.schema_directory).iterdir():
            self.producers[topic_dir.name] = Producer(topic_dir, kafka_config)

    def write_metric(self, topic: str, key: typing.Any, value: typing.Any) -> None:
        """Encode key and value using Avro and send event to Kafka."""
        self.producers[topic].write_metric(key, value)


def run(args: argparse.Namespace) -> int:
    """Wait for the query to settle or timeout and then dump ingest metrics."""

    start = dateparser.parse('now')
    metrics = BenchmarkMetrics(args, SCHEMAS_DIRECTORY)

    try:
        return wait_metric(args, metrics, start)
    except:
        record_results(args, metrics, start, passed=False)
        raise


def record_results(args: argparse.Namespace, metrics: BenchmarkMetrics, start: datetime.datetime,
        rows_per_second: int = 0, passed: bool = False):
    """Write the results of this benchmark to the Kafka topic"""

    # TODO: Construct the URL externally from this script
    # Create parameters to see a dashboard with the metrics from this benchmark run
    # Add padding to make the charts nicer to read
    # Grafana expects timestamps with milliseconds
    path = '/d/materialize-overview/materialize-overview'
    query = urllib.parse.urlencode( {
              "from": round((start - datetime.timedelta(seconds=30)).timestamp()) * 1000,
                "to": round(dateparser.parse('in 30 seconds').timestamp()) * 1000,
                "tz": "UTC"
             })
    dashboard_url = urllib.parse.urlunparse(('http', args.grafana_location, path, '', query, ''))

    metrics.write_metric('dev.mtrlz.benchmarks.results.v0',
                         {"benchmark_id": args.benchmark_id},
                         {"passed": passed,
                          "rows_per_second": rows_per_second,
                          "start_ms": int(start.timestamp()) * 1000,
                          "end_ms": int(dateparser.parse('now').timestamp()) *  1000,
                          "dashboard_url": dashboard_url
                        })


    print(f'Grafana URL: {dashboard_url}')


def wait_metric(args: argparse.Namespace, metrics: BenchmarkMetrics, start: datetime.datetime) -> int:
    """Wait for the given metric, returning desired exit code (0 is success)."""
    prom = prometheus_api_client.PrometheusConnect(f'http://{args.prometheus_host}:{args.prometheus_port}')
    time_so_far = 0
    begin = int(time.monotonic())
    while time_so_far < args.timeout_seconds:
        current_values = prom.custom_query(args.prom_query)
        if args.verbose:
            print(current_values)

        if len(current_values) > 1:
            print('ERROR: Prometheus query must only return a zero or one results!')
            prometheus_api_client.utils.pretty_print_metric(current_values)
            return 1

        # We aren't running query_range, so there should only be a single timestamp and point in the reponse
        if current_values:
            (ts, point) = [float(i) for i in current_values[0]['value']]
            if point == args.expected_value:
                rate = round(point / max(time_so_far, 1))
                print(f"SUCCESS! seconds_taken={time_so_far} rows_per_sec={rate}")
                record_results(args, metrics, start, rate)
                return 0

        time.sleep(1)
        time_so_far = int(time.monotonic()) - begin

    # Check this last because it's okay to have a 1-2 second grace period and we want the
    # ability to print the most recent result
    print(f"FAILED! Query response is '{point}' after {time_so_far} seconds")
    return 1


if __name__ == "__main__":
    parser = argparse.ArgumentParser()

    parser.add_argument(
        "-v",
        "--verbose",
        action="store_true",
        help="Enable verbose logging to print the results of each prometheus query",
    )

    parser.add_argument(
        "--benchmark-id",
        type=str,
        default=random_name.generate_name(),
        help="Unique identifier that uniquely identifies this iteration of this benchmark",
    )

    parser.add_argument(
        "--grafana-location",
        type=str,
        default='localhost:3000',
        help="Default URL net location (host and port) for Grafana",
    )

    parser.add_argument(
        "--kafka-host",
        help="Name of the kafka broker",
        type=str,
        default="kafka",
    )
    parser.add_argument(
        "--kafka-port", help="Port the connect to the broker over", type=int, default=9092
    )

    parser.add_argument(
        "--csr-host", help="Hostname of the schema registry", type=str,
        default="schema-registry"
    )
    parser.add_argument(
        "--csr-port", help="Port that schema registry is listening on", type=int,
        default=8081
    )


    parser.add_argument(
        "--prometheus-host",
        type=str,
        default='dashboard',
        help="Hostname of the prometheus instance to query",
    )

    parser.add_argument(
        "--prometheus-port",
        type=int,
        default=9090,
        help="Port on which the prometheus instance is running",
    )

    parser.add_argument(
        "-t",
        "--timeout-seconds",
        type=int,
        default=900,
        help="Length of time to wait until the metric reaches the specified value",
    )

    parser.add_argument(
        "prom_query",
        type=str,
        help="Prometheus query to run",
    )

    parser.add_argument(
        "expected_value",
        type=float,
        help="Expected value of the metric queried",
    )

    args = parser.parse_args()
    sys.exit(run(args))
