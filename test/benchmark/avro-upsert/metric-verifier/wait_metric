#!/usr/bin/env python3

# Copyright Materialize, Inc. All rights reserved.
#
# Use of this software is governed by the Business Source License
# included in the LICENSE file at the root of this repository.
#
# As of the Change Date specified in that file, in accordance with
# the Business Source License, use of this software will be governed
# by the Apache License, Version 2.0.

import argparse
import dateparser
import datetime
import io
import pathlib
import sys
import time
import typing
import urllib
import uuid

import avro.schema
import avro.io
import kafka  # type: ignore
import prometheus_api_client
import prometheus_api_client.utils
import requests


SCHEMAS_DIRECTORY = "/usr/share/benchmarks/schemas"


class Metric:

    def __init__(self, topic_name: str, key_schema: str, value_schema: str):
        self.topic_name = topic_name
        self.key_schema = avro.schema.parse(key_schema)
        self.value_schema = avro.schema.parse(value_schema)

        # Namespace for both schemas should be the name of the topic
        assert self.key_schema.namespace == self.value_schema.namespace
        assert self.topic_name == self.key_schema.namespace

        self.key_writer = avro.io.DatumWriter(self.key_schema)
        self.value_writer = avro.io.DatumWriter(self.value_schema)

    def encode_key(self, metric_key: typing.Any) -> bytes:
        buf = io.BytesIO()
        self.key_writer.write(metric_key, avro.io.BinaryEncoder(buf))
        return buf.getvalue()

    def encode_value(self, metric_value: typing.Any) -> bytes:
        buf = io.BytesIO()
        self.value_writer.write(metric_value, avro.io.BinaryEncoder(buf))
        return buf.getvalue()


class BenchmarkMetrics:

    def __init__(self, args: argparse.Namespace, schema_directory: str):
        self.args = args
        self.schema_directory = schema_directory
        self.metrics: typing.Dict[str, Metric] = {}

    def load_schemas(self, args: argparse.Namespace) -> None:
        """Read schemas from disk and update schema registry as well."""
        for topic_dir in pathlib.Path(self.schema_directory).iterdir():
            topic_name = topic_dir.name
            key_schema = pathlib.Path(topic_dir, 'key-schema.avsc').read_text().strip()
            value_schema = pathlib.Path(topic_dir, 'value-schema.avsc').read_text().strip()
            self.metrics[topic_name] = Metric(topic_name, key_schema, value_schema)
            self.update_csr(topic_name, key_schema, value_schema)

    def write_metric(self, topic: str, key: typing.Any, value: typing.Any) -> None:
        """Write the metric to Kafka."""

        # TODO: Create the topic, if it doesn't exist, and configure schema validation using CSR
        producer = kafka.KafkaProducer(
                            bootstrap_servers=[f"{self.args.kafka_host}:{self.args.kafka_port}"],
                            retries=3,
                            key_serializer=self.metrics[topic].encode_key,
                            value_serializer=self.metrics[topic].encode_value,
                        )

        def on_error(excp: Exception) -> None:
            print(f"ERROR: Failed to send message {excp}")
            sys.exit(1)

        producer.send(topic, key=key, value=value)
        producer.flush()


    def update_csr(self, topic: str, key_schema: str, value_schema: str) -> None:
        """Write our JSON schema definition to schema registry."""

        schemas = {f"{topic}-key": key_schema,
                   f"{topic}-value": value_schema}

        # TODO: Query CSR to get the current version of the schema and bail if it doesn't match
        headers = {"Content-Type": "application/vnd.schemaregistry.v1+json"}
        for schema_name, schema in schemas.items():
            response = requests.post(
                f"http://{self.args.csr_host}:{self.args.csr_port}/subjects/{schema_name}/versions",
                json={"schema": schema},
                headers=headers,
            )

            response.raise_for_status()
            if args.verbose:
                print(f"Wrote schema for {schema_name}, subject_id {response.json()['id']}")


def run(args: argparse.Namespace) -> int:
    """Wait for the query to settle or timeout and then dump ingest metrics."""

    start = dateparser.parse('now')
    dashboard_start = dateparser.parse('30 seconds ago')

    metrics = BenchmarkMetrics(args, SCHEMAS_DIRECTORY)
    metrics.load_schemas(args)

    try:
        return wait_metric(args)
    finally:
        record_results(args, metrics, start, dashboard_start)


def record_results(args: argparse.Namespace, metrics: BenchmarkMetrics, start: datetime.datetime, dashboard_start: datetime.datetime):
    """Write the results of this benchmark to the Kafka topic"""

    # Create parameters to see a dashboard with the metrics from this benchmark run
    # Add padding to make the charts nicer to read
    # Grafana expects timestamps with milliseconds
    path = '/d/materialize-overview/materialize-overview'
    query = urllib.parse.urlencode( {
              "from": round(dashboard_start.timestamp()) * 1000,
                "to": round(dateparser.parse('in 30 seconds').timestamp()) * 1000,
                "tz": "UTC"
             })
    dashboard_url = urllib.parse.urlunparse(('http', args.grafana_location, path, '', query, ''))

    metrics.write_metric('dev.mtrlz.benchmarks.results.v0',
                         {"benchmark_id": args.benchmark_id},
                         {"start_ms": int(start.timestamp()) * 1000,
                          "end_ms": int(dateparser.parse('now').timestamp()) *  1000,
                          "dashboard_url": dashboard_url
                        })


    print(f'Grafana URL: {dashboard_url}')


def wait_metric(args: argparse.Namespace) -> int:
    """Wait for the given metric, returning desired exit code (0 is success)."""
    prom = prometheus_api_client.PrometheusConnect(f'http://{args.prometheus_host}:{args.prometheus_port}')
    time_so_far = 0
    start = int(time.monotonic())
    while time_so_far < args.timeout_seconds:
        current_values = prom.custom_query(args.prom_query)
        if args.verbose:
            print(current_values)

        if len(current_values) > 1:
            print('ERROR: Prometheus query must only return a zero or one results!')
            prometheus_api_client.utils.pretty_print_metric(current_values)
            return 1

        # We aren't running query_range, so there should only be a single timestamp and point in the reponse
        if current_values:
            (ts, point) = [float(i) for i in current_values[0]['value']]
            if point == args.expected_value:
                rate = round(point / max(time_so_far, 1))
                print(f"SUCCESS! seconds_taken={time_so_far} rows_per_sec={rate}")
                return 0

        time.sleep(1)
        time_so_far = int(time.monotonic()) - start

    # Check this last because it's okay to have a 1-2 second grace period and we want the
    # ability to print the most recent result
    print(f"FAILED! Query response is '{point}' after {time_so_far} seconds")
    return 1


if __name__ == "__main__":
    parser = argparse.ArgumentParser()

    parser.add_argument(
        "-v",
        "--verbose",
        action="store_true",
        help="Enable verbose logging to print the results of each prometheus query",
    )

    parser.add_argument(
        "--benchmark-id",
        type=str,
        default=str(uuid.uuid4()),
        help="Unique identifier that identifies this benchmark",
    )

    parser.add_argument(
        "--grafana-location",
        type=str,
        default='localhost:3000',
        help="Default URL net location (host and port) for Grafana",
    )

    parser.add_argument(
        "--kafka-host",
        help="Name of the kafka broker",
        type=str,
        default="kafka",
    )
    parser.add_argument(
        "--kafka-port", help="Port the connect to the broker over", type=int, default=9092
    )

    parser.add_argument(
        "--csr-host", help="Hostname of the schema registry", type=str,
        default="schema-registry"
    )
    parser.add_argument(
        "--csr-port", help="Port that schema registry is listening on", type=int,
        default=8081
    )


    parser.add_argument(
        "--prometheus-host",
        type=str,
        default='dashboard',
        help="Hostname of the prometheus instance to query",
    )

    parser.add_argument(
        "--prometheus-port",
        type=int,
        default=9090,
        help="Port on which the prometheus instance is running",
    )

    parser.add_argument(
        "-t",
        "--timeout-seconds",
        type=int,
        default=900,
        help="Length of time to wait until the metric reaches the specified value",
    )

    parser.add_argument(
        "prom_query",
        type=str,
        help="Prometheus query to run",
    )

    parser.add_argument(
        "expected_value",
        type=float,
        help="Expected value of the metric queried",
    )

    args = parser.parse_args()
    sys.exit(run(args))
